{"cells":[{"cell_type":"code","execution_count":37,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-10-04T06:40:30.055491Z","iopub.status.busy":"2023-10-04T06:40:30.055032Z","iopub.status.idle":"2023-10-04T06:40:48.625658Z","shell.execute_reply":"2023-10-04T06:40:48.624671Z","shell.execute_reply.started":"2023-10-04T06:40:30.055452Z"},"trusted":true},"outputs":[],"source":["# !pip install \".inputautocorrect/autocorrect-2.6.1.tar\"\n","# !pip install \".inputpyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\""]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:40:48.628788Z","iopub.status.busy":"2023-10-04T06:40:48.628441Z","iopub.status.idle":"2023-10-04T06:40:48.633138Z","shell.execute_reply":"2023-10-04T06:40:48.632140Z","shell.execute_reply.started":"2023-10-04T06:40:48.628747Z"},"trusted":true},"outputs":[],"source":["# nltk.download(\"punkt\")"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:40:48.635088Z","iopub.status.busy":"2023-10-04T06:40:48.634616Z","iopub.status.idle":"2023-10-04T06:40:48.653149Z","shell.execute_reply":"2023-10-04T06:40:48.652357Z","shell.execute_reply.started":"2023-10-04T06:40:48.635053Z"},"trusted":true},"outputs":[],"source":["from typing import List\n","import numpy as np\n","import pandas as pd\n","import warnings\n","import logging\n","import os\n","import shutil\n","import json\n","import transformers\n","from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n","from transformers import DataCollatorWithPadding\n","from datasets import Dataset,load_dataset, load_from_disk\n","from transformers import TrainingArguments, Trainer\n","from datasets import load_metric, disable_progress_bar\n","from sklearn.metrics import mean_squared_error\n","import torch\n","from sklearn.model_selection import KFold, GroupKFold\n","from tqdm import tqdm\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize.treebank import TreebankWordDetokenizer\n","from collections import Counter\n","import spacy\n","import re\n","from autocorrect import Speller\n","from spellchecker import SpellChecker\n","import lightgbm as lgb\n","warnings.simplefilter(\"ignore\")\n","logging.disable(logging.ERROR)\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n","disable_progress_bar()\n","tqdm.pandas()"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:40:48.654853Z","iopub.status.busy":"2023-10-04T06:40:48.654636Z","iopub.status.idle":"2023-10-04T06:40:48.664885Z","shell.execute_reply":"2023-10-04T06:40:48.663979Z","shell.execute_reply.started":"2023-10-04T06:40:48.654792Z"},"trusted":true},"outputs":[],"source":["def seed_everything(seed: int):\n","    import random, os\n","    import numpy as np\n","    import torch\n","    \n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True # load seed\n","    \n","seed_everything(seed=42)"]},{"cell_type":"markdown","metadata":{},"source":["## Class CFG"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:42:32.950196Z","iopub.status.busy":"2023-10-04T06:42:32.949946Z","iopub.status.idle":"2023-10-04T06:42:32.956667Z","shell.execute_reply":"2023-10-04T06:42:32.954789Z","shell.execute_reply.started":"2023-10-04T06:42:32.950170Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["scale :  5.035864598681746\n","list_model_infer :  dict_keys(['upload_model/debertav3base_lr17e-05', 'upload_model/debertav3base_lr21e-05', 'upload_model/debertav3base_lr22e-05', 'upload_model/debertav3large_lr12e-05', 'upload_model/debertav3large_lr13e-05', 'debertav3large_lr11e-05_att_0007', 'debertav3large_lr17e-05_att_0007', 'debertav3large_lr7e-06_att_0006', 'debertav3large_lr8e-06_att_0006', 'debertav3large_lr12e-05_clean_text', 'debertav3large_lr7e-06_clean_text', 'debertav3large_lr8e-06_clean_text'])\n"]}],"source":["class CFG:\n","    model_name=\"debertav3base\"\n","    learning_rate=0.000016\n","    weight_decay=0.03\n","    hidden_dropout_prob=0.007\n","    attention_probs_dropout_prob=0.007\n","    num_train_epochs=5\n","    n_splits=4\n","    batch_size= 128\n","    random_seed=42\n","    save_steps=100\n","    max_length= 512\n","    number_base_model = 0\n","    test_mode = False\n","    device = 'CPU'\n","    infer_mode = True\n","    list_model_infer = []\n","weight_for_model = {'upload_model/debertav3base_lr17e-05': 0.0019079150150938945,\n"," 'upload_model/debertav3base_lr21e-05': 0.03981389608805265,\n"," 'upload_model/debertav3base_lr22e-05': 0.3286661462956012,\n"," 'upload_model/debertav3large_lr12e-05': 0.7145523014565327,\n"," 'upload_model/debertav3large_lr13e-05': 0.07855318181918133,\n"," 'debertav3large_lr11e-05_att_0007': 0.6653541871957587,\n"," 'debertav3large_lr17e-05_att_0007': 0.8419912793363682,\n"," 'debertav3large_lr7e-06_att_0006': 0.9574590119255362,\n"," 'debertav3large_lr8e-06_att_0006': 0.6382262893087756,\n"," 'debertav3large_lr12e-05_clean_text': -0.39936047178908896,\n"," 'debertav3large_lr7e-06_clean_text': 0.4246156134306383,\n"," 'debertav3large_lr8e-06_clean_text': 0.7440852485992964}\n","\n","scale = np.sum(list(weight_for_model.values()))\n","CFG.list_model_infer = weight_for_model.keys()\n","print(\"scale : \" , scale)\n","print(\"list_model_infer : \" , CFG.list_model_infer)"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:42:33.445629Z","iopub.status.busy":"2023-10-04T06:42:33.445350Z","iopub.status.idle":"2023-10-04T06:42:33.450949Z","shell.execute_reply":"2023-10-04T06:42:33.450072Z","shell.execute_reply.started":"2023-10-04T06:42:33.445577Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["# print device\n","if CFG.device != 'CPU':\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # print device \n","else :\n","    device = torch.device(\"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{},"source":["## Dataload"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:42:33.519669Z","iopub.status.busy":"2023-10-04T06:42:33.519426Z","iopub.status.idle":"2023-10-04T06:42:33.580911Z","shell.execute_reply":"2023-10-04T06:42:33.580042Z","shell.execute_reply.started":"2023-10-04T06:42:33.519644Z"},"trusted":true},"outputs":[],"source":["DATA_DIR = \"input/commonlit-evaluate-student-summaries/\"\n","prompts_train = pd.read_csv(DATA_DIR + \"prompts_train.csv\")\n","prompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")\n","summaries_train = pd.read_csv(DATA_DIR + \"summaries_train.csv\")\n","summaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")\n","sample_submission = pd.read_csv(DATA_DIR + \"sample_submission.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["## Exploratory Data Analysis"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:42:33.610090Z","iopub.status.busy":"2023-10-04T06:42:33.609903Z","iopub.status.idle":"2023-10-04T06:42:33.618920Z","shell.execute_reply":"2023-10-04T06:42:33.617939Z","shell.execute_reply.started":"2023-10-04T06:42:33.610070Z"},"trusted":true},"outputs":[],"source":["# prompts_train.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocess 2\n"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["from textblob import TextBlob\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from nltk import ne_chunk, word_tokenize, pos_tag\n","from bs4 import BeautifulSoup\n","\n","# nltk.downloader.download('vader_lexicon')\n","import pyphen\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","\n","dic = pyphen.Pyphen(lang='en')\n","sid = SentimentIntensityAnalyzer()\n","\n","class Preprocessor:\n","    def __init__(self, \n","                model_name: str,\n","                clean_text_mode = False) -> None:\n","        self.tokenizer = AutoTokenizer.from_pretrained(f\"input/{model_name}\")\n","        self.twd = TreebankWordDetokenizer()\n","        self.STOP_WORDS = set(stopwords.words('english'))\n","        \n","        self.spacy_ner_model = spacy.load('en_core_web_sm',)\n","        self.speller = Speller(lang='en')\n","        self.spellchecker = SpellChecker() \n","        self.clean_text_mode = clean_text_mode\n","        \n","    def calculate_text_similarity(self, row):\n","        vectorizer = TfidfVectorizer()\n","        tfidf_matrix = vectorizer.fit_transform([row['prompt_text'], row['text']])\n","        return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2]).flatten()[0]\n","    \n","    def sentiment_analysis(self, text):\n","        analysis = TextBlob(text)\n","        return analysis.sentiment.polarity, analysis.sentiment.subjectivity\n","    \n","    def word_overlap_count(self, row):\n","        \"\"\" intersection(prompt_text, text) \"\"\"        \n","        def check_is_stop_word(word):\n","            return word in self.STOP_WORDS\n","        \n","        prompt_words = row['prompt_tokens']\n","        summary_words = row['summary_tokens']\n","        if self.STOP_WORDS:\n","            prompt_words = list(filter(check_is_stop_word, prompt_words))\n","            summary_words = list(filter(check_is_stop_word, summary_words))\n","        return len(set(prompt_words).intersection(set(summary_words)))\n","            \n","    def ngrams(self, token, n):\n","        # Use the zip function to help us generate n-grams\n","        # Concatentate the tokens into ngrams and return\n","        ngrams = zip(*[token[i:] for i in range(n)])\n","        return [\" \".join(ngram) for ngram in ngrams]\n","\n","    def ngram_co_occurrence(self, row, n: int) -> int:\n","        # Tokenize the original text and summary into words\n","        original_tokens = row['prompt_tokens']\n","        summary_tokens = row['summary_tokens']\n","\n","        # Generate n-grams for the original text and summary\n","        original_ngrams = set(self.ngrams(original_tokens, n))\n","        summary_ngrams = set(self.ngrams(summary_tokens, n))\n","\n","        # Calculate the number of common n-grams\n","        common_ngrams = original_ngrams.intersection(summary_ngrams)\n","        return len(common_ngrams)\n","    \n","    def ner_overlap_count(self, row, mode:str):\n","        model = self.spacy_ner_model\n","        def clean_ners(ner_list):\n","            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n","        prompt = model(row['prompt_text'])\n","        summary = model(row['text'])\n","\n","        if \"spacy\" in str(model):\n","            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n","            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n","        elif \"stanza\" in str(model):\n","            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n","            summary_ner = set([(token.text, token.type) for token in summary.ents])\n","        else:\n","            raise Exception(\"Model not supported\")\n","\n","        prompt_ner = clean_ners(prompt_ner)\n","        summary_ner = clean_ners(summary_ner)\n","\n","        intersecting_ners = prompt_ner.intersection(summary_ner)\n","        \n","        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n","        \n","        if mode == \"train\":\n","            return ner_dict\n","        elif mode == \"test\":\n","            return {key: ner_dict.get(key) for key in self.ner_keys}\n","\n","    \n","    def quotes_count(self, row):\n","        summary = row['text']\n","        text = row['prompt_text']\n","        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n","        if len(quotes_from_summary)>0:\n","            return [quote in text for quote in quotes_from_summary].count(True)\n","        else:\n","            return 0\n","\n","    def spelling(self, text):\n","        \n","        wordlist=text.split()\n","        amount_miss = len(list(self.spellchecker.unknown(wordlist)))\n","\n","        return amount_miss\n","    \n","    def calculate_unique_words(self,text):\n","        unique_words = set(text.split())\n","        return len(unique_words)\n","    \n","    def add_spelling_dictionary(self, tokens: List[str]) -> List[str]:\n","        \"\"\"dictionary update for pyspell checker and autocorrect\"\"\"\n","        self.spellchecker.word_frequency.load_words(tokens)\n","        self.speller.nlp_data.update({token:1000 for token in tokens})\n","        \n","    def calculate_pos_ratios(self , text):\n","        pos_tags = pos_tag(nltk.word_tokenize(text))\n","        pos_counts = Counter(tag for word, tag in pos_tags)\n","        total_words = len(pos_tags)\n","        ratios = {tag: count / total_words for tag, count in pos_counts.items()}\n","        return ratios\n","    \n","    def calculate_punctuation_ratios(self,text):\n","        total_chars = len(text)\n","        punctuation_counts = Counter(char for char in text if char in '.,!?;:\"()[]{}')\n","        ratios = {char: count / total_chars for char, count in punctuation_counts.items()}\n","        return ratios\n","    \n","    def calculate_keyword_density(self,row):\n","        keywords = set(row['prompt_text'].split())\n","        text_words = row['text'].split()\n","        keyword_count = sum(1 for word in text_words if word in keywords)\n","        return keyword_count / len(text_words)\n","    \n","    def count_syllables(self,word):\n","        hyphenated_word = dic.inserted(word)\n","        return len(hyphenated_word.split('-'))\n","\n","    def flesch_reading_ease_manual(self,text):\n","        total_sentences = len(TextBlob(text).sentences)\n","        total_words = len(TextBlob(text).words)\n","        total_syllables = sum(self.count_syllables(word) for word in TextBlob(text).words)\n","\n","        if total_sentences == 0 or total_words == 0:\n","            return 0\n","\n","        flesch_score = 206.835 - 1.015 * (total_words / total_sentences) - 84.6 * (total_syllables / total_words)\n","        return flesch_score\n","    \n","    def flesch_kincaid_grade_level(self, text):\n","        total_sentences = len(TextBlob(text).sentences)\n","        total_words = len(TextBlob(text).words)\n","        total_syllables = sum(self.count_syllables(word) for word in TextBlob(text).words)\n","\n","        if total_sentences == 0 or total_words == 0:\n","            return 0\n","\n","        fk_grade = 0.39 * (total_words / total_sentences) + 11.8 * (total_syllables / total_words) - 15.59\n","        return fk_grade\n","    \n","    def gunning_fog(self, text):\n","        total_sentences = len(TextBlob(text).sentences)\n","        total_words = len(TextBlob(text).words)\n","        complex_words = sum(1 for word in TextBlob(text).words if self.count_syllables(word) > 2)\n","\n","        if total_sentences == 0 or total_words == 0:\n","            return 0\n","\n","        fog_index = 0.4 * ((total_words / total_sentences) + 100 * (complex_words / total_words))\n","        return fog_index\n","    \n","    def calculate_sentiment_scores(self,text):\n","        sentiment_scores = sid.polarity_scores(text)\n","        return sentiment_scores\n","    \n","    def count_difficult_words(self, text, syllable_threshold=3):\n","        words = TextBlob(text).words\n","        difficult_words_count = sum(1 for word in words if self.count_syllables(word) >= syllable_threshold)\n","        return difficult_words_count\n","\n","    def text_cleaning(self, text):\n","        '''\n","        Cleans text into a basic form for NLP. Operations include the following:-\n","        1. Remove special charecters like &, #, etc\n","        2. Removes extra spaces\n","        3. Removes embedded URL links\n","        4. Removes HTML tags\n","        5. Removes emojis\n","\n","        text - Text piece to be cleaned.\n","        '''\n","        template = re.compile(r'https?://\\S+|www\\.\\S+')  # Removes website links\n","        text = template.sub(r'', text)\n","\n","        soup = BeautifulSoup(text, 'lxml')  # Removes HTML tags\n","        only_text = soup.get_text()\n","        text = only_text\n","\n","        emoji_pattern = re.compile(\"[\"\n","                                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                                u\"\\U00002702-\\U000027B0\"\n","                                u\"\\U000024C2-\\U0001F251\"\n","                                \"]+\", flags=re.UNICODE)\n","        text = emoji_pattern.sub(r'', text)\n","\n","        text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) # Remove special Charecters\n","        text = re.sub('\\n+', '\\n', text) \n","        text = re.sub('\\.+', '.', text) \n","        text = re.sub(' +', ' ', text) # Remove Extra Spaces \n","\n","        return text\n","    \n","    def run(self, \n","            prompts: pd.DataFrame,\n","            summaries:pd.DataFrame,\n","            mode:str\n","        ) -> pd.DataFrame:\n","        # experiment\n","        if self.clean_text_mode :\n","            summaries['text'] = summaries['text'].progress_apply(self.text_cleaning)\n","        # before merge preprocess\n","        prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n","            lambda x: len(word_tokenize(x))\n","        )\n","        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n","            lambda x: word_tokenize(x)\n","        )\n","\n","        summaries[\"summary_length\"] = summaries[\"text\"].apply(\n","            lambda x: len(word_tokenize(x))\n","        )\n","        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n","            lambda x: word_tokenize(x)\n","        )\n","        \n","        # Add prompt tokens into spelling checker dictionary\n","        prompts[\"prompt_tokens\"].apply(\n","            lambda x: self.add_spelling_dictionary(x)\n","        )\n","        \n","        prompts['gunning_fog_prompt'] = prompts['prompt_text'].apply(self.gunning_fog)\n","        prompts['flesch_kincaid_grade_level_prompt'] = prompts['prompt_text'].apply(self.flesch_kincaid_grade_level)\n","        prompts['flesch_reading_ease_prompt'] = prompts['prompt_text'].apply(self.flesch_reading_ease_manual)\n","\n","        \n","#         from IPython.core.debugger import Pdb; Pdb().set_trace()\n","        if self.clean_text_mode:\n","            summaries[\"fixed_summary_text\"] = summaries[\"text\"].progress_apply(\n","                lambda x: self.text_cleaning(x)\n","            )\n","            summaries[\"fixed_summary_text\"] = summaries[\"text\"].progress_apply(\n","                lambda x: self.speller(x)\n","            )\n","        else:\n","            summaries[\"fixed_summary_text\"] = summaries[\"text\"].progress_apply(\n","                lambda x: self.speller(x)\n","            )\n","        \n","        \n","        # count misspelling\n","        summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n","        \n","        # merge prompts and summaries\n","        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n","        input_df['flesch_reading_ease'] = input_df['text'].apply(self.flesch_reading_ease_manual)\n","        input_df['word_count'] = input_df['text'].apply(lambda x: len(x.split()))\n","        input_df['sentence_length'] = input_df['text'].apply(lambda x: len(x.split('.')))\n","        input_df['vocabulary_richness'] = input_df['text'].apply(lambda x: len(set(x.split())))\n","\n","        input_df['word_count2'] = [len(t.split(' ')) for t in input_df.text]\n","        input_df['num_unq_words']=[len(list(set(x.lower().split(' ')))) for x in input_df.text]\n","        input_df['num_chars']= [len(x) for x in input_df.text]\n","\n","        # Additional features\n","        input_df['avg_word_length'] = input_df['text'].apply(lambda x: np.mean([len(word) for word in x.split()]))\n","        input_df['comma_count'] = input_df['text'].apply(lambda x: x.count(','))\n","        input_df['semicolon_count'] = input_df['text'].apply(lambda x: x.count(';'))\n","\n","        # after merge preprocess\n","        input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n","        \n","        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n","        input_df['bigram_overlap_count'] = input_df.progress_apply(\n","            self.ngram_co_occurrence,args=(2,), axis=1 \n","        )\n","        input_df['bigram_overlap_ratio'] = input_df['bigram_overlap_count'] / (input_df['summary_length'] - 1)\n","        \n","        input_df['trigram_overlap_count'] = input_df.progress_apply(\n","            self.ngram_co_occurrence, args=(3,), axis=1\n","        )\n","        input_df['trigram_overlap_ratio'] = input_df['trigram_overlap_count'] / (input_df['summary_length'] - 2)\n","        \n","        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n","        \n","        input_df['exclamation_count'] = input_df['text'].apply(lambda x: x.count('!'))\n","        input_df['question_count'] = input_df['text'].apply(lambda x: x.count('?'))\n","        input_df['pos_ratios'] = input_df['text'].apply(self.calculate_pos_ratios)\n","\n","        # Convert the dictionary of POS ratios into a single value (mean)\n","        input_df['pos_mean'] = input_df['pos_ratios'].apply(lambda x: np.mean(list(x.values())))\n","        input_df['punctuation_ratios'] = input_df['text'].apply(self.calculate_punctuation_ratios)\n","\n","        # Convert the dictionary of punctuation ratios into a single value (sum)\n","        input_df['punctuation_sum'] = input_df['punctuation_ratios'].apply(lambda x: np.sum(list(x.values())))\n","        input_df['keyword_density'] = input_df.apply(self.calculate_keyword_density, axis=1)\n","        input_df['jaccard_similarity'] = input_df.apply(lambda row: len(set(word_tokenize(row['prompt_text'])) & set(word_tokenize(row['text']))) / len(set(word_tokenize(row['prompt_text'])) | set(word_tokenize(row['text']))), axis=1)\n","        tqdm.pandas(desc=\"Performing Sentiment Analysis\")\n","        input_df[['sentiment_polarity', 'sentiment_subjectivity']] = input_df['text'].progress_apply(\n","            lambda x: pd.Series(self.sentiment_analysis(x))\n","        )\n","        tqdm.pandas(desc=\"Calculating Text Similarity\")\n","        input_df['text_similarity'] = input_df.progress_apply(self.calculate_text_similarity, axis=1)\n","        #Calculate sentiment scores for each row\n","        input_df['sentiment_scores'] = input_df['text'].apply(self.calculate_sentiment_scores)\n","        \n","        input_df['gunning_fog'] = input_df['text'].apply(self.gunning_fog)\n","        input_df['flesch_kincaid_grade_level'] = input_df['text'].apply(self.flesch_kincaid_grade_level)\n","        input_df['count_difficult_words'] = input_df['text'].apply(self.count_difficult_words)\n","\n","        # Convert sentiment_scores into individual columns\n","        sentiment_columns = pd.DataFrame(list(input_df['sentiment_scores']))\n","        input_df = pd.concat([input_df, sentiment_columns], axis=1)\n","        input_df['sentiment_scores_prompt'] = input_df['prompt_text'].apply(self.calculate_sentiment_scores)\n","        # Convert sentiment_scores_prompt into individual columns\n","        sentiment_columns_prompt = pd.DataFrame(list(input_df['sentiment_scores_prompt']))\n","        sentiment_columns_prompt.columns = [col +'_prompt' for col in sentiment_columns_prompt.columns]\n","        input_df = pd.concat([input_df, sentiment_columns_prompt], axis=1)\n","        columns =  ['pos_ratios', 'sentiment_scores', 'punctuation_ratios', 'sentiment_scores_prompt']\n","        cols_to_drop = [col for col in columns if col in input_df.columns]\n","        if cols_to_drop:\n","            input_df = input_df.drop(columns=cols_to_drop)\n","        \n","        print(cols_to_drop)\n","        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Create the train and test sets\n"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:42:35.231162Z","iopub.status.busy":"2023-10-04T06:42:35.230619Z","iopub.status.idle":"2023-10-04T06:42:35.236187Z","shell.execute_reply":"2023-10-04T06:42:35.235261Z","shell.execute_reply.started":"2023-10-04T06:42:35.231130Z"},"trusted":true},"outputs":[],"source":["if CFG.test_mode : \n","    prompts_train = prompts_train[:12]\n","    prompts_test = prompts_test[:12]\n","    summaries_train = summaries_train[:12]\n","    summaries_test = summaries_test[:12]"]},{"cell_type":"markdown","metadata":{},"source":["## Load the train data"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:42:35.237834Z","iopub.status.busy":"2023-10-04T06:42:35.237577Z","iopub.status.idle":"2023-10-04T06:49:18.913211Z","shell.execute_reply":"2023-10-04T06:49:18.912332Z","shell.execute_reply.started":"2023-10-04T06:42:35.237803Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 4/4 [00:00<00:00, 22641.32it/s]\n","100%|██████████| 4/4 [00:00<00:00, 20738.22it/s]\n","100%|██████████| 4/4 [00:00<00:00, 7407.16it/s]\n","100%|██████████| 4/4 [00:00<00:00, 8872.14it/s]\n","100%|██████████| 4/4 [00:00<00:00, 10584.99it/s]\n","100%|██████████| 4/4 [00:00<00:00, 10866.07it/s]\n","Performing Sentiment Analysis: 100%|██████████| 4/4 [00:00<00:00, 5260.96it/s]\n","Calculating Text Similarity: 100%|██████████| 4/4 [00:00<00:00, 830.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["['pos_ratios', 'sentiment_scores', 'punctuation_ratios', 'sentiment_scores_prompt']\n"]},{"name":"stderr","output_type":"stream","text":["Calculating Text Similarity: 100%|██████████| 4/4 [00:00<00:00, 5751.53it/s]\n","Calculating Text Similarity: 100%|██████████| 4/4 [00:00<00:00, 7090.96it/s]\n","Calculating Text Similarity: 100%|██████████| 4/4 [00:00<00:00, 25003.30it/s]\n","Calculating Text Similarity: 100%|██████████| 4/4 [00:00<00:00, 31011.49it/s]\n","Calculating Text Similarity: 100%|██████████| 4/4 [00:00<00:00, 7509.94it/s]\n","Calculating Text Similarity: 100%|██████████| 4/4 [00:00<00:00, 10936.91it/s]\n","Calculating Text Similarity: 100%|██████████| 4/4 [00:00<00:00, 8844.08it/s]\n","Calculating Text Similarity: 100%|██████████| 4/4 [00:00<00:00, 11088.71it/s]\n","Performing Sentiment Analysis: 100%|██████████| 4/4 [00:00<00:00, 5269.23it/s]\n","Calculating Text Similarity: 100%|██████████| 4/4 [00:00<00:00, 857.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["['pos_ratios', 'sentiment_scores', 'punctuation_ratios', 'sentiment_scores_prompt']\n"]}],"source":["preprocessor = Preprocessor(model_name=CFG.model_name)\n","# train = preprocessor.run(prompts_train, summaries_train, mode=\"train\")\n","test = preprocessor.run(prompts_test, summaries_test, mode=\"test\")\n","\n","# load preprocess\n","preprocessor = Preprocessor(model_name=CFG.model_name, clean_text_mode=True)\n","test_2 = preprocessor.run(prompts_test, summaries_test, mode=\"test\")# save data train to csv \n","# train_2 = preprocessor.run(prompts_train, summaries_train, mode=\"train\")\n","# train.to_csv(\"input/train_clean_text.csv\", index=False)\n","# load train data\n","# train = pd.read_csv(\"input/train_clean_text.csv\")\n","# train_2.to_csv(\"input/train_preprocess_2.csv\", index=False)\n","train = pd.read_csv(\"input/train.csv\")\n","train_2 = pd.read_csv(\"input/train_preprocess_2.csv\")"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:49:18.916276Z","iopub.status.busy":"2023-10-04T06:49:18.915510Z","iopub.status.idle":"2023-10-04T06:49:18.943400Z","shell.execute_reply":"2023-10-04T06:49:18.942559Z","shell.execute_reply.started":"2023-10-04T06:49:18.916242Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>prompt_id</th>\n","      <th>text</th>\n","      <th>content</th>\n","      <th>wording</th>\n","      <th>summary_length</th>\n","      <th>fixed_summary_text</th>\n","      <th>splling_err_num</th>\n","      <th>prompt_question</th>\n","      <th>prompt_title</th>\n","      <th>...</th>\n","      <th>neu</th>\n","      <th>pos</th>\n","      <th>compound</th>\n","      <th>neg_prompt</th>\n","      <th>neu_prompt</th>\n","      <th>pos_prompt</th>\n","      <th>compound_prompt</th>\n","      <th>fold</th>\n","      <th>wording_pred</th>\n","      <th>content_pred</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000e8c3c7ddb</td>\n","      <td>814d6b</td>\n","      <td>The third wave was an experimentto see how peo...</td>\n","      <td>0.205683</td>\n","      <td>0.380538</td>\n","      <td>64</td>\n","      <td>The third wave was an experimental see how peo...</td>\n","      <td>5</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>...</td>\n","      <td>0.832</td>\n","      <td>0.135</td>\n","      <td>0.7845</td>\n","      <td>0.027</td>\n","      <td>0.873</td>\n","      <td>0.100</td>\n","      <td>0.9915</td>\n","      <td>3.0</td>\n","      <td>-0.222891</td>\n","      <td>0.84066</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0020ae56ffbf</td>\n","      <td>ebad26</td>\n","      <td>They would rub it up with soda to make the sme...</td>\n","      <td>-0.548304</td>\n","      <td>0.506755</td>\n","      <td>54</td>\n","      <td>They would rub it up with soda to make the sme...</td>\n","      <td>2</td>\n","      <td>Summarize the various ways the factory would u...</td>\n","      <td>Excerpt from The Jungle</td>\n","      <td>...</td>\n","      <td>0.946</td>\n","      <td>0.054</td>\n","      <td>0.4310</td>\n","      <td>0.086</td>\n","      <td>0.879</td>\n","      <td>0.035</td>\n","      <td>-0.9949</td>\n","      <td>2.0</td>\n","      <td>-0.800661</td>\n","      <td>-0.27018</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>004e978e639e</td>\n","      <td>3b9047</td>\n","      <td>In Egypt, there were many occupations and soci...</td>\n","      <td>3.128928</td>\n","      <td>4.231226</td>\n","      <td>269</td>\n","      <td>In Egypt, there were many occupations and soci...</td>\n","      <td>32</td>\n","      <td>In complete sentences, summarize the structure...</td>\n","      <td>Egyptian Social Structure</td>\n","      <td>...</td>\n","      <td>0.814</td>\n","      <td>0.139</td>\n","      <td>0.9725</td>\n","      <td>0.063</td>\n","      <td>0.845</td>\n","      <td>0.092</td>\n","      <td>0.9283</td>\n","      <td>1.0</td>\n","      <td>2.513938</td>\n","      <td>2.13275</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3 rows × 55 columns</p>\n","</div>"],"text/plain":["     student_id prompt_id                                               text  \\\n","0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n","1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme...   \n","2  004e978e639e    3b9047  In Egypt, there were many occupations and soci...   \n","\n","    content   wording  summary_length  \\\n","0  0.205683  0.380538              64   \n","1 -0.548304  0.506755              54   \n","2  3.128928  4.231226             269   \n","\n","                                  fixed_summary_text  splling_err_num  \\\n","0  The third wave was an experimental see how peo...                5   \n","1  They would rub it up with soda to make the sme...                2   \n","2  In Egypt, there were many occupations and soci...               32   \n","\n","                                     prompt_question  \\\n","0  Summarize how the Third Wave developed over su...   \n","1  Summarize the various ways the factory would u...   \n","2  In complete sentences, summarize the structure...   \n","\n","                prompt_title  ...    neu    pos  compound  neg_prompt  \\\n","0             The Third Wave  ...  0.832  0.135    0.7845       0.027   \n","1    Excerpt from The Jungle  ...  0.946  0.054    0.4310       0.086   \n","2  Egyptian Social Structure  ...  0.814  0.139    0.9725       0.063   \n","\n","   neu_prompt  pos_prompt  compound_prompt  fold  wording_pred  content_pred  \n","0       0.873       0.100           0.9915   3.0     -0.222891       0.84066  \n","1       0.879       0.035          -0.9949   2.0     -0.800661      -0.27018  \n","2       0.845       0.092           0.9283   1.0      2.513938       2.13275  \n","\n","[3 rows x 55 columns]"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["gkf = GroupKFold(n_splits=CFG.n_splits)\n","\n","for i, (_, val_index) in enumerate(gkf.split(train, groups=train[\"prompt_id\"])):\n","    train.loc[val_index, \"fold\"] = i\n","    train_2.loc[val_index, \"fold\"] = i\n","train.head(3)"]},{"cell_type":"markdown","metadata":{},"source":["## Model Function Definition"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:49:18.944706Z","iopub.status.busy":"2023-10-04T06:49:18.944482Z","iopub.status.idle":"2023-10-04T06:49:18.951510Z","shell.execute_reply":"2023-10-04T06:49:18.950756Z","shell.execute_reply.started":"2023-10-04T06:49:18.944677Z"},"trusted":true},"outputs":[],"source":["def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    rmse = mean_squared_error(labels, predictions, squared=False)\n","    return {\"rmse\": rmse}\n","\n","def compute_mcrmse(eval_pred):\n","    \"\"\"\n","    Calculates mean columnwise root mean squared error\n","    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n","    \"\"\"\n","    preds, labels = eval_pred\n","\n","    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n","    mcrmse = np.mean(col_rmse)\n","\n","    return {\n","        \"content_rmse\": col_rmse[0],\n","        \"wording_rmse\": col_rmse[1],\n","        \"mcrmse\": mcrmse,\n","    }\n","\n","def compt_score(content_true, content_pred, wording_true, wording_pred):\n","    content_score = mean_squared_error(content_true, content_pred)**(1/2)\n","    wording_score = mean_squared_error(wording_true, wording_pred)**(1/2)\n","    \n","    return (content_score + wording_score)/2"]},{"cell_type":"markdown","metadata":{},"source":["## Deberta Regressor"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:49:18.953587Z","iopub.status.busy":"2023-10-04T06:49:18.953198Z","iopub.status.idle":"2023-10-04T06:49:18.971775Z","shell.execute_reply":"2023-10-04T06:49:18.970910Z","shell.execute_reply.started":"2023-10-04T06:49:18.953557Z"},"trusted":true},"outputs":[],"source":["class ScoreRegressor:\n","    def __init__(self, \n","                model_name: str,\n","                model_dir: str,\n","                target: list,\n","                hidden_dropout_prob: float,\n","                attention_probs_dropout_prob: float,\n","                max_length: int,\n","                ):\n","        self.inputs = [\"prompt_text\", \"prompt_title\", \"prompt_question\", \"fixed_summary_text\"] # fix summary text have prompt text in it \n","        self.input_col = \"input\"\n","        \n","        self.text_cols = [self.input_col] \n","        self.target = target\n","        self.target_cols = target\n","\n","        self.model_name = model_name\n","        lr = str(CFG.learning_rate).replace(\".\", \"\")\n","        self.model_dir = model_dir\n","        self.max_length = max_length\n","        \n","        self.tokenizer = AutoTokenizer.from_pretrained(f\"input/{model_name}\")\n","        self.model_config = AutoConfig.from_pretrained(f\"input/{model_name}\" )\n","        # print(self.model_config)\n","        self.model_config.update({\n","            \"hidden_dropout_prob\": hidden_dropout_prob,\n","            \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n","            \"num_labels\": 2,\n","            \"problem_type\": \"regression\",\n","        })\n","        seed_everything(seed=42)\n","\n","        self.data_collator = DataCollatorWithPadding(\n","            tokenizer=self.tokenizer\n","        )\n","\n","\n","    def tokenize_function(self, examples: pd.DataFrame):\n","        # labels = ['content' , 'wording']\n","        # print('labels', labels)\n","        tokenized = self.tokenizer(examples[self.input_col],\n","                         padding=False,\n","                         truncation=True,\n","                         max_length=self.max_length)\n","        return {\n","            **tokenized,\n","            \"labels\": [examples['content'], examples['wording']],\n","        }\n","    \n","    def tokenize_function_test(self, examples: pd.DataFrame):\n","        tokenized = self.tokenizer(examples[self.input_col],\n","                         padding=False,\n","                         truncation=True,\n","                         max_length=self.max_length)\n","        return tokenized\n","        \n","    def predict(self, \n","                test_df: pd.DataFrame,\n","                fold: int,\n","               ):\n","        \"\"\"predict content score\"\"\"\n","        \n","        sep = self.tokenizer.sep_token\n","        in_text = (\n","                    test_df[\"prompt_title\"] + sep \n","                    + test_df[\"prompt_question\"] + sep \n","                    + test_df[\"fixed_summary_text\"]\n","                  )\n","        test_df[self.input_col] = in_text\n","\n","        test_ = test_df[[self.input_col]]\n","    \n","        test_dataset = Dataset.from_pandas(test_, preserve_index=False) \n","        test_tokenized_dataset = test_dataset.map(self.tokenize_function_test, batched=True)\n","\n","        model_content = AutoModelForSequenceClassification.from_pretrained(f\"{self.model_dir}\")\n","        model_content.eval()\n","        \n","        # eg. \"bert/fold_0/\"\n","        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n","        # print(\"model_fold_dir\",model_fold_dir)\n","        test_args = TrainingArguments(\n","            output_dir=model_fold_dir,\n","            do_train = False,\n","            do_predict = True,\n","            per_device_eval_batch_size = CFG.batch_size,   \n","            dataloader_drop_last = False,\n","        )\n","\n","        # init trainer\n","        infer_content = Trainer(\n","                      model = model_content, \n","                      tokenizer=self.tokenizer,\n","                      data_collator=self.data_collator,\n","                      args = test_args)\n","\n","        preds = infer_content.predict(test_tokenized_dataset)[0]\n","\n","        return preds"]},{"cell_type":"markdown","metadata":{},"source":["## Train by fold function\n"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:49:18.973809Z","iopub.status.busy":"2023-10-04T06:49:18.973053Z","iopub.status.idle":"2023-10-04T06:49:18.989797Z","shell.execute_reply":"2023-10-04T06:49:18.988950Z","shell.execute_reply.started":"2023-10-04T06:49:18.973779Z"},"trusted":true},"outputs":[],"source":["def validate(\n","    train_df: pd.DataFrame,\n","    target:str,\n","    save_each_model: bool,\n","    model_name: str,\n","    model_dir_base: str,\n","    hidden_dropout_prob: float,\n","    attention_probs_dropout_prob: float,\n","    max_length : int\n","    ) -> pd.DataFrame:\n","    \"\"\"predict oof data\"\"\"\n","    for fold in range(CFG.n_splits):\n","        # print(f\"fold {fold}:\")\n","        \n","        valid_data = train_df[train_df[\"fold\"] == fold]\n","        \n","        if save_each_model == True:\n","            model_dir =  f\"{target}/{model_dir_base}/fold_{fold}\"\n","        else: \n","            model_dir =  f\"{model_dir_base}/fold_{fold}\"\n","        csr = ScoreRegressor(\n","            model_name=model_name,\n","            target=target,\n","            model_dir = model_dir,\n","            hidden_dropout_prob=hidden_dropout_prob,\n","            attention_probs_dropout_prob=attention_probs_dropout_prob,\n","            max_length=max_length,\n","           )\n","        \n","        pred = csr.predict(\n","            test_df=valid_data, \n","            fold=fold\n","        )\n","        # print('pred shape', pred.shape)\n","        train_df.loc[valid_data.index, f\"wording_pred\"] = pred[:,0]\n","        train_df.loc[valid_data.index, f\"content_pred\"] = pred[:,1]\n","\n","    return train_df\n","    \n","def predict(\n","    test_df: pd.DataFrame,\n","    target:str,\n","    save_each_model: bool,\n","    model_name: str,\n","    model_dir_base: str,\n","    hidden_dropout_prob: float,\n","    attention_probs_dropout_prob: float,\n","    max_length : int\n","    ):\n","    \"\"\"predict using mean folds\"\"\"\n","    for fold in range(CFG.n_splits):\n","        # print(f\"fold {fold}:\")\n","        \n","        if save_each_model == True:\n","            model_dir =  f\"{target}/{model_dir_base}/fold_{fold}\"\n","        else: \n","            model_dir =  f\"{model_dir_base}/fold_{fold}\"\n","        csr = ScoreRegressor(\n","            model_name=model_name,\n","            target=target,\n","            model_dir = model_dir, \n","            hidden_dropout_prob=hidden_dropout_prob,\n","            attention_probs_dropout_prob=attention_probs_dropout_prob,\n","            max_length=max_length,\n","           )\n","        \n","        pred = csr.predict(\n","            test_df=test_df, \n","            fold=fold\n","        )\n","        \n","        # test_df[f\"{target}_pred_{fold}\"] = pred\n","        test_df[f\"wording_pred_{fold}\"] = pred[:,0]\n","        test_df[f\"content_pred_{fold}\"] = pred[:,1]\n","        \n","    # test_df[f\"{target}\"] = test_df[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n","    test_df[f\"wording_pred\"] = test_df[[f\"wording_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n","    test_df[f\"content_pred\"] = test_df[[f\"content_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n","    return test_df\n","targets =  [\"content\", \"wording\"]\n"]},{"cell_type":"markdown","metadata":{},"source":["## Infer model dont clean text\n","\n"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["# ensembling_results_test = pd.DataFrame()"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:49:18.991423Z","iopub.status.busy":"2023-10-04T06:49:18.990863Z","iopub.status.idle":"2023-10-04T06:51:09.807207Z","shell.execute_reply":"2023-10-04T06:51:09.806306Z","shell.execute_reply.started":"2023-10-04T06:49:18.991394Z"},"trusted":true},"outputs":[],"source":["# dem = 0\n","# if CFG.infer_mode:\n","#     for model_dir in CFG.list_model_infer:\n","#         if 'clean_text' in model_dir:\n","#             continue\n","#         if CFG.number_base_model > 0:\n","#             print(\"percent of model\", dem/CFG.number_base_model)\n","#         print(model_dir)    \n","#         dem = dem +1 \n","#         if dem >= CFG.number_base_model:\n","#             CFG.batch_size = 16\n","#             CFG.max_length = 1462\n","#             CFG.model_name = \"debertav3large\"\n","#         # train = validate(\n","#         #     train,\n","#         #     target=targets,\n","#         #     save_each_model=False,\n","#         #     model_name=CFG.model_name,\n","#         #     model_dir_base = model_dir,\n","#         #     hidden_dropout_prob=CFG.hidden_dropout_prob,\n","#         #     attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n","#         #     max_length=CFG.max_length\n","#         # )\n","#         # for target in targets:\n","#         #     rmse = mean_squared_error(train[target], train[f\"{target}_pred\"], squared=False)\n","#         #     print(f\"cv {target} rmse: {rmse}\")\n","#         # print('done validate')\n","#         test = predict(\n","#             test,\n","#             target=targets,\n","#             save_each_model=False,\n","#             model_name=CFG.model_name,\n","#             model_dir_base = model_dir,\n","#             hidden_dropout_prob=CFG.hidden_dropout_prob,\n","#             attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n","#             max_length=CFG.max_length\n","#         )\n","#         print('done predict')\n","#         # add wording_pred and content_pred to ensembling_results\n","#         # ensembling_results_val[f\"{model_dir}_wording_pred\"] = train[\"wording_pred\"]\n","#         # ensembling_results_val[f\"{model_dir}_content_pred\"] = train[\"content_pred\"]\n","#         ensembling_results_test[f\"{model_dir}_wording_pred\"] = test[\"wording_pred\"]\n","#         ensembling_results_test[f\"{model_dir}_content_pred\"] = test[\"content_pred\"]\n","#         # print('ensembling_results_val \\n', ensembling_results_val.head() )\n","        "]},{"cell_type":"markdown","metadata":{},"source":["## infer model clean_text"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["# if CFG.infer_mode:\n","#     for model_dir in CFG.list_model_infer:\n","#         if 'clean_text' not in model_dir:\n","#             continue\n","#         print(model_dir)   \n","#         CFG.batch_size = 5\n","#         # train_2 = validate(\n","#         #     train_2,\n","#         #     target=targets,\n","#         #     save_each_model=False,\n","#         #     model_name=CFG.model_name,\n","#         #     model_dir_base = model_dir,\n","#         #     hidden_dropout_prob=CFG.hidden_dropout_prob,\n","#         #     attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n","#         #     max_length=CFG.max_length\n","#         # )\n","#         # for target in targets:\n","#         #     rmse = mean_squared_error(train_2[target], train_2[f\"{target}_pred\"], squared=False)\n","#         #     print(f\"cv {target} rmse: {rmse}\")\n","#         # print('done validate')\n","#         test_2 = predict(\n","#             test_2,\n","#             target=targets,\n","#             save_each_model=False,\n","#             model_name=CFG.model_name,\n","#             model_dir_base = model_dir,\n","#             hidden_dropout_prob=CFG.hidden_dropout_prob,\n","#             attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n","#             max_length=CFG.max_length\n","#         )\n","#         print('done predict')\n","#         # add wording_pred and content_pred to ensembling_results\n","#         # ensembling_results_val[f\"{model_dir}_wording_pred\"] = train_2[\"wording_pred\"]\n","#         # ensembling_results_val[f\"{model_dir}_content_pred\"] = train_2[\"content_pred\"]\n","#         ensembling_results_test[f\"{model_dir}_wording_pred\"] = test_2[\"wording_pred\"]\n","#         ensembling_results_test[f\"{model_dir}_content_pred\"] = test_2[\"content_pred\"]\n","#         # print('ensembling_results_val \\n', ensembling_results_val.head())"]},{"cell_type":"markdown","metadata":{},"source":["## LGBM model"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["# save ensembling_results_val and ensembling_results_test\n","# ensembling_results_val.to_csv(\"input/ensembling_results_val_clean_text_1110.csv\", index=False)\n","# ensembling_results_test.to_csv(\"ensembling_results_test.csv\", index=False)\n","# load ensembling_results_val and ensembling_results_test"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>upload_model/debertav3base_lr5e-05_wording_pred</th>\n","      <th>upload_model/debertav3base_lr5e-05_content_pred</th>\n","      <th>upload_model/debertav3base_lr15e-05_wording_pred</th>\n","      <th>upload_model/debertav3base_lr15e-05_content_pred</th>\n","      <th>upload_model/debertav3base_lr17e-05_wording_pred</th>\n","      <th>upload_model/debertav3base_lr17e-05_content_pred</th>\n","      <th>upload_model/debertav3base_lr18e-05_wording_pred</th>\n","      <th>upload_model/debertav3base_lr18e-05_content_pred</th>\n","      <th>upload_model/debertav3base_lr21e-05_wording_pred</th>\n","      <th>upload_model/debertav3base_lr21e-05_content_pred</th>\n","      <th>...</th>\n","      <th>debertav3large_lr7e-06_att_0006_wording_pred</th>\n","      <th>debertav3large_lr7e-06_att_0006_content_pred</th>\n","      <th>debertav3large_lr8e-06_att_0006_wording_pred</th>\n","      <th>debertav3large_lr8e-06_att_0006_content_pred</th>\n","      <th>debertav3large_lr12e-05_clean_text_wording_pred</th>\n","      <th>debertav3large_lr12e-05_clean_text_content_pred</th>\n","      <th>debertav3large_lr7e-06_clean_text_wording_pred</th>\n","      <th>debertav3large_lr7e-06_clean_text_content_pred</th>\n","      <th>debertav3large_lr8e-06_clean_text_wording_pred</th>\n","      <th>debertav3large_lr8e-06_clean_text_content_pred</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.289577</td>\n","      <td>0.754494</td>\n","      <td>0.255495</td>\n","      <td>0.757381</td>\n","      <td>0.253911</td>\n","      <td>0.798586</td>\n","      <td>0.238143</td>\n","      <td>0.805690</td>\n","      <td>0.226564</td>\n","      <td>0.868932</td>\n","      <td>...</td>\n","      <td>0.029472</td>\n","      <td>0.575491</td>\n","      <td>0.008973</td>\n","      <td>0.591914</td>\n","      <td>0.064121</td>\n","      <td>0.634637</td>\n","      <td>0.096928</td>\n","      <td>0.661756</td>\n","      <td>0.160011</td>\n","      <td>0.796565</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.702410</td>\n","      <td>-0.433037</td>\n","      <td>-0.801842</td>\n","      <td>-0.464458</td>\n","      <td>-0.772340</td>\n","      <td>-0.413345</td>\n","      <td>-0.665685</td>\n","      <td>-0.327464</td>\n","      <td>-0.641115</td>\n","      <td>-0.371242</td>\n","      <td>...</td>\n","      <td>-0.972083</td>\n","      <td>-0.664230</td>\n","      <td>-1.004822</td>\n","      <td>-0.483753</td>\n","      <td>-0.819019</td>\n","      <td>-0.288662</td>\n","      <td>-0.934642</td>\n","      <td>-0.343674</td>\n","      <td>-0.887494</td>\n","      <td>-0.609832</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2 rows × 52 columns</p>\n","</div>"],"text/plain":["   upload_model/debertav3base_lr5e-05_wording_pred  \\\n","0                                         0.289577   \n","1                                        -0.702410   \n","\n","   upload_model/debertav3base_lr5e-05_content_pred  \\\n","0                                         0.754494   \n","1                                        -0.433037   \n","\n","   upload_model/debertav3base_lr15e-05_wording_pred  \\\n","0                                          0.255495   \n","1                                         -0.801842   \n","\n","   upload_model/debertav3base_lr15e-05_content_pred  \\\n","0                                          0.757381   \n","1                                         -0.464458   \n","\n","   upload_model/debertav3base_lr17e-05_wording_pred  \\\n","0                                          0.253911   \n","1                                         -0.772340   \n","\n","   upload_model/debertav3base_lr17e-05_content_pred  \\\n","0                                          0.798586   \n","1                                         -0.413345   \n","\n","   upload_model/debertav3base_lr18e-05_wording_pred  \\\n","0                                          0.238143   \n","1                                         -0.665685   \n","\n","   upload_model/debertav3base_lr18e-05_content_pred  \\\n","0                                          0.805690   \n","1                                         -0.327464   \n","\n","   upload_model/debertav3base_lr21e-05_wording_pred  \\\n","0                                          0.226564   \n","1                                         -0.641115   \n","\n","   upload_model/debertav3base_lr21e-05_content_pred  ...  \\\n","0                                          0.868932  ...   \n","1                                         -0.371242  ...   \n","\n","   debertav3large_lr7e-06_att_0006_wording_pred  \\\n","0                                      0.029472   \n","1                                     -0.972083   \n","\n","   debertav3large_lr7e-06_att_0006_content_pred  \\\n","0                                      0.575491   \n","1                                     -0.664230   \n","\n","   debertav3large_lr8e-06_att_0006_wording_pred  \\\n","0                                      0.008973   \n","1                                     -1.004822   \n","\n","   debertav3large_lr8e-06_att_0006_content_pred  \\\n","0                                      0.591914   \n","1                                     -0.483753   \n","\n","   debertav3large_lr12e-05_clean_text_wording_pred  \\\n","0                                         0.064121   \n","1                                        -0.819019   \n","\n","   debertav3large_lr12e-05_clean_text_content_pred  \\\n","0                                         0.634637   \n","1                                        -0.288662   \n","\n","   debertav3large_lr7e-06_clean_text_wording_pred  \\\n","0                                        0.096928   \n","1                                       -0.934642   \n","\n","   debertav3large_lr7e-06_clean_text_content_pred  \\\n","0                                        0.661756   \n","1                                       -0.343674   \n","\n","   debertav3large_lr8e-06_clean_text_wording_pred  \\\n","0                                        0.160011   \n","1                                       -0.887494   \n","\n","   debertav3large_lr8e-06_clean_text_content_pred  \n","0                                        0.796565  \n","1                                       -0.609832  \n","\n","[2 rows x 52 columns]"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["ensembling_results_val = pd.read_csv(\"input/ensembling_results_val_clean_text_1110.csv\")\n","ensembling_results_val.head(2)"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["# replace upload_model/ with \"\"\n","# ensembling_results_val = ensembling_results_val.rename(columns=lambda x: x.replace(\"upload_model/\", \"\"))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Find the best weight with optuna"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["# weight_for_model = {}\n","# scale = 0 \n"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["# weight_for_model = {'upload_model/debertav3base_lr17e-05': -0.2566815804974416,\n","#  'upload_model/debertav3base_lr18e-05': 0.2377480721568757,\n","#  'upload_model/debertav3base_lr21e-05': 0.3056613125593628,\n","#  'upload_model/debertav3base_lr22e-05': 0.2365739173075792,\n","#  'upload_model/debertav3large_lr12e-05': 0.4367184075524858,\n","#  'upload_model/debertav3large_lr13e-05': 0.5280894407480876,\n","#  'debertav3large_lr8e-06_att_0007': 0.13148220704647767,\n","#  'debertav3large_lr11e-05_att_0007': 0.031119103031034406,\n","#  'debertav3large_lr13e-05_att_0007': 0.16499189628048969,\n","#  'debertav3large_lr14e-05_att_0007': 0.43258254559068815,\n","#  'debertav3large_lr17e-05_att_0007': 0.5705531664577692,\n","#  'debertav3large_lr18e-05_att_0007': -0.00013989467668473754,\n","#  'debertav3large_lr7e-06_att_0006': 0.6892058130214527,\n","#  'debertav3large_lr8e-06_att_0006': 0.90651196780627,\n","#  'debertav3large_lr12e-05_clean_text': -0.2307792071192416,\n","#  'debertav3large_lr7e-06_clean_text': 0.6074702448549276,\n","#  'debertav3large_lr8e-06_clean_text': 0.5278679871145304}\n","\n","# scale = np.sum([weight_for_model[model] for model in CFG.list_model_infer])\n","# CFG.list_model_infer = weight_for_model.keys()\n","# CFG.list_model_infer = []\n","# CFG.list_model_infer_2 = []\n","# for key in weight_for_model.keys():\n","#     if \"clean_text\" not in key:\n","#         CFG.list_model_infer.append(key)\n","#     else:\n","#         CFG.list_model_infer_2.append(key)"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["upload_model/debertav3base_lr17e-05 mcrmse: 0.5543694149276929\n","upload_model/debertav3base_lr21e-05 mcrmse: 0.553756334918185\n","upload_model/debertav3base_lr22e-05 mcrmse: 0.5480178090069433\n","upload_model/debertav3large_lr12e-05 mcrmse: 0.5531885292652218\n","upload_model/debertav3large_lr13e-05 mcrmse: 0.5520867504987613\n","debertav3large_lr11e-05_att_0007 mcrmse: 0.5395042177968217\n","debertav3large_lr17e-05_att_0007 mcrmse: 0.5517737704359214\n","debertav3large_lr7e-06_att_0006 mcrmse: 0.5199209254995885\n","debertav3large_lr8e-06_att_0006 mcrmse: 0.5229320451563924\n","debertav3large_lr12e-05_clean_text mcrmse: 0.5873813203343701\n","debertav3large_lr7e-06_clean_text mcrmse: 0.5629714450649694\n","debertav3large_lr8e-06_clean_text mcrmse: 0.5638971562771529\n"]}],"source":["for model in CFG.list_model_infer:\n","    results = ensembling_results_val[[f\"{model}_wording_pred\", f\"{model}_content_pred\"]]\n","    mcrmse = compute_mcrmse((results.values, train[targets].values))\n","    print(f\"{model} mcrmse: {mcrmse['mcrmse']}\")"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'upload_model/debertav3base_lr17e-05': 0.0019079150150938945, 'upload_model/debertav3base_lr21e-05': 0.03981389608805265, 'upload_model/debertav3base_lr22e-05': 0.3286661462956012, 'upload_model/debertav3large_lr12e-05': 0.7145523014565327, 'upload_model/debertav3large_lr13e-05': 0.07855318181918133, 'debertav3large_lr11e-05_att_0007': 0.6653541871957587, 'debertav3large_lr17e-05_att_0007': 0.8419912793363682, 'debertav3large_lr7e-06_att_0006': 0.9574590119255362, 'debertav3large_lr8e-06_att_0006': 0.6382262893087756, 'debertav3large_lr12e-05_clean_text': -0.39936047178908896, 'debertav3large_lr7e-06_clean_text': 0.4246156134306383, 'debertav3large_lr8e-06_clean_text': 0.7440852485992964}\n","5.035864598681746\n"]}],"source":["print(weight_for_model)\n","print(scale)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["train[f\"wording_pred\"] = np.sum([ensembling_results_val[f\"{model}_wording_pred\"] * weight_for_model[model] for model in CFG.list_model_infer], axis=0) / scale    \n","train[f\"content_pred\"] = np.sum([ensembling_results_val[f\"{model}_content_pred\"] * weight_for_model[model] for model in CFG.list_model_infer], axis=0) / scale\n","# test[f\"wording_pred\"] = np.sum([ensembling_results_test[f\"{model}_wording_pred\"] * weight_for_model[model] for model in CFG.list_model_infer], axis=0) / scale\n","# test[f\"content_pred\"] = np.sum([ensembling_results_test[f\"{model}_content_pred\"] * weight_for_model[model] for model in CFG.list_model_infer], axis=0) / scale\n"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["# save train and test\n","# train.to_csv(\"input/train.csv\", index=False)\n","# load train and test\n","# test = pd.read_csv(\"input/test.csv\")"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:09.809377Z","iopub.status.busy":"2023-10-04T06:51:09.808548Z","iopub.status.idle":"2023-10-04T06:51:09.814418Z","shell.execute_reply":"2023-10-04T06:51:09.813556Z","shell.execute_reply.started":"2023-10-04T06:51:09.809343Z"},"trusted":true},"outputs":[],"source":["targets = [\"content\", \"wording\"]\n","\n","drop_columns = [\"fold\", \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n","                \"prompt_question\", \"prompt_title\", \n","                \"prompt_text\"\n","               ] + targets"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>prompt_id</th>\n","      <th>text</th>\n","      <th>content</th>\n","      <th>wording</th>\n","      <th>summary_length</th>\n","      <th>fixed_summary_text</th>\n","      <th>splling_err_num</th>\n","      <th>prompt_question</th>\n","      <th>prompt_title</th>\n","      <th>...</th>\n","      <th>neu</th>\n","      <th>pos</th>\n","      <th>compound</th>\n","      <th>neg_prompt</th>\n","      <th>neu_prompt</th>\n","      <th>pos_prompt</th>\n","      <th>compound_prompt</th>\n","      <th>fold</th>\n","      <th>wording_pred</th>\n","      <th>content_pred</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000e8c3c7ddb</td>\n","      <td>814d6b</td>\n","      <td>The third wave was an experimentto see how peo...</td>\n","      <td>0.205683</td>\n","      <td>0.380538</td>\n","      <td>64</td>\n","      <td>The third wave was an experimental see how peo...</td>\n","      <td>5</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>...</td>\n","      <td>0.832</td>\n","      <td>0.135</td>\n","      <td>0.7845</td>\n","      <td>0.027</td>\n","      <td>0.873</td>\n","      <td>0.100</td>\n","      <td>0.9915</td>\n","      <td>3.0</td>\n","      <td>-0.002716</td>\n","      <td>0.735807</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0020ae56ffbf</td>\n","      <td>ebad26</td>\n","      <td>They would rub it up with soda to make the sme...</td>\n","      <td>-0.548304</td>\n","      <td>0.506755</td>\n","      <td>54</td>\n","      <td>They would rub it up with soda to make the sme...</td>\n","      <td>2</td>\n","      <td>Summarize the various ways the factory would u...</td>\n","      <td>Excerpt from The Jungle</td>\n","      <td>...</td>\n","      <td>0.946</td>\n","      <td>0.054</td>\n","      <td>0.4310</td>\n","      <td>0.086</td>\n","      <td>0.879</td>\n","      <td>0.035</td>\n","      <td>-0.9949</td>\n","      <td>2.0</td>\n","      <td>-0.872361</td>\n","      <td>-0.496474</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2 rows × 55 columns</p>\n","</div>"],"text/plain":["     student_id prompt_id                                               text  \\\n","0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n","1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme...   \n","\n","    content   wording  summary_length  \\\n","0  0.205683  0.380538              64   \n","1 -0.548304  0.506755              54   \n","\n","                                  fixed_summary_text  splling_err_num  \\\n","0  The third wave was an experimental see how peo...                5   \n","1  They would rub it up with soda to make the sme...                2   \n","\n","                                     prompt_question             prompt_title  \\\n","0  Summarize how the Third Wave developed over su...           The Third Wave   \n","1  Summarize the various ways the factory would u...  Excerpt from The Jungle   \n","\n","   ...    neu    pos  compound  neg_prompt  neu_prompt  pos_prompt  \\\n","0  ...  0.832  0.135    0.7845       0.027       0.873       0.100   \n","1  ...  0.946  0.054    0.4310       0.086       0.879       0.035   \n","\n","   compound_prompt  fold  wording_pred  content_pred  \n","0           0.9915   3.0     -0.002716      0.735807  \n","1          -0.9949   2.0     -0.872361     -0.496474  \n","\n","[2 rows x 55 columns]"]},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":["train.head(2)"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:09.818456Z","iopub.status.busy":"2023-10-04T06:51:09.817994Z","iopub.status.idle":"2023-10-04T06:51:10.548361Z","shell.execute_reply":"2023-10-04T06:51:10.547516Z","shell.execute_reply.started":"2023-10-04T06:51:09.818419Z"},"trusted":true},"outputs":[],"source":["def create_model_dict(targets,train):\n","  model_dict = {}\n","  for target in targets:\n","      models = []\n","\n","      for fold in range(CFG.n_splits):\n","          X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns, inplace=False)\n","          y_train_cv = train[train[\"fold\"] != fold][target]\n","\n","          X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n","          y_eval_cv = train[train[\"fold\"] == fold][target]\n","\n","          dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n","          dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n","\n","          params = {\n","              'boosting_type': 'gbdt',\n","              'random_state': 42,\n","              'objective': 'regression',\n","              'metric': 'rmse',\n","              'learning_rate': 0.048,\n","              'max_depth': 3,\n","              'lambda_l1': 0.0,\n","              'lambda_l2': 0.011,\n","              'verbose': -1,\n","          }\n","\n","          evaluation_results = {}\n","          model = lgb.train(params,\n","                            num_boost_round=10000,\n","                            valid_names=['train', 'valid'],\n","                            train_set=dtrain,\n","                            valid_sets=dval,\n","                            callbacks=[\n","                                lgb.early_stopping(stopping_rounds=70, verbose=False),\n","                                # lgb.log_evaluation(100),\n","                                lgb.callback.record_evaluation(evaluation_results)\n","                              ],\n","                            )\n","          models.append(model)\n","\n","      model_dict[target] = models\n","  return model_dict\n","model_dict = create_model_dict(targets,train)"]},{"cell_type":"markdown","metadata":{},"source":["## CV Score"]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:10.587200Z","iopub.status.busy":"2023-10-04T06:51:10.586764Z","iopub.status.idle":"2023-10-04T06:51:10.684496Z","shell.execute_reply":"2023-10-04T06:51:10.683623Z","shell.execute_reply.started":"2023-10-04T06:51:10.587169Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["content_rmse : 0.4110683251226793\n","wording_rmse : 0.5342570264645475\n","mcrmse: 0.47266267579361343\n"]}],"source":["# cv\n","import optuna\n","def cal_mcrmse(model_dict, targets, train  = train , print_rmse = False):\n","    rmses = []\n","    for target in targets:\n","        models = model_dict[target]\n","\n","        preds = []\n","        trues = []\n","        \n","        for fold, model in enumerate(models):\n","            X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns , inplace=False)\n","            y_eval_cv = train[train[\"fold\"] == fold][target]\n","\n","            pred = model.predict(X_eval_cv)\n","\n","            trues.extend(y_eval_cv)\n","            preds.extend(pred)\n","            \n","        rmse = np.sqrt(mean_squared_error(trues, preds))\n","        if print_rmse:\n","            print(f\"{target}_rmse : {rmse}\")\n","        rmses = rmses + [rmse]\n","    return sum(rmses) / len(rmses)\n","mcrmse = cal_mcrmse(model_dict, targets, train = train , print_rmse = True)\n","print(f\"mcrmse: {mcrmse}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Optuna"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["scale :  5.435225070470835\n","list_model_infer :  dict_keys(['upload_model/debertav3base_lr17e-05', 'upload_model/debertav3base_lr21e-05', 'upload_model/debertav3base_lr22e-05', 'upload_model/debertav3large_lr12e-05', 'upload_model/debertav3large_lr13e-05', 'debertav3large_lr11e-05_att_0007', 'debertav3large_lr17e-05_att_0007', 'debertav3large_lr7e-06_att_0006', 'debertav3large_lr8e-06_att_0006', 'debertav3large_lr7e-06_clean_text', 'debertav3large_lr8e-06_clean_text'])\n"]}],"source":["weight_for_model = {'upload_model/debertav3base_lr17e-05': 0.0019079150150938945,\n"," 'upload_model/debertav3base_lr21e-05': 0.03981389608805265,\n"," 'upload_model/debertav3base_lr22e-05': 0.3286661462956012,\n"," 'upload_model/debertav3large_lr12e-05': 0.7145523014565327,\n"," 'upload_model/debertav3large_lr13e-05': 0.07855318181918133,\n"," 'debertav3large_lr11e-05_att_0007': 0.6653541871957587,\n"," 'debertav3large_lr17e-05_att_0007': 0.8419912793363682,\n"," 'debertav3large_lr7e-06_att_0006': 0.9574590119255362,\n"," 'debertav3large_lr8e-06_att_0006': 0.6382262893087756,\n","#  'debertav3large_lr12e-05_clean_text': -0.39936047178908896,\n"," 'debertav3large_lr7e-06_clean_text': 0.4246156134306383,\n"," 'debertav3large_lr8e-06_clean_text': 0.7440852485992964}\n","\n","scale = np.sum(list(weight_for_model.values()))\n","CFG.list_model_infer = weight_for_model.keys()\n","print(\"scale : \" , scale)\n","print(\"list_model_infer : \" , CFG.list_model_infer)"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["import optuna\n","def objective(trial,train = train):\n","    weight_for_model = {}\n","    for model in CFG.list_model_infer:\n","        # if \"lr12e-05_clean_text\" in model:\n","        #     weight_for_model[model] = trial.suggest_float(model, -0.5, 0)\n","        # elif \"debertav3base_lr17e-05\" in model:\n","        #     weight_for_model[model] = trial.suggest_float(model, -0.5, 1)\n","        if \"clean_text\" not in model:\n","            # weight_for_model[model] = trial.suggest_float(model, 0, 1)\n","            weight_for_model[model] = trial.suggest_float(model, -0.5, 1, step=0.01)\n","        else:\n","            weight_for_model[model] = trial.suggest_float(model, -0.5, 1,step=0.01)\n","    scale = np.sum([weight_for_model[model] for model in CFG.list_model_infer])\n","    train[f\"wording_pred\"] = np.sum([ensembling_results_val[f\"{model}_wording_pred\"] * weight_for_model[model] for model in CFG.list_model_infer], axis=0) / scale    \n","    train[f\"content_pred\"] = np.sum([ensembling_results_val[f\"{model}_content_pred\"] * weight_for_model[model] for model in CFG.list_model_infer], axis=0) / scale\n","    model_dict = create_model_dict(targets,train)\n","    lost = cal_mcrmse(model_dict, targets)\n","    # print(f\"mcrmse: {lost}\")\n","    # print(f\"weight_for_model: {weight_for_model}\")\n","    return lost"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":["# study = optuna.create_study(direction='minimize')\n","# study.optimize(objective, n_trials=70)\n","# print('Best trial:')\n","# trial_ = study.best_trial\n","\n","# print('Value: ', trial_.value)\n"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":["# trial_ = study.best_trial\n","\n","# print('Value: ', trial_.value)\n","# for model in CFG.list_model_infer:\n","#     weight_for_model[model] = trial_.params[model]\n","# weight_for_model"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[],"source":["# Value:  0.47266267579361343\n","# {'upload_model/debertav3base_lr17e-05': 0.0019079150150938945,\n","#  'upload_model/debertav3base_lr21e-05': 0.03981389608805265,\n","#  'upload_model/debertav3base_lr22e-05': 0.3286661462956012,\n","#  'upload_model/debertav3large_lr12e-05': 0.7145523014565327,\n","#  'upload_model/debertav3large_lr13e-05': 0.07855318181918133,\n","#  'debertav3large_lr11e-05_att_0007': 0.6653541871957587,\n","#  'debertav3large_lr17e-05_att_0007': 0.8419912793363682,\n","#  'debertav3large_lr7e-06_att_0006': 0.9574590119255362,\n","#  'debertav3large_lr8e-06_att_0006': 0.6382262893087756,\n","#  'debertav3large_lr12e-05_clean_text': -0.39936047178908896,\n","#  'debertav3large_lr7e-06_clean_text': 0.4246156134306383,\n","#  'debertav3large_lr8e-06_clean_text': 0.7440852485992964}\n","# Value:  0.47429363257870777\n","# {'upload_model/debertav3base_lr21e-05': 0.5169433346605325,\n","#  'upload_model/debertav3base_lr22e-05': 0.3160789115864947,\n","#  'upload_model/debertav3large_lr12e-05': 0.24532844648357505,\n","#  'upload_model/debertav3large_lr13e-05': 0.40061388588662983,\n","#  'debertav3large_lr11e-05_att_0007': 0.6918758534348053,\n","#  'debertav3large_lr17e-05_att_0007': 0.8988704616729967,\n","#  'debertav3large_lr7e-06_att_0006': 0.6345256607067367,\n","#  'debertav3large_lr8e-06_att_0006': 0.41687335172737405,\n","#  'debertav3large_lr7e-06_clean_text': 0.3243698075644693,\n","#  'debertav3large_lr8e-06_clean_text': 0.37001952423498624}\n","\n","# Value:  0.47449842967399963\n","# {'upload_model/debertav3base_lr17e-05': -0.5426571999177554,\n","#  'upload_model/debertav3base_lr18e-05': -0.41787792532519863,\n","#  'upload_model/debertav3base_lr21e-05': 0.17508025799518462,\n","#  'upload_model/debertav3base_lr22e-05': 0.6593405312017622,\n","#  'upload_model/debertav3large_lr12e-05': 0.8448523828520981,\n","#  'upload_model/debertav3large_lr13e-05': 0.05576651155391085,\n","#  'debertav3large_lr8e-06_att_0007': 0.8261560233885576,\n","#  'debertav3large_lr13e-05_att_0007': 0.235186734542206,\n","#  'debertav3large_lr17e-05_att_0007': 0.8366619848207301,\n","#  'debertav3large_lr7e-06_att_0006': 0.6825759732490162,\n","#  'debertav3large_lr8e-06_att_0006': 0.7447354029009094,\n","#  'debertav3large_lr12e-05_clean_text': 0.001859259890705671,\n","#  'debertav3large_lr7e-06_clean_text': 0.6840497323404321,\n","#  'debertav3large_lr8e-06_clean_text': 0.5555605862244913}"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":["\n","# {'upload_model/debertav3base_lr17e-05': 0,\n","#  'upload_model/debertav3base_lr18e-05': 0,\n","#  'upload_model/debertav3base_lr21e-05': 0,\n","#  'upload_model/debertav3base_lr22e-05': 1,\n","#  'upload_model/debertav3large_lr12e-05': 1,\n","#  'upload_model/debertav3large_lr13e-05': 0,\n","#  'debertav3large_lr8e-06_att_0007': 1,\n","#  'debertav3large_lr9e-06_att_0007': 1,\n","#  'debertav3large_lr11e-05_att_0007': 1,\n","#  'debertav3large_lr12e-05_att_0007': 0,\n","#  'debertav3large_lr13e-05_att_0007': 0,\n","#  'debertav3large_lr14e-05_att_0007': 1,\n","#  'debertav3large_lr15e-05_att_0007': 0,\n","#  'debertav3large_lr16e-05_att_0007': 0,\n","#  'debertav3large_lr17e-05_att_0007': 0,\n","#  'debertav3large_lr18e-05_att_0007': 1,\n","#  'debertav3large_lr6e-06_att_0006': 1,\n","#  'debertav3large_lr7e-06_att_0006': 0,\n","#  'debertav3large_lr8e-06_att_0006': 1}\n","\n","\n","# Value:  0.47623294237768676\n","# Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\n","# {'upload_model/debertav3base_lr22e-05': 0.31611353255666413,\n","#  'upload_model/debertav3large_lr12e-05': 0.4865869028883723,\n","#  'debertav3large_lr8e-06_att_0007': 0.4262645629989505,\n","#  'debertav3large_lr9e-06_att_0007': 0.6016147381680528,\n","#  'debertav3large_lr11e-05_att_0007': 0.531486831481976,\n","#  'debertav3large_lr14e-05_att_0007': 0.18456428603709735,\n","#  'debertav3large_lr18e-05_att_0007': 0.5094108368580108,\n","#  'debertav3large_lr6e-06_att_0006': 0.19869967624910256,\n","#  'debertav3large_lr8e-06_att_0006': 0.7153757700914547}\n","\n","\n","# Value:  0.474565053465253\n","# {'upload_model/debertav3base_lr17e-05': 0.10605670751223599,\n","#  'upload_model/debertav3base_lr18e-05': 0.8151372494853444,\n","#  'upload_model/debertav3base_lr21e-05': -0.17229060080581526,\n","#  'upload_model/debertav3base_lr22e-05': 0.46124428559398767,\n","#  'upload_model/debertav3large_lr12e-05': 0.5213469207997752,\n","#  'upload_model/debertav3large_lr13e-05': 0.33587208988869377,\n","#  'debertav3large_lr8e-06_att_0007': 0.14044911254263082,\n","#  'debertav3large_lr11e-05_att_0007': 0.7504131871645143,\n","#  'debertav3large_lr13e-05_att_0007': 0.6190990620288409,\n","#  'debertav3large_lr14e-05_att_0007': -0.12134513614876408,\n","#  'debertav3large_lr17e-05_att_0007': 0.8917623536805175,\n","#  'debertav3large_lr18e-05_att_0007': -0.07388702753834085,\n","#  'debertav3large_lr7e-06_att_0006': 0.7887573166417163,\n","#  'debertav3large_lr8e-06_att_0006': 0.7195186292932556}\n","\n","# Value:  0.4920279789740877\n","\n","# Value:  0.4742319767376059\n","# {'upload_model/debertav3base_lr17e-05': -0.09377885311717854,\n","#  'upload_model/debertav3base_lr18e-05': 0.26760320345410316,\n","#  'upload_model/debertav3base_lr21e-05': 0.88539432526781,\n","#  'upload_model/debertav3base_lr22e-05': 0.2935469679741507,\n","#  'upload_model/debertav3large_lr12e-05': 0.2033256861331822,\n","#  'upload_model/debertav3large_lr13e-05': 0.4433115247767127,\n","#  'debertav3large_lr8e-06_att_0007': 0.04675646827396546,\n","#  'debertav3large_lr11e-05_att_0007': 0.34646024174157664,\n","#  'debertav3large_lr13e-05_att_0007': -0.0582651730084329,\n","#  'debertav3large_lr14e-05_att_0007': 0.43846159372900956,\n","#  'debertav3large_lr17e-05_att_0007': 0.8677427876536555,\n","#  'debertav3large_lr18e-05_att_0007': 0.4427276761785578,\n","#  'debertav3large_lr7e-06_att_0006': 0.3011481466829745,\n","#  'debertav3large_lr8e-06_att_0006': 0.9613027370178678,\n","#  'debertav3large_lr12e-05_clean_text': 1.7024715723522574,\n","#  'debertav3large_lr7e-06_clean_text': 1.7762890263192577,\n","#  'debertav3large_lr8e-06_clean_text': 1.7733730146858633}\n","\n","# Value:  0.4736596085393129\n","# {'upload_model/debertav3base_lr17e-05': -0.42714818429335777,\n","#  'upload_model/debertav3base_lr18e-05': 0.6006425574784804,\n","#  'upload_model/debertav3base_lr21e-05': 0.2872232605158334,\n","#  'upload_model/debertav3base_lr22e-05': 0.42822073813160527,\n","#  'upload_model/debertav3large_lr12e-05': 0.5375131266654083,\n","#  'upload_model/debertav3large_lr13e-05': 0.53145405551725,\n","#  'debertav3large_lr8e-06_att_0007': -0.14203117484296998,\n","#  'debertav3large_lr11e-05_att_0007': 0.39667694838241024,\n","#  'debertav3large_lr13e-05_att_0007': 0.4038910679295646,\n","#  'debertav3large_lr14e-05_att_0007': 0.13930448236676002,\n","#  'debertav3large_lr17e-05_att_0007': 0.8530494548738371,\n","#  'debertav3large_lr18e-05_att_0007': -0.11474598952902004,\n","#  'debertav3large_lr7e-06_att_0006': 0.573636572433278,\n","#  'debertav3large_lr8e-06_att_0006': 0.25103546345067773,\n","#  'debertav3large_lr12e-05_clean_text': -0.10625543347740543,\n","#  'debertav3large_lr7e-06_clean_text': 0.804519691705804,\n","#  'debertav3large_lr8e-06_clean_text': 0.18404255780368686}\n","\n","# # Value:  0.473264804390935\n","\n","# Value:  0.47579687207458526\n","# {'upload_model/debertav3base_lr17e-05': -0.4584521019285323,\n","#  'upload_model/debertav3base_lr18e-05': 0.35657793888637473,\n","#  'upload_model/debertav3base_lr21e-05': 0.5254525428979288,\n","#  'upload_model/debertav3base_lr22e-05': 0.295101403704541,\n","#  'upload_model/debertav3large_lr12e-05': 0.5567205554812437,\n","#  'upload_model/debertav3large_lr13e-05': 0.9628404481437787,\n","#  'debertav3large_lr8e-06_att_0007': 0.9541992240007228,\n","#  'debertav3large_lr11e-05_att_0007': 0.7563604229334446,\n","#  'debertav3large_lr13e-05_att_0007': 0.4773221931525713,\n","#  'debertav3large_lr14e-05_att_0007': 0.1631474121193286,\n","#  'debertav3large_lr17e-05_att_0007': 0.560998940274175,\n","#  'debertav3large_lr18e-05_att_0007': 0.22442245592329782,\n","#  'debertav3large_lr7e-06_att_0006': 0.7308355347546078,\n","#  'debertav3large_lr8e-06_att_0006': 0.8432284776295845,\n","#  'debertav3large_lr12e-05_clean_text': 0.6105120624791835,\n","#  'debertav3large_lr7e-06_clean_text': 0.7784015817477296,\n","#  'debertav3large_lr8e-06_clean_text': 0.7255528211557266}\n","\n"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[],"source":["# for model in CFG.list_model_infer:\n","#     weight_for_model[model] = trial_.params[model]\n","# weight_for_model"]},{"cell_type":"markdown","metadata":{},"source":["## Predict"]},{"cell_type":"code","execution_count":75,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:10.686383Z","iopub.status.busy":"2023-10-04T06:51:10.685913Z","iopub.status.idle":"2023-10-04T06:51:10.691848Z","shell.execute_reply":"2023-10-04T06:51:10.691034Z","shell.execute_reply.started":"2023-10-04T06:51:10.686351Z"},"trusted":true},"outputs":[],"source":["drop_columns_2 = [\n","                # \"fold\", \n","                \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n","                \"prompt_question\", \"prompt_title\", \n","                \"prompt_text\",\n","                \"input\"\n","               ] + [\n","                f\"content_pred_{i}\" for i in range(CFG.n_splits)\n","                ] + [\n","                f\"wording_pred_{i}\" for i in range(CFG.n_splits)\n","                ]"]},{"cell_type":"code","execution_count":76,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:10.693770Z","iopub.status.busy":"2023-10-04T06:51:10.693252Z","iopub.status.idle":"2023-10-04T06:51:10.714520Z","shell.execute_reply":"2023-10-04T06:51:10.713791Z","shell.execute_reply.started":"2023-10-04T06:51:10.693733Z"},"trusted":true},"outputs":[{"ename":"KeyError","evalue":"\"['input', 'content_pred_0', 'content_pred_1', 'content_pred_2', 'content_pred_3', 'wording_pred_0', 'wording_pred_1', 'wording_pred_2', 'wording_pred_3'] not found in axis\"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[1;32m/home/nghiaph/nghiaph_workspace_115/CommonLit/commonlit-merge-model-infer-weight-final-1110.ipynb Cell 59\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.166.128.115/home/nghiaph/nghiaph_workspace_115/CommonLit/commonlit-merge-model-infer-weight-final-1110.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m preds \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.166.128.115/home/nghiaph/nghiaph_workspace_115/CommonLit/commonlit-merge-model-infer-weight-final-1110.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m fold, model \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(models):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.166.128.115/home/nghiaph/nghiaph_workspace_115/CommonLit/commonlit-merge-model-infer-weight-final-1110.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     X_eval_cv \u001b[39m=\u001b[39m test\u001b[39m.\u001b[39;49mdrop(columns\u001b[39m=\u001b[39;49mdrop_columns_2)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.166.128.115/home/nghiaph/nghiaph_workspace_115/CommonLit/commonlit-merge-model-infer-weight-final-1110.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# print(X_eval_cv.head())\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.166.128.115/home/nghiaph/nghiaph_workspace_115/CommonLit/commonlit-merge-model-infer-weight-final-1110.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_eval_cv)\n","File \u001b[0;32m~/nghiaph_workspace_115/CommonLit/.venv/lib/python3.8/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/nghiaph_workspace_115/CommonLit/.venv/lib/python3.8/site-packages/pandas/core/frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5251\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m   5252\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   5253\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   5261\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5262\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   5263\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5264\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5397\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5398\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5399\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[1;32m   5400\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   5401\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m   5402\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   5403\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m   5404\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m   5405\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[1;32m   5406\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   5407\u001b[0m     )\n","File \u001b[0;32m~/nghiaph_workspace_115/CommonLit/.venv/lib/python3.8/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/nghiaph_workspace_115/CommonLit/.venv/lib/python3.8/site-packages/pandas/core/generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   4504\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 4505\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   4507\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[1;32m   4508\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n","File \u001b[0;32m~/nghiaph_workspace_115/CommonLit/.venv/lib/python3.8/site-packages/pandas/core/generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4544\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[1;32m   4545\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 4546\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   4547\u001b[0m     indexer \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4549\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4550\u001b[0m \u001b[39melse\u001b[39;00m:\n","File \u001b[0;32m~/nghiaph_workspace_115/CommonLit/.venv/lib/python3.8/site-packages/pandas/core/indexes/base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6932\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[1;32m   6933\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 6934\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(labels[mask])\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6935\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[1;32m   6936\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n","\u001b[0;31mKeyError\u001b[0m: \"['input', 'content_pred_0', 'content_pred_1', 'content_pred_2', 'content_pred_3', 'wording_pred_0', 'wording_pred_1', 'wording_pred_2', 'wording_pred_3'] not found in axis\""]}],"source":["pred_dict = {}\n","for target in targets:\n","    models = model_dict[target]\n","    preds = []\n","\n","    for fold, model in enumerate(models):\n","        X_eval_cv = test.drop(columns=drop_columns_2)\n","        # print(X_eval_cv.head())\n","        pred = model.predict(X_eval_cv)\n","        # print('pred shape'  , pred.shape)\n","        preds.append(pred)\n","    \n","    pred_dict[target] = preds"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:10.716195Z","iopub.status.busy":"2023-10-04T06:51:10.715767Z","iopub.status.idle":"2023-10-04T06:51:10.726302Z","shell.execute_reply":"2023-10-04T06:51:10.725328Z","shell.execute_reply.started":"2023-10-04T06:51:10.716164Z"},"trusted":true},"outputs":[],"source":["for target in targets:\n","    preds = pred_dict[target]\n","    for i, pred in enumerate(preds):\n","        test[f\"{target}_pred_{i}\"] = pred\n","\n","    test[target] = test[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:10.728082Z","iopub.status.busy":"2023-10-04T06:51:10.727853Z","iopub.status.idle":"2023-10-04T06:51:10.748281Z","shell.execute_reply":"2023-10-04T06:51:10.747376Z","shell.execute_reply.started":"2023-10-04T06:51:10.728052Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>prompt_id</th>\n","      <th>text</th>\n","      <th>summary_length</th>\n","      <th>fixed_summary_text</th>\n","      <th>splling_err_num</th>\n","      <th>prompt_question</th>\n","      <th>prompt_title</th>\n","      <th>prompt_text</th>\n","      <th>prompt_length</th>\n","      <th>...</th>\n","      <th>wording_pred_1</th>\n","      <th>content_pred_1</th>\n","      <th>wording_pred_2</th>\n","      <th>content_pred_2</th>\n","      <th>wording_pred_3</th>\n","      <th>content_pred_3</th>\n","      <th>wording_pred</th>\n","      <th>content_pred</th>\n","      <th>content</th>\n","      <th>wording</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000000ffffff</td>\n","      <td>abc123</td>\n","      <td>Example text 1</td>\n","      <td>3</td>\n","      <td>Example text 1</td>\n","      <td>0</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 1</td>\n","      <td>Heading\\nText...</td>\n","      <td>3</td>\n","      <td>...</td>\n","      <td>-1.150856</td>\n","      <td>-1.73808</td>\n","      <td>-1.567721</td>\n","      <td>-1.456033</td>\n","      <td>-1.557598</td>\n","      <td>-1.396392</td>\n","      <td>-1.632939</td>\n","      <td>-1.372714</td>\n","      <td>-1.542456</td>\n","      <td>-1.433212</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>111111eeeeee</td>\n","      <td>def789</td>\n","      <td>Example text 2</td>\n","      <td>3</td>\n","      <td>Example text 2</td>\n","      <td>0</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 2</td>\n","      <td>Heading\\nText...</td>\n","      <td>3</td>\n","      <td>...</td>\n","      <td>-1.150856</td>\n","      <td>-1.73808</td>\n","      <td>-1.567721</td>\n","      <td>-1.456033</td>\n","      <td>-1.557598</td>\n","      <td>-1.396392</td>\n","      <td>-1.635602</td>\n","      <td>-1.375756</td>\n","      <td>-1.542456</td>\n","      <td>-1.433212</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>222222cccccc</td>\n","      <td>abc123</td>\n","      <td>Example text 3</td>\n","      <td>3</td>\n","      <td>Example text 3</td>\n","      <td>0</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 1</td>\n","      <td>Heading\\nText...</td>\n","      <td>3</td>\n","      <td>...</td>\n","      <td>-1.150856</td>\n","      <td>-1.73808</td>\n","      <td>-1.567721</td>\n","      <td>-1.456033</td>\n","      <td>-1.557598</td>\n","      <td>-1.396392</td>\n","      <td>-1.625720</td>\n","      <td>-1.386526</td>\n","      <td>-1.542456</td>\n","      <td>-1.433212</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>333333dddddd</td>\n","      <td>def789</td>\n","      <td>Example text 4</td>\n","      <td>3</td>\n","      <td>Example text 4</td>\n","      <td>0</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 2</td>\n","      <td>Heading\\nText...</td>\n","      <td>3</td>\n","      <td>...</td>\n","      <td>-1.150856</td>\n","      <td>-1.73808</td>\n","      <td>-1.567721</td>\n","      <td>-1.456033</td>\n","      <td>-1.557598</td>\n","      <td>-1.396392</td>\n","      <td>-1.626241</td>\n","      <td>-1.385102</td>\n","      <td>-1.542456</td>\n","      <td>-1.433212</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4 rows × 63 columns</p>\n","</div>"],"text/plain":["     student_id prompt_id            text  summary_length fixed_summary_text  \\\n","0  000000ffffff    abc123  Example text 1               3     Example text 1   \n","1  111111eeeeee    def789  Example text 2               3     Example text 2   \n","2  222222cccccc    abc123  Example text 3               3     Example text 3   \n","3  333333dddddd    def789  Example text 4               3     Example text 4   \n","\n","   splling_err_num prompt_question     prompt_title       prompt_text  \\\n","0                0    Summarize...  Example Title 1  Heading\\nText...   \n","1                0    Summarize...  Example Title 2  Heading\\nText...   \n","2                0    Summarize...  Example Title 1  Heading\\nText...   \n","3                0    Summarize...  Example Title 2  Heading\\nText...   \n","\n","   prompt_length  ...  wording_pred_1  content_pred_1  wording_pred_2  \\\n","0              3  ...       -1.150856        -1.73808       -1.567721   \n","1              3  ...       -1.150856        -1.73808       -1.567721   \n","2              3  ...       -1.150856        -1.73808       -1.567721   \n","3              3  ...       -1.150856        -1.73808       -1.567721   \n","\n","   content_pred_2  wording_pred_3  content_pred_3  wording_pred  content_pred  \\\n","0       -1.456033       -1.557598       -1.396392     -1.632939     -1.372714   \n","1       -1.456033       -1.557598       -1.396392     -1.635602     -1.375756   \n","2       -1.456033       -1.557598       -1.396392     -1.625720     -1.386526   \n","3       -1.456033       -1.557598       -1.396392     -1.626241     -1.385102   \n","\n","    content   wording  \n","0 -1.542456 -1.433212  \n","1 -1.542456 -1.433212  \n","2 -1.542456 -1.433212  \n","3 -1.542456 -1.433212  \n","\n","[4 rows x 63 columns]"]},"execution_count":168,"metadata":{},"output_type":"execute_result"}],"source":["test"]},{"cell_type":"markdown","metadata":{},"source":["## Create Submission file"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:10.750060Z","iopub.status.busy":"2023-10-04T06:51:10.749559Z","iopub.status.idle":"2023-10-04T06:51:10.758788Z","shell.execute_reply":"2023-10-04T06:51:10.757827Z","shell.execute_reply.started":"2023-10-04T06:51:10.750030Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>content</th>\n","      <th>wording</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000000ffffff</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>111111eeeeee</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>222222cccccc</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>333333dddddd</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     student_id  content  wording\n","0  000000ffffff      0.0      0.0\n","1  111111eeeeee      0.0      0.0\n","2  222222cccccc      0.0      0.0\n","3  333333dddddd      0.0      0.0"]},"execution_count":169,"metadata":{},"output_type":"execute_result"}],"source":["sample_submission"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:10.760952Z","iopub.status.busy":"2023-10-04T06:51:10.759923Z","iopub.status.idle":"2023-10-04T06:51:10.770471Z","shell.execute_reply":"2023-10-04T06:51:10.769640Z","shell.execute_reply.started":"2023-10-04T06:51:10.760921Z"},"trusted":true},"outputs":[],"source":["test[[\"student_id\", \"content\", \"wording\"]].to_csv(\"submission.csv\", index=False)"]},{"cell_type":"markdown","metadata":{},"source":["## Summary\n","\n","CV result is like this.\n","\n","| | content rmse |wording rmse | mcrmse | LB| |\n","| -- | -- | -- | -- | -- | -- |\n","|baseline| 0.494 | 0.630 | 0.562 | 0.509 | [link](https://www.kaggle.com/code/tsunotsuno/debertav3-baseline-content-and-wording-models)|\n","| use title and question field | 0.476| 0.619 | 0.548 | 0.508 | [link](https://www.kaggle.com/code/tsunotsuno/debertav3-w-prompt-title-question-fields) |\n","| Debertav3 + LGBM | 0.451 | 0.591 | 0.521 | 0.461 | [link](https://www.kaggle.com/code/tsunotsuno/debertav3-lgbm-with-feature-engineering) |\n","| Debertav3 + LGBM with spell autocorrect | 0.448 | 0.581 | 0.514 | 0.459 |nogawanogawa's original code\n","| Debertav3 + LGBM with spell autocorrect and tuning | 0.442 | 0.566 | 0.504 | 0.453 | this notebook |\n","\n","The CV values improved slightly, and the LB value is improved."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
