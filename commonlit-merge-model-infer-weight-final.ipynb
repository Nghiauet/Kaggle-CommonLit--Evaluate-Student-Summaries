{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-10-04T06:40:30.055491Z","iopub.status.busy":"2023-10-04T06:40:30.055032Z","iopub.status.idle":"2023-10-04T06:40:48.625658Z","shell.execute_reply":"2023-10-04T06:40:48.624671Z","shell.execute_reply.started":"2023-10-04T06:40:30.055452Z"},"trusted":true},"outputs":[],"source":["# !pip install \".inputautocorrect/autocorrect-2.6.1.tar\"\n","# !pip install \".inputpyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\""]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:40:48.628788Z","iopub.status.busy":"2023-10-04T06:40:48.628441Z","iopub.status.idle":"2023-10-04T06:40:48.633138Z","shell.execute_reply":"2023-10-04T06:40:48.632140Z","shell.execute_reply.started":"2023-10-04T06:40:48.628747Z"},"trusted":true},"outputs":[],"source":["# nltk.download(\"punkt\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:40:48.635088Z","iopub.status.busy":"2023-10-04T06:40:48.634616Z","iopub.status.idle":"2023-10-04T06:40:48.653149Z","shell.execute_reply":"2023-10-04T06:40:48.652357Z","shell.execute_reply.started":"2023-10-04T06:40:48.635053Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/nghiaph/nghiaph_workspace_115/CommonLit/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","2023-10-10 13:08:49.784894: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-10-10 13:08:49.840212: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-10-10 13:08:50.124393: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-10-10 13:08:50.126251: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-10-10 13:08:50.737116: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/home/nghiaph/nghiaph_workspace_115/CommonLit/.venv/lib/python3.8/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/home/nghiaph/nghiaph_workspace_115/CommonLit/.venv/lib/python3.8/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n","caused by: ['/home/nghiaph/nghiaph_workspace_115/CommonLit/.venv/lib/python3.8/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n","  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n","/home/nghiaph/nghiaph_workspace_115/CommonLit/.venv/lib/python3.8/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/home/nghiaph/nghiaph_workspace_115/CommonLit/.venv/lib/python3.8/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n","caused by: ['/home/nghiaph/nghiaph_workspace_115/CommonLit/.venv/lib/python3.8/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n","  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","2023-10-10 13:08:53.152231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-10-10 13:08:53.152820: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n","Skipping registering GPU devices...\n"]}],"source":["from typing import List\n","import numpy as np\n","import pandas as pd\n","import warnings\n","import logging\n","import os\n","import shutil\n","import json\n","import transformers\n","from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n","from transformers import DataCollatorWithPadding\n","from datasets import Dataset,load_dataset, load_from_disk\n","from transformers import TrainingArguments, Trainer\n","from datasets import load_metric, disable_progress_bar\n","from sklearn.metrics import mean_squared_error\n","import torch\n","from sklearn.model_selection import KFold, GroupKFold\n","from tqdm import tqdm\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize.treebank import TreebankWordDetokenizer\n","from collections import Counter\n","import spacy\n","import re\n","from autocorrect import Speller\n","from spellchecker import SpellChecker\n","import lightgbm as lgb\n","warnings.simplefilter(\"ignore\")\n","logging.disable(logging.ERROR)\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n","disable_progress_bar()\n","tqdm.pandas()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:40:48.654853Z","iopub.status.busy":"2023-10-04T06:40:48.654636Z","iopub.status.idle":"2023-10-04T06:40:48.664885Z","shell.execute_reply":"2023-10-04T06:40:48.663979Z","shell.execute_reply.started":"2023-10-04T06:40:48.654792Z"},"trusted":true},"outputs":[],"source":["def seed_everything(seed: int):\n","    import random, os\n","    import numpy as np\n","    import torch\n","    \n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True # load seed\n","    \n","seed_everything(seed=42)"]},{"cell_type":"markdown","metadata":{},"source":["## Class CFG"]},{"cell_type":"code","execution_count":174,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:42:32.950196Z","iopub.status.busy":"2023-10-04T06:42:32.949946Z","iopub.status.idle":"2023-10-04T06:42:32.956667Z","shell.execute_reply":"2023-10-04T06:42:32.954789Z","shell.execute_reply.started":"2023-10-04T06:42:32.950170Z"},"trusted":true},"outputs":[],"source":["class CFG:\n","    model_name=\"debertav3base\"\n","    learning_rate=0.000016\n","    weight_decay=0.03\n","    hidden_dropout_prob=0.007\n","    attention_probs_dropout_prob=0.007\n","    num_train_epochs=5\n","    n_splits=4\n","    batch_size= 128\n","    random_seed=42\n","    save_steps=100\n","    max_length= 512\n","    number_base_model = 0\n","    test_mode = False\n","    device = 'CPU'\n","    infer_mode = True\n","    list_model_infer = [\n","        # 'upload_model/debertav3base_lr17e-05', #keep\n","        # 'upload_model/debertav3base_lr18e-05', #keep\n","        # 'upload_model/debertav3base_lr21e-05', # keep\n","        # 'upload_model/debertav3base_lr22e-05', #keep \n","        # 'upload_model/debertav3large_lr12e-05', # upload\n","        # 'upload_model/debertav3large_lr13e-05',  # upload\n","        # 'debertav3large_lr8e-06_att_0007', # upload \n","        # 'debertav3large_lr9e-06_att_0007', # upload \n","        # 'debertav3large_lr11e-05_att_0007', # upload\n","        # 'debertav3large_lr12e-05_att_0007', # upload\n","        # 'debertav3large_lr13e-05_att_0007',# upload\n","        # 'debertav3large_lr14e-05_att_0007',# upload\n","        # 'debertav3large_lr15e-05_att_0007', # upload \n","        # 'debertav3large_lr16e-05_att_0007', # upload \n","        # 'debertav3large_lr17e-05_att_0007', # upload \n","        # 'debertav3large_lr18e-05_att_0007', # upload \n","        'debertav3large_lr6e-06_att_0006',\n","        'debertav3large_lr7e-06_att_0006',\n","        'debertav3large_lr8e-06_att_0006',\n","        ]\n","    "]},{"cell_type":"code","execution_count":175,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:42:33.445629Z","iopub.status.busy":"2023-10-04T06:42:33.445350Z","iopub.status.idle":"2023-10-04T06:42:33.450949Z","shell.execute_reply":"2023-10-04T06:42:33.450072Z","shell.execute_reply.started":"2023-10-04T06:42:33.445577Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["# print device\n","if CFG.device != 'CPU':\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # print device \n","else :\n","    device = torch.device(\"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{},"source":["## Dataload"]},{"cell_type":"code","execution_count":176,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:42:33.519669Z","iopub.status.busy":"2023-10-04T06:42:33.519426Z","iopub.status.idle":"2023-10-04T06:42:33.580911Z","shell.execute_reply":"2023-10-04T06:42:33.580042Z","shell.execute_reply.started":"2023-10-04T06:42:33.519644Z"},"trusted":true},"outputs":[],"source":["DATA_DIR = \"input/commonlit-evaluate-student-summaries/\"\n","prompts_train = pd.read_csv(DATA_DIR + \"prompts_train.csv\")\n","prompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")\n","summaries_train = pd.read_csv(DATA_DIR + \"summaries_train.csv\")\n","summaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")\n","sample_submission = pd.read_csv(DATA_DIR + \"sample_submission.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["## Exploratory Data Analysis"]},{"cell_type":"code","execution_count":177,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:42:33.610090Z","iopub.status.busy":"2023-10-04T06:42:33.609903Z","iopub.status.idle":"2023-10-04T06:42:33.618920Z","shell.execute_reply":"2023-10-04T06:42:33.617939Z","shell.execute_reply.started":"2023-10-04T06:42:33.610070Z"},"trusted":true},"outputs":[],"source":["# prompts_train.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocess 2\n"]},{"cell_type":"code","execution_count":178,"metadata":{},"outputs":[],"source":["from textblob import TextBlob\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from nltk import ne_chunk, word_tokenize, pos_tag\n","from bs4 import BeautifulSoup\n","\n","# nltk.downloader.download('vader_lexicon')\n","import pyphen\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","\n","dic = pyphen.Pyphen(lang='en')\n","sid = SentimentIntensityAnalyzer()\n","\n","class Preprocessor2:\n","    def __init__(self, \n","                model_name: str,\n","                ) -> None:\n","        self.tokenizer = AutoTokenizer.from_pretrained(f\"input/{model_name}\")\n","        self.twd = TreebankWordDetokenizer()\n","        self.STOP_WORDS = set(stopwords.words('english'))\n","        \n","        self.spacy_ner_model = spacy.load('en_core_web_sm',)\n","        self.speller = Speller(lang='en')\n","        self.spellchecker = SpellChecker() \n","        \n","    def calculate_text_similarity(self, row):\n","        vectorizer = TfidfVectorizer()\n","        tfidf_matrix = vectorizer.fit_transform([row['prompt_text'], row['text']])\n","        return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2]).flatten()[0]\n","    \n","    def sentiment_analysis(self, text):\n","        analysis = TextBlob(text)\n","        return analysis.sentiment.polarity, analysis.sentiment.subjectivity\n","    \n","    def word_overlap_count(self, row):\n","        \"\"\" intersection(prompt_text, text) \"\"\"        \n","        def check_is_stop_word(word):\n","            return word in self.STOP_WORDS\n","        \n","        prompt_words = row['prompt_tokens']\n","        summary_words = row['summary_tokens']\n","        if self.STOP_WORDS:\n","            prompt_words = list(filter(check_is_stop_word, prompt_words))\n","            summary_words = list(filter(check_is_stop_word, summary_words))\n","        return len(set(prompt_words).intersection(set(summary_words)))\n","            \n","    def ngrams(self, token, n):\n","        # Use the zip function to help us generate n-grams\n","        # Concatentate the tokens into ngrams and return\n","        ngrams = zip(*[token[i:] for i in range(n)])\n","        return [\" \".join(ngram) for ngram in ngrams]\n","\n","    def ngram_co_occurrence(self, row, n: int) -> int:\n","        # Tokenize the original text and summary into words\n","        original_tokens = row['prompt_tokens']\n","        summary_tokens = row['summary_tokens']\n","\n","        # Generate n-grams for the original text and summary\n","        original_ngrams = set(self.ngrams(original_tokens, n))\n","        summary_ngrams = set(self.ngrams(summary_tokens, n))\n","\n","        # Calculate the number of common n-grams\n","        common_ngrams = original_ngrams.intersection(summary_ngrams)\n","        return len(common_ngrams)\n","    \n","    def ner_overlap_count(self, row, mode:str):\n","        model = self.spacy_ner_model\n","        def clean_ners(ner_list):\n","            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n","        prompt = model(row['prompt_text'])\n","        summary = model(row['text'])\n","\n","        if \"spacy\" in str(model):\n","            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n","            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n","        elif \"stanza\" in str(model):\n","            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n","            summary_ner = set([(token.text, token.type) for token in summary.ents])\n","        else:\n","            raise Exception(\"Model not supported\")\n","\n","        prompt_ner = clean_ners(prompt_ner)\n","        summary_ner = clean_ners(summary_ner)\n","\n","        intersecting_ners = prompt_ner.intersection(summary_ner)\n","        \n","        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n","        \n","        if mode == \"train\":\n","            return ner_dict\n","        elif mode == \"test\":\n","            return {key: ner_dict.get(key) for key in self.ner_keys}\n","\n","    \n","    def quotes_count(self, row):\n","        summary = row['text']\n","        text = row['prompt_text']\n","        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n","        if len(quotes_from_summary)>0:\n","            return [quote in text for quote in quotes_from_summary].count(True)\n","        else:\n","            return 0\n","\n","    def spelling(self, text):\n","        \n","        wordlist=text.split()\n","        amount_miss = len(list(self.spellchecker.unknown(wordlist)))\n","\n","        return amount_miss\n","    \n","    def calculate_unique_words(self,text):\n","        unique_words = set(text.split())\n","        return len(unique_words)\n","    \n","    def add_spelling_dictionary(self, tokens: List[str]) -> List[str]:\n","        \"\"\"dictionary update for pyspell checker and autocorrect\"\"\"\n","        self.spellchecker.word_frequency.load_words(tokens)\n","        self.speller.nlp_data.update({token:1000 for token in tokens})\n","        \n","    def calculate_pos_ratios(self , text):\n","        pos_tags = pos_tag(nltk.word_tokenize(text))\n","        pos_counts = Counter(tag for word, tag in pos_tags)\n","        total_words = len(pos_tags)\n","        ratios = {tag: count / total_words for tag, count in pos_counts.items()}\n","        return ratios\n","    \n","    def calculate_punctuation_ratios(self,text):\n","        total_chars = len(text)\n","        punctuation_counts = Counter(char for char in text if char in '.,!?;:\"()[]{}')\n","        ratios = {char: count / total_chars for char, count in punctuation_counts.items()}\n","        return ratios\n","    \n","    def calculate_keyword_density(self,row):\n","        keywords = set(row['prompt_text'].split())\n","        text_words = row['text'].split()\n","        keyword_count = sum(1 for word in text_words if word in keywords)\n","        return keyword_count / len(text_words)\n","    \n","    def count_syllables(self,word):\n","        hyphenated_word = dic.inserted(word)\n","        return len(hyphenated_word.split('-'))\n","\n","    def flesch_reading_ease_manual(self,text):\n","        total_sentences = len(TextBlob(text).sentences)\n","        total_words = len(TextBlob(text).words)\n","        total_syllables = sum(self.count_syllables(word) for word in TextBlob(text).words)\n","\n","        if total_sentences == 0 or total_words == 0:\n","            return 0\n","\n","        flesch_score = 206.835 - 1.015 * (total_words / total_sentences) - 84.6 * (total_syllables / total_words)\n","        return flesch_score\n","    \n","    def flesch_kincaid_grade_level(self, text):\n","        total_sentences = len(TextBlob(text).sentences)\n","        total_words = len(TextBlob(text).words)\n","        total_syllables = sum(self.count_syllables(word) for word in TextBlob(text).words)\n","\n","        if total_sentences == 0 or total_words == 0:\n","            return 0\n","\n","        fk_grade = 0.39 * (total_words / total_sentences) + 11.8 * (total_syllables / total_words) - 15.59\n","        return fk_grade\n","    \n","    def gunning_fog(self, text):\n","        total_sentences = len(TextBlob(text).sentences)\n","        total_words = len(TextBlob(text).words)\n","        complex_words = sum(1 for word in TextBlob(text).words if self.count_syllables(word) > 2)\n","\n","        if total_sentences == 0 or total_words == 0:\n","            return 0\n","\n","        fog_index = 0.4 * ((total_words / total_sentences) + 100 * (complex_words / total_words))\n","        return fog_index\n","    \n","    def calculate_sentiment_scores(self,text):\n","        sentiment_scores = sid.polarity_scores(text)\n","        return sentiment_scores\n","    \n","    def count_difficult_words(self, text, syllable_threshold=3):\n","        words = TextBlob(text).words\n","        difficult_words_count = sum(1 for word in words if self.count_syllables(word) >= syllable_threshold)\n","        return difficult_words_count\n","\n","    def text_cleaning(self, text):\n","        '''\n","        Cleans text into a basic form for NLP. Operations include the following:-\n","        1. Remove special charecters like &, #, etc\n","        2. Removes extra spaces\n","        3. Removes embedded URL links\n","        4. Removes HTML tags\n","        5. Removes emojis\n","\n","        text - Text piece to be cleaned.\n","        '''\n","        template = re.compile(r'https?://\\S+|www\\.\\S+')  # Removes website links\n","        text = template.sub(r'', text)\n","\n","        soup = BeautifulSoup(text, 'lxml')  # Removes HTML tags\n","        only_text = soup.get_text()\n","        text = only_text\n","\n","        emoji_pattern = re.compile(\"[\"\n","                                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                                u\"\\U00002702-\\U000027B0\"\n","                                u\"\\U000024C2-\\U0001F251\"\n","                                \"]+\", flags=re.UNICODE)\n","        text = emoji_pattern.sub(r'', text)\n","\n","        text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) # Remove special Charecters\n","        text = re.sub('\\n+', '\\n', text) \n","        text = re.sub('\\.+', '.', text) \n","        text = re.sub(' +', ' ', text) # Remove Extra Spaces \n","\n","        return text\n","    \n","    def run(self, \n","            prompts: pd.DataFrame,\n","            summaries:pd.DataFrame,\n","            mode:str\n","        ) -> pd.DataFrame:\n","        \n","        # before merge preprocess\n","        prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n","            lambda x: len(word_tokenize(x))\n","        )\n","        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n","            lambda x: word_tokenize(x)\n","        )\n","\n","        summaries[\"summary_length\"] = summaries[\"text\"].apply(\n","            lambda x: len(word_tokenize(x))\n","        )\n","        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n","            lambda x: word_tokenize(x)\n","        )\n","        \n","        # Add prompt tokens into spelling checker dictionary\n","        prompts[\"prompt_tokens\"].apply(\n","            lambda x: self.add_spelling_dictionary(x)\n","        )\n","        \n","        prompts['gunning_fog_prompt'] = prompts['prompt_text'].apply(self.gunning_fog)\n","        prompts['flesch_kincaid_grade_level_prompt'] = prompts['prompt_text'].apply(self.flesch_kincaid_grade_level)\n","        prompts['flesch_reading_ease_prompt'] = prompts['prompt_text'].apply(self.flesch_reading_ease_manual)\n","\n","        \n","#         from IPython.core.debugger import Pdb; Pdb().set_trace()\n","        # fix misspelling\n","        # summaries[\"fixed_summary_text\"] = summaries[\"text\"].progress_apply(\n","        #     lambda x: self.text_cleaning(x)\n","        # )\n","        summaries[\"fixed_summary_text\"] = summaries[\"text\"].progress_apply(\n","            lambda x: self.speller(x)\n","        )\n","        \n","        \n","        # count misspelling\n","        summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n","        \n","        # merge prompts and summaries\n","        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n","        input_df['flesch_reading_ease'] = input_df['text'].apply(self.flesch_reading_ease_manual)\n","        input_df['word_count'] = input_df['text'].apply(lambda x: len(x.split()))\n","        input_df['sentence_length'] = input_df['text'].apply(lambda x: len(x.split('.')))\n","        input_df['vocabulary_richness'] = input_df['text'].apply(lambda x: len(set(x.split())))\n","\n","        input_df['word_count2'] = [len(t.split(' ')) for t in input_df.text]\n","        input_df['num_unq_words']=[len(list(set(x.lower().split(' ')))) for x in input_df.text]\n","        input_df['num_chars']= [len(x) for x in input_df.text]\n","\n","        # Additional features\n","        input_df['avg_word_length'] = input_df['text'].apply(lambda x: np.mean([len(word) for word in x.split()]))\n","        input_df['comma_count'] = input_df['text'].apply(lambda x: x.count(','))\n","        input_df['semicolon_count'] = input_df['text'].apply(lambda x: x.count(';'))\n","\n","        # after merge preprocess\n","        input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n","        \n","        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n","        input_df['bigram_overlap_count'] = input_df.progress_apply(\n","            self.ngram_co_occurrence,args=(2,), axis=1 \n","        )\n","        input_df['bigram_overlap_ratio'] = input_df['bigram_overlap_count'] / (input_df['summary_length'] - 1)\n","        \n","        input_df['trigram_overlap_count'] = input_df.progress_apply(\n","            self.ngram_co_occurrence, args=(3,), axis=1\n","        )\n","        input_df['trigram_overlap_ratio'] = input_df['trigram_overlap_count'] / (input_df['summary_length'] - 2)\n","        \n","        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n","        \n","        input_df['exclamation_count'] = input_df['text'].apply(lambda x: x.count('!'))\n","        input_df['question_count'] = input_df['text'].apply(lambda x: x.count('?'))\n","        input_df['pos_ratios'] = input_df['text'].apply(self.calculate_pos_ratios)\n","\n","        # Convert the dictionary of POS ratios into a single value (mean)\n","        input_df['pos_mean'] = input_df['pos_ratios'].apply(lambda x: np.mean(list(x.values())))\n","        input_df['punctuation_ratios'] = input_df['text'].apply(self.calculate_punctuation_ratios)\n","\n","        # Convert the dictionary of punctuation ratios into a single value (sum)\n","        input_df['punctuation_sum'] = input_df['punctuation_ratios'].apply(lambda x: np.sum(list(x.values())))\n","        input_df['keyword_density'] = input_df.apply(self.calculate_keyword_density, axis=1)\n","        input_df['jaccard_similarity'] = input_df.apply(lambda row: len(set(word_tokenize(row['prompt_text'])) & set(word_tokenize(row['text']))) / len(set(word_tokenize(row['prompt_text'])) | set(word_tokenize(row['text']))), axis=1)\n","        tqdm.pandas(desc=\"Performing Sentiment Analysis\")\n","        input_df[['sentiment_polarity', 'sentiment_subjectivity']] = input_df['text'].progress_apply(\n","            lambda x: pd.Series(self.sentiment_analysis(x))\n","        )\n","        tqdm.pandas(desc=\"Calculating Text Similarity\")\n","        input_df['text_similarity'] = input_df.progress_apply(self.calculate_text_similarity, axis=1)\n","        #Calculate sentiment scores for each row\n","        input_df['sentiment_scores'] = input_df['text'].apply(self.calculate_sentiment_scores)\n","        \n","        input_df['gunning_fog'] = input_df['text'].apply(self.gunning_fog)\n","        input_df['flesch_kincaid_grade_level'] = input_df['text'].apply(self.flesch_kincaid_grade_level)\n","        input_df['count_difficult_words'] = input_df['text'].apply(self.count_difficult_words)\n","\n","        # Convert sentiment_scores into individual columns\n","        sentiment_columns = pd.DataFrame(list(input_df['sentiment_scores']))\n","        input_df = pd.concat([input_df, sentiment_columns], axis=1)\n","        input_df['sentiment_scores_prompt'] = input_df['prompt_text'].apply(self.calculate_sentiment_scores)\n","        # Convert sentiment_scores_prompt into individual columns\n","        sentiment_columns_prompt = pd.DataFrame(list(input_df['sentiment_scores_prompt']))\n","        sentiment_columns_prompt.columns = [col +'_prompt' for col in sentiment_columns_prompt.columns]\n","        input_df = pd.concat([input_df, sentiment_columns_prompt], axis=1)\n","        columns =  ['pos_ratios', 'sentiment_scores', 'punctuation_ratios', 'sentiment_scores_prompt']\n","        cols_to_drop = [col for col in columns if col in input_df.columns]\n","        if cols_to_drop:\n","            input_df = input_df.drop(columns=cols_to_drop)\n","        \n","        print(cols_to_drop)\n","        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n","    "]},{"cell_type":"code","execution_count":179,"metadata":{},"outputs":[],"source":["preprocessor = Preprocessor2(model_name=CFG.model_name)"]},{"cell_type":"markdown","metadata":{},"source":["## Create the train and test sets\n"]},{"cell_type":"code","execution_count":180,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:42:35.231162Z","iopub.status.busy":"2023-10-04T06:42:35.230619Z","iopub.status.idle":"2023-10-04T06:42:35.236187Z","shell.execute_reply":"2023-10-04T06:42:35.235261Z","shell.execute_reply.started":"2023-10-04T06:42:35.231130Z"},"trusted":true},"outputs":[],"source":["if CFG.test_mode : \n","    prompts_train = prompts_train[:12]\n","    prompts_test = prompts_test[:12]\n","    summaries_train = summaries_train[:12]\n","    summaries_test = summaries_test[:12]"]},{"cell_type":"code","execution_count":181,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:42:35.237834Z","iopub.status.busy":"2023-10-04T06:42:35.237577Z","iopub.status.idle":"2023-10-04T06:49:18.913211Z","shell.execute_reply":"2023-10-04T06:49:18.912332Z","shell.execute_reply.started":"2023-10-04T06:42:35.237803Z"},"trusted":true},"outputs":[],"source":["# train = preprocessor.run(prompts_train, summaries_train, mode=\"train\")\n","# test = preprocessor.run(prompts_test, summaries_test, mode=\"test\")"]},{"cell_type":"markdown","metadata":{},"source":["## save train data"]},{"cell_type":"code","execution_count":182,"metadata":{},"outputs":[],"source":["# save data train to csv \n","# train.to_csv(\"input/train_clean_text.csv\", index=False)\n","# load train data\n","# train = pd.read_csv(\"input/train_clean_text.csv\")\n","# train.to_csv(\"input/train_preprocess_2.csv\", index=False)\n","train = pd.read_csv(\"input/train_preprocess_2.csv\")"]},{"cell_type":"code","execution_count":183,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:49:18.916276Z","iopub.status.busy":"2023-10-04T06:49:18.915510Z","iopub.status.idle":"2023-10-04T06:49:18.943400Z","shell.execute_reply":"2023-10-04T06:49:18.942559Z","shell.execute_reply.started":"2023-10-04T06:49:18.916242Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>prompt_id</th>\n","      <th>text</th>\n","      <th>content</th>\n","      <th>wording</th>\n","      <th>summary_length</th>\n","      <th>fixed_summary_text</th>\n","      <th>splling_err_num</th>\n","      <th>prompt_question</th>\n","      <th>prompt_title</th>\n","      <th>...</th>\n","      <th>neu</th>\n","      <th>pos</th>\n","      <th>compound</th>\n","      <th>neg_prompt</th>\n","      <th>neu_prompt</th>\n","      <th>pos_prompt</th>\n","      <th>compound_prompt</th>\n","      <th>fold</th>\n","      <th>wording_pred</th>\n","      <th>content_pred</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000e8c3c7ddb</td>\n","      <td>814d6b</td>\n","      <td>The third wave was an experimentto see how peo...</td>\n","      <td>0.205683</td>\n","      <td>0.380538</td>\n","      <td>64</td>\n","      <td>The third wave was an experimental see how peo...</td>\n","      <td>5</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>...</td>\n","      <td>0.832</td>\n","      <td>0.135</td>\n","      <td>0.7845</td>\n","      <td>0.027</td>\n","      <td>0.873</td>\n","      <td>0.100</td>\n","      <td>0.9915</td>\n","      <td>3.0</td>\n","      <td>-0.222891</td>\n","      <td>0.840660</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0020ae56ffbf</td>\n","      <td>ebad26</td>\n","      <td>They would rub it up with soda to make the sme...</td>\n","      <td>-0.548304</td>\n","      <td>0.506755</td>\n","      <td>54</td>\n","      <td>They would rub it up with soda to make the sme...</td>\n","      <td>2</td>\n","      <td>Summarize the various ways the factory would u...</td>\n","      <td>Excerpt from The Jungle</td>\n","      <td>...</td>\n","      <td>0.946</td>\n","      <td>0.054</td>\n","      <td>0.4310</td>\n","      <td>0.086</td>\n","      <td>0.879</td>\n","      <td>0.035</td>\n","      <td>-0.9949</td>\n","      <td>2.0</td>\n","      <td>-0.800661</td>\n","      <td>-0.270180</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>004e978e639e</td>\n","      <td>3b9047</td>\n","      <td>In Egypt, there were many occupations and soci...</td>\n","      <td>3.128928</td>\n","      <td>4.231226</td>\n","      <td>269</td>\n","      <td>In Egypt, there were many occupations and soci...</td>\n","      <td>32</td>\n","      <td>In complete sentences, summarize the structure...</td>\n","      <td>Egyptian Social Structure</td>\n","      <td>...</td>\n","      <td>0.814</td>\n","      <td>0.139</td>\n","      <td>0.9725</td>\n","      <td>0.063</td>\n","      <td>0.845</td>\n","      <td>0.092</td>\n","      <td>0.9283</td>\n","      <td>1.0</td>\n","      <td>2.513938</td>\n","      <td>2.132750</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>005ab0199905</td>\n","      <td>3b9047</td>\n","      <td>The highest class was Pharaohs these people we...</td>\n","      <td>-0.210614</td>\n","      <td>-0.471415</td>\n","      <td>28</td>\n","      <td>The highest class was Pharaohs these people we...</td>\n","      <td>5</td>\n","      <td>In complete sentences, summarize the structure...</td>\n","      <td>Egyptian Social Structure</td>\n","      <td>...</td>\n","      <td>1.000</td>\n","      <td>0.000</td>\n","      <td>0.0000</td>\n","      <td>0.063</td>\n","      <td>0.845</td>\n","      <td>0.092</td>\n","      <td>0.9283</td>\n","      <td>1.0</td>\n","      <td>-1.081251</td>\n","      <td>-1.108729</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0070c9e7af47</td>\n","      <td>814d6b</td>\n","      <td>The Third Wave developed  rapidly because the ...</td>\n","      <td>3.272894</td>\n","      <td>3.219757</td>\n","      <td>232</td>\n","      <td>The Third Wave developed  rapidly because the ...</td>\n","      <td>29</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>...</td>\n","      <td>0.896</td>\n","      <td>0.104</td>\n","      <td>0.9696</td>\n","      <td>0.027</td>\n","      <td>0.873</td>\n","      <td>0.100</td>\n","      <td>0.9915</td>\n","      <td>3.0</td>\n","      <td>2.375733</td>\n","      <td>2.604945</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 55 columns</p>\n","</div>"],"text/plain":["     student_id prompt_id                                               text  \\\n","0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n","1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme...   \n","2  004e978e639e    3b9047  In Egypt, there were many occupations and soci...   \n","3  005ab0199905    3b9047  The highest class was Pharaohs these people we...   \n","4  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...   \n","\n","    content   wording  summary_length  \\\n","0  0.205683  0.380538              64   \n","1 -0.548304  0.506755              54   \n","2  3.128928  4.231226             269   \n","3 -0.210614 -0.471415              28   \n","4  3.272894  3.219757             232   \n","\n","                                  fixed_summary_text  splling_err_num  \\\n","0  The third wave was an experimental see how peo...                5   \n","1  They would rub it up with soda to make the sme...                2   \n","2  In Egypt, there were many occupations and soci...               32   \n","3  The highest class was Pharaohs these people we...                5   \n","4  The Third Wave developed  rapidly because the ...               29   \n","\n","                                     prompt_question  \\\n","0  Summarize how the Third Wave developed over su...   \n","1  Summarize the various ways the factory would u...   \n","2  In complete sentences, summarize the structure...   \n","3  In complete sentences, summarize the structure...   \n","4  Summarize how the Third Wave developed over su...   \n","\n","                prompt_title  ...    neu    pos  compound  neg_prompt  \\\n","0             The Third Wave  ...  0.832  0.135    0.7845       0.027   \n","1    Excerpt from The Jungle  ...  0.946  0.054    0.4310       0.086   \n","2  Egyptian Social Structure  ...  0.814  0.139    0.9725       0.063   \n","3  Egyptian Social Structure  ...  1.000  0.000    0.0000       0.063   \n","4             The Third Wave  ...  0.896  0.104    0.9696       0.027   \n","\n","   neu_prompt  pos_prompt  compound_prompt  fold  wording_pred  content_pred  \n","0       0.873       0.100           0.9915   3.0     -0.222891      0.840660  \n","1       0.879       0.035          -0.9949   2.0     -0.800661     -0.270180  \n","2       0.845       0.092           0.9283   1.0      2.513938      2.132750  \n","3       0.845       0.092           0.9283   1.0     -1.081251     -1.108729  \n","4       0.873       0.100           0.9915   3.0      2.375733      2.604945  \n","\n","[5 rows x 55 columns]"]},"execution_count":183,"metadata":{},"output_type":"execute_result"}],"source":["gkf = GroupKFold(n_splits=CFG.n_splits)\n","\n","for i, (_, val_index) in enumerate(gkf.split(train, groups=train[\"prompt_id\"])):\n","    train.loc[val_index, \"fold\"] = i\n","\n","train.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Model Function Definition"]},{"cell_type":"code","execution_count":184,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:49:18.944706Z","iopub.status.busy":"2023-10-04T06:49:18.944482Z","iopub.status.idle":"2023-10-04T06:49:18.951510Z","shell.execute_reply":"2023-10-04T06:49:18.950756Z","shell.execute_reply.started":"2023-10-04T06:49:18.944677Z"},"trusted":true},"outputs":[],"source":["def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    rmse = mean_squared_error(labels, predictions, squared=False)\n","    return {\"rmse\": rmse}\n","\n","def compute_mcrmse(eval_pred):\n","    \"\"\"\n","    Calculates mean columnwise root mean squared error\n","    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n","    \"\"\"\n","    preds, labels = eval_pred\n","\n","    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n","    mcrmse = np.mean(col_rmse)\n","\n","    return {\n","        \"content_rmse\": col_rmse[0],\n","        \"wording_rmse\": col_rmse[1],\n","        \"mcrmse\": mcrmse,\n","    }\n","\n","def compt_score(content_true, content_pred, wording_true, wording_pred):\n","    content_score = mean_squared_error(content_true, content_pred)**(1/2)\n","    wording_score = mean_squared_error(wording_true, wording_pred)**(1/2)\n","    \n","    return (content_score + wording_score)/2"]},{"cell_type":"markdown","metadata":{},"source":["## Deberta Regressor"]},{"cell_type":"code","execution_count":185,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:49:18.953587Z","iopub.status.busy":"2023-10-04T06:49:18.953198Z","iopub.status.idle":"2023-10-04T06:49:18.971775Z","shell.execute_reply":"2023-10-04T06:49:18.970910Z","shell.execute_reply.started":"2023-10-04T06:49:18.953557Z"},"trusted":true},"outputs":[],"source":["class ScoreRegressor:\n","    def __init__(self, \n","                model_name: str,\n","                model_dir: str,\n","                target: list,\n","                hidden_dropout_prob: float,\n","                attention_probs_dropout_prob: float,\n","                max_length: int,\n","                ):\n","        self.inputs = [\"prompt_text\", \"prompt_title\", \"prompt_question\", \"fixed_summary_text\"] # fix summary text have prompt text in it \n","        self.input_col = \"input\"\n","        \n","        self.text_cols = [self.input_col] \n","        self.target = target\n","        self.target_cols = target\n","\n","        self.model_name = model_name\n","        lr = str(CFG.learning_rate).replace(\".\", \"\")\n","        self.model_dir = model_dir\n","        self.max_length = max_length\n","        \n","        self.tokenizer = AutoTokenizer.from_pretrained(f\"input/{model_name}\")\n","        self.model_config = AutoConfig.from_pretrained(f\"input/{model_name}\" )\n","        # print(self.model_config)\n","        self.model_config.update({\n","            \"hidden_dropout_prob\": hidden_dropout_prob,\n","            \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n","            \"num_labels\": 2,\n","            \"problem_type\": \"regression\",\n","        })\n","        seed_everything(seed=42)\n","\n","        self.data_collator = DataCollatorWithPadding(\n","            tokenizer=self.tokenizer\n","        )\n","\n","\n","    def tokenize_function(self, examples: pd.DataFrame):\n","        # labels = ['content' , 'wording']\n","        # print('labels', labels)\n","        tokenized = self.tokenizer(examples[self.input_col],\n","                         padding=False,\n","                         truncation=True,\n","                         max_length=self.max_length)\n","        return {\n","            **tokenized,\n","            \"labels\": [examples['content'], examples['wording']],\n","        }\n","    \n","    def tokenize_function_test(self, examples: pd.DataFrame):\n","        tokenized = self.tokenizer(examples[self.input_col],\n","                         padding=False,\n","                         truncation=True,\n","                         max_length=self.max_length)\n","        return tokenized\n","        \n","    def train(self, \n","            fold: int,\n","            train_df: pd.DataFrame,\n","            valid_df: pd.DataFrame,\n","            batch_size: int,\n","            learning_rate: float,\n","            weight_decay: float,\n","            num_train_epochs: float,\n","            save_steps: int,\n","        ) -> None:\n","        \"\"\"fine-tuning\"\"\"\n","        \n","        sep = self.tokenizer.sep_token\n","        # print('sep', sep)\n","        train_df[self.input_col] = (\n","                    train_df[\"prompt_title\"] + sep \n","                    + train_df[\"prompt_question\"] + sep \n","                    + train_df[\"fixed_summary_text\"]\n","                  )\n","\n","        valid_df[self.input_col] = (\n","                    valid_df[\"prompt_title\"] + sep \n","                    + valid_df[\"prompt_question\"] + sep \n","                    + valid_df[\"fixed_summary_text\"]\n","                  )\n","        # filter train_df with input_col have more than 5000 tokens\n","        # print('create train and val data frame ')\n","        # print('self.target_cols', self.target_cols)\n","        # print('self.input_col', self.input_col)\n","        train_df = train_df[[self.input_col] + self.target_cols]\n","        valid_df = valid_df[[self.input_col] + self.target_cols]\n","        \n","        model_content = AutoModelForSequenceClassification.from_pretrained(\n","            f\"input/{self.model_name}\", \n","            config=self.model_config\n","        )\n","\n","        train_dataset = Dataset.from_pandas(train_df, preserve_index=False) \n","        val_dataset = Dataset.from_pandas(valid_df, preserve_index=False) \n","        train_tokenized_datasets = train_dataset.map(self.tokenize_function, batched=True)\n","        val_tokenized_datasets = val_dataset.map(self.tokenize_function, batched=True)\n","        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n","        # print('model_fold_dir', model_fold_dir)\n","        training_args = TrainingArguments(\n","            output_dir=model_fold_dir,\n","            load_best_model_at_end=True, # select best model\n","            learning_rate=learning_rate,\n","            per_device_train_batch_size=batch_size,\n","            per_device_eval_batch_size=batch_size,\n","            num_train_epochs=num_train_epochs,\n","            weight_decay=weight_decay,\n","            report_to='none',\n","            greater_is_better=False,\n","            save_strategy=\"steps\",\n","            evaluation_strategy=\"steps\",\n","            eval_steps=save_steps,\n","            save_steps=save_steps,\n","            metric_for_best_model=\"rmse\",\n","            save_total_limit=1\n","        )\n","        # print('define trainer')\n","        trainer = Trainer(\n","            model=model_content,\n","            args=training_args,\n","            train_dataset=train_tokenized_datasets,\n","            eval_dataset=val_tokenized_datasets,\n","            tokenizer=self.tokenizer,\n","            compute_metrics=compute_metrics,\n","            data_collator=self.data_collator\n","        )\n","        print('start training')\n","        # print('trainer.train_dataset[0]' , trainer.train_dataset[0])\n","        trainer.train()\n","        print('finish training')\n","        model_content.save_pretrained(self.model_dir)\n","        self.tokenizer.save_pretrained(self.model_dir)\n","\n","        \n","    def predict(self, \n","                test_df: pd.DataFrame,\n","                fold: int,\n","               ):\n","        \"\"\"predict content score\"\"\"\n","        \n","        sep = self.tokenizer.sep_token\n","        in_text = (\n","                    test_df[\"prompt_title\"] + sep \n","                    + test_df[\"prompt_question\"] + sep \n","                    + test_df[\"fixed_summary_text\"]\n","                  )\n","        test_df[self.input_col] = in_text\n","\n","        test_ = test_df[[self.input_col]]\n","    \n","        test_dataset = Dataset.from_pandas(test_, preserve_index=False) \n","        test_tokenized_dataset = test_dataset.map(self.tokenize_function_test, batched=True)\n","\n","        model_content = AutoModelForSequenceClassification.from_pretrained(f\"{self.model_dir}\")\n","        model_content.eval()\n","        \n","        # eg. \"bert/fold_0/\"\n","        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n","        # print(\"model_fold_dir\",model_fold_dir)\n","        test_args = TrainingArguments(\n","            output_dir=model_fold_dir,\n","            do_train = False,\n","            do_predict = True,\n","            per_device_eval_batch_size = CFG.batch_size,   \n","            dataloader_drop_last = False,\n","        )\n","\n","        # init trainer\n","        infer_content = Trainer(\n","                      model = model_content, \n","                      tokenizer=self.tokenizer,\n","                      data_collator=self.data_collator,\n","                      args = test_args)\n","\n","        preds = infer_content.predict(test_tokenized_dataset)[0]\n","\n","        return preds"]},{"cell_type":"markdown","metadata":{},"source":["## Train by fold function\n"]},{"cell_type":"code","execution_count":186,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:49:18.973809Z","iopub.status.busy":"2023-10-04T06:49:18.973053Z","iopub.status.idle":"2023-10-04T06:49:18.989797Z","shell.execute_reply":"2023-10-04T06:49:18.988950Z","shell.execute_reply.started":"2023-10-04T06:49:18.973779Z"},"trusted":true},"outputs":[],"source":["def validate(\n","    train_df: pd.DataFrame,\n","    target:str,\n","    save_each_model: bool,\n","    model_name: str,\n","    model_dir_base: str,\n","    hidden_dropout_prob: float,\n","    attention_probs_dropout_prob: float,\n","    max_length : int\n","    ) -> pd.DataFrame:\n","    \"\"\"predict oof data\"\"\"\n","    for fold in range(CFG.n_splits):\n","        # print(f\"fold {fold}:\")\n","        \n","        valid_data = train_df[train_df[\"fold\"] == fold]\n","        \n","        if save_each_model == True:\n","            model_dir =  f\"{target}/{model_dir_base}/fold_{fold}\"\n","        else: \n","            model_dir =  f\"{model_dir_base}/fold_{fold}\"\n","        csr = ScoreRegressor(\n","            model_name=model_name,\n","            target=target,\n","            model_dir = model_dir,\n","            hidden_dropout_prob=hidden_dropout_prob,\n","            attention_probs_dropout_prob=attention_probs_dropout_prob,\n","            max_length=max_length,\n","           )\n","        \n","        pred = csr.predict(\n","            test_df=valid_data, \n","            fold=fold\n","        )\n","        # print('pred shape', pred.shape)\n","        train_df.loc[valid_data.index, f\"wording_pred\"] = pred[:,0]\n","        train_df.loc[valid_data.index, f\"content_pred\"] = pred[:,1]\n","\n","    return train_df\n","    \n","def predict(\n","    test_df: pd.DataFrame,\n","    target:str,\n","    save_each_model: bool,\n","    model_name: str,\n","    model_dir_base: str,\n","    hidden_dropout_prob: float,\n","    attention_probs_dropout_prob: float,\n","    max_length : int\n","    ):\n","    \"\"\"predict using mean folds\"\"\"\n","    for fold in range(CFG.n_splits):\n","        # print(f\"fold {fold}:\")\n","        \n","        if save_each_model == True:\n","            model_dir =  f\"{target}/{model_dir_base}/fold_{fold}\"\n","        else: \n","            model_dir =  f\"{model_dir_base}/fold_{fold}\"\n","        csr = ScoreRegressor(\n","            model_name=model_name,\n","            target=target,\n","            model_dir = model_dir, \n","            hidden_dropout_prob=hidden_dropout_prob,\n","            attention_probs_dropout_prob=attention_probs_dropout_prob,\n","            max_length=max_length,\n","           )\n","        \n","        pred = csr.predict(\n","            test_df=test_df, \n","            fold=fold\n","        )\n","        \n","        # test_df[f\"{target}_pred_{fold}\"] = pred\n","        test_df[f\"wording_pred_{fold}\"] = pred[:,0]\n","        test_df[f\"content_pred_{fold}\"] = pred[:,1]\n","        \n","    # test_df[f\"{target}\"] = test_df[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n","    test_df[f\"wording_pred\"] = test_df[[f\"wording_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n","    test_df[f\"content_pred\"] = test_df[[f\"content_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n","    return test_df\n","targets =  [\"content\", \"wording\"]\n"]},{"cell_type":"markdown","metadata":{},"source":["## Infer\n","\n"]},{"cell_type":"code","execution_count":187,"metadata":{},"outputs":[],"source":["ensembling_results_val = pd.read_csv(\"ensembling_results_val.csv\")\n","ensembling_results_test = pd.DataFrame()\n"]},{"cell_type":"code","execution_count":188,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:49:18.991423Z","iopub.status.busy":"2023-10-04T06:49:18.990863Z","iopub.status.idle":"2023-10-04T06:51:09.807207Z","shell.execute_reply":"2023-10-04T06:51:09.806306Z","shell.execute_reply.started":"2023-10-04T06:49:18.991394Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["debertav3large_lr6e-06_att_0006\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["cv content rmse: 0.6359938629366321\n","cv wording rmse: 0.7409556596518395\n","done validate\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["done predict\n","debertav3large_lr7e-06_att_0006\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["cv content rmse: 0.5964487706859211\n","cv wording rmse: 0.7580703679131006\n","done validate\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["done predict\n","debertav3large_lr8e-06_att_0006\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["cv content rmse: 0.5966513036338134\n","cv wording rmse: 0.7539435834830153\n","done validate\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["done predict\n"]}],"source":["dem = 0\n","if CFG.infer_mode:\n","    for model_dir in CFG.list_model_infer:\n","        if CFG.number_base_model > 0:\n","            print(\"percent of model\", dem/CFG.number_base_model)\n","        print(model_dir)    \n","        dem = dem +1 \n","        if dem >= CFG.number_base_model:\n","            CFG.batch_size = 16\n","            CFG.max_length = 1462\n","            CFG.model_name = \"debertav3large\"\n","        train = validate(\n","            train,\n","            target=targets,\n","            save_each_model=False,\n","            model_name=CFG.model_name,\n","            model_dir_base = model_dir,\n","            hidden_dropout_prob=CFG.hidden_dropout_prob,\n","            attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n","            max_length=CFG.max_length\n","        )\n","        for target in targets:\n","            rmse = mean_squared_error(train[target], train[f\"{target}_pred\"], squared=False)\n","            print(f\"cv {target} rmse: {rmse}\")\n","        print('done validate')\n","        test = predict(\n","            test,\n","            target=targets,\n","            save_each_model=False,\n","            model_name=CFG.model_name,\n","            model_dir_base = model_dir,\n","            hidden_dropout_prob=CFG.hidden_dropout_prob,\n","            attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n","            max_length=CFG.max_length\n","        )\n","        print('done predict')\n","        # add wording_pred and content_pred to ensembling_results\n","        ensembling_results_val[f\"{model_dir}_wording_pred\"] = train[\"wording_pred\"]\n","        ensembling_results_val[f\"{model_dir}_content_pred\"] = train[\"content_pred\"]\n","        ensembling_results_test[f\"{model_dir}_wording_pred\"] = test[\"wording_pred\"]\n","        ensembling_results_test[f\"{model_dir}_content_pred\"] = test[\"content_pred\"]\n","        # print('ensembling_results_val \\n', ensembling_results_val.head() )\n","        "]},{"cell_type":"markdown","metadata":{},"source":["## LGBM model"]},{"cell_type":"code","execution_count":384,"metadata":{},"outputs":[],"source":["# save ensembling_results_val and ensembling_results_test\n","ensembling_results_val.to_csv(\"ensembling_results_val.csv\", index=False)\n","# ensembling_results_test.to_csv(\"ensembling_results_test.csv\", index=False)\n","# load ensembling_results_val and ensembling_results_test\n","# ensembling_results_val = pd.read_csv(\"ensembling_results_val.csv\")\n","# ensembling_results_test = pd.read_csv(\"ensembling_results_test.csv\")"]},{"cell_type":"code","execution_count":383,"metadata":{},"outputs":[{"data":{"text/plain":["(7165, 46)"]},"execution_count":383,"metadata":{},"output_type":"execute_result"}],"source":["ensembling_results_val.shape"]},{"cell_type":"code","execution_count":191,"metadata":{},"outputs":[],"source":["# replace upload_model/ with \"\"\n","# ensembling_results_val = ensembling_results_val.rename(columns=lambda x: x.replace(\"upload_model/\", \"\"))"]},{"cell_type":"code","execution_count":192,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>upload_model/debertav3base_lr5e-05_wording_pred</th>\n","      <th>upload_model/debertav3base_lr5e-05_content_pred</th>\n","      <th>upload_model/debertav3base_lr15e-05_wording_pred</th>\n","      <th>upload_model/debertav3base_lr15e-05_content_pred</th>\n","      <th>upload_model/debertav3base_lr17e-05_wording_pred</th>\n","      <th>upload_model/debertav3base_lr17e-05_content_pred</th>\n","      <th>upload_model/debertav3base_lr18e-05_wording_pred</th>\n","      <th>upload_model/debertav3base_lr18e-05_content_pred</th>\n","      <th>upload_model/debertav3base_lr21e-05_wording_pred</th>\n","      <th>upload_model/debertav3base_lr21e-05_content_pred</th>\n","      <th>...</th>\n","      <th>debertav3large_lr17e-05_att_0007_wording_pred</th>\n","      <th>debertav3large_lr17e-05_att_0007_content_pred</th>\n","      <th>debertav3large_lr18e-05_att_0007_wording_pred</th>\n","      <th>debertav3large_lr18e-05_att_0007_content_pred</th>\n","      <th>debertav3large_lr6e-06_att_0006_wording_pred</th>\n","      <th>debertav3large_lr6e-06_att_0006_content_pred</th>\n","      <th>debertav3large_lr7e-06_att_0006_wording_pred</th>\n","      <th>debertav3large_lr7e-06_att_0006_content_pred</th>\n","      <th>debertav3large_lr8e-06_att_0006_wording_pred</th>\n","      <th>debertav3large_lr8e-06_att_0006_content_pred</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.289577</td>\n","      <td>0.754494</td>\n","      <td>0.255495</td>\n","      <td>0.757381</td>\n","      <td>0.253911</td>\n","      <td>0.798586</td>\n","      <td>0.238143</td>\n","      <td>0.805690</td>\n","      <td>0.226564</td>\n","      <td>0.868932</td>\n","      <td>...</td>\n","      <td>-0.035948</td>\n","      <td>0.880852</td>\n","      <td>-0.025384</td>\n","      <td>0.916015</td>\n","      <td>0.058602</td>\n","      <td>0.790532</td>\n","      <td>0.029472</td>\n","      <td>0.575491</td>\n","      <td>0.008973</td>\n","      <td>0.591914</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.702410</td>\n","      <td>-0.433037</td>\n","      <td>-0.801842</td>\n","      <td>-0.464458</td>\n","      <td>-0.772340</td>\n","      <td>-0.413345</td>\n","      <td>-0.665685</td>\n","      <td>-0.327464</td>\n","      <td>-0.641115</td>\n","      <td>-0.371242</td>\n","      <td>...</td>\n","      <td>-0.893634</td>\n","      <td>-0.387197</td>\n","      <td>-1.166269</td>\n","      <td>-0.696791</td>\n","      <td>-0.703040</td>\n","      <td>-0.181907</td>\n","      <td>-0.972083</td>\n","      <td>-0.664230</td>\n","      <td>-1.004822</td>\n","      <td>-0.483753</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.476874</td>\n","      <td>1.968442</td>\n","      <td>2.216656</td>\n","      <td>2.133213</td>\n","      <td>2.264356</td>\n","      <td>2.114351</td>\n","      <td>2.413981</td>\n","      <td>2.368245</td>\n","      <td>2.114522</td>\n","      <td>2.113158</td>\n","      <td>...</td>\n","      <td>2.042683</td>\n","      <td>2.011548</td>\n","      <td>2.101691</td>\n","      <td>2.138166</td>\n","      <td>2.485754</td>\n","      <td>1.965636</td>\n","      <td>2.735226</td>\n","      <td>2.239623</td>\n","      <td>2.414855</td>\n","      <td>1.586793</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-1.046302</td>\n","      <td>-0.884815</td>\n","      <td>-1.117746</td>\n","      <td>-0.960289</td>\n","      <td>-1.006900</td>\n","      <td>-0.931899</td>\n","      <td>-0.985633</td>\n","      <td>-0.782978</td>\n","      <td>-1.084898</td>\n","      <td>-0.928009</td>\n","      <td>...</td>\n","      <td>-1.103294</td>\n","      <td>-0.969418</td>\n","      <td>-1.277837</td>\n","      <td>-1.192987</td>\n","      <td>-0.931144</td>\n","      <td>-0.856378</td>\n","      <td>-0.955553</td>\n","      <td>-0.831370</td>\n","      <td>-0.958076</td>\n","      <td>-0.937072</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2.218238</td>\n","      <td>1.984394</td>\n","      <td>2.125614</td>\n","      <td>2.388381</td>\n","      <td>2.147007</td>\n","      <td>2.516407</td>\n","      <td>2.084431</td>\n","      <td>2.448008</td>\n","      <td>2.143604</td>\n","      <td>2.350667</td>\n","      <td>...</td>\n","      <td>1.824292</td>\n","      <td>2.288516</td>\n","      <td>2.421074</td>\n","      <td>2.707077</td>\n","      <td>2.164462</td>\n","      <td>2.571666</td>\n","      <td>2.270405</td>\n","      <td>2.407635</td>\n","      <td>2.479643</td>\n","      <td>2.635478</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 46 columns</p>\n","</div>"],"text/plain":["   upload_model/debertav3base_lr5e-05_wording_pred  \\\n","0                                         0.289577   \n","1                                        -0.702410   \n","2                                         2.476874   \n","3                                        -1.046302   \n","4                                         2.218238   \n","\n","   upload_model/debertav3base_lr5e-05_content_pred  \\\n","0                                         0.754494   \n","1                                        -0.433037   \n","2                                         1.968442   \n","3                                        -0.884815   \n","4                                         1.984394   \n","\n","   upload_model/debertav3base_lr15e-05_wording_pred  \\\n","0                                          0.255495   \n","1                                         -0.801842   \n","2                                          2.216656   \n","3                                         -1.117746   \n","4                                          2.125614   \n","\n","   upload_model/debertav3base_lr15e-05_content_pred  \\\n","0                                          0.757381   \n","1                                         -0.464458   \n","2                                          2.133213   \n","3                                         -0.960289   \n","4                                          2.388381   \n","\n","   upload_model/debertav3base_lr17e-05_wording_pred  \\\n","0                                          0.253911   \n","1                                         -0.772340   \n","2                                          2.264356   \n","3                                         -1.006900   \n","4                                          2.147007   \n","\n","   upload_model/debertav3base_lr17e-05_content_pred  \\\n","0                                          0.798586   \n","1                                         -0.413345   \n","2                                          2.114351   \n","3                                         -0.931899   \n","4                                          2.516407   \n","\n","   upload_model/debertav3base_lr18e-05_wording_pred  \\\n","0                                          0.238143   \n","1                                         -0.665685   \n","2                                          2.413981   \n","3                                         -0.985633   \n","4                                          2.084431   \n","\n","   upload_model/debertav3base_lr18e-05_content_pred  \\\n","0                                          0.805690   \n","1                                         -0.327464   \n","2                                          2.368245   \n","3                                         -0.782978   \n","4                                          2.448008   \n","\n","   upload_model/debertav3base_lr21e-05_wording_pred  \\\n","0                                          0.226564   \n","1                                         -0.641115   \n","2                                          2.114522   \n","3                                         -1.084898   \n","4                                          2.143604   \n","\n","   upload_model/debertav3base_lr21e-05_content_pred  ...  \\\n","0                                          0.868932  ...   \n","1                                         -0.371242  ...   \n","2                                          2.113158  ...   \n","3                                         -0.928009  ...   \n","4                                          2.350667  ...   \n","\n","   debertav3large_lr17e-05_att_0007_wording_pred  \\\n","0                                      -0.035948   \n","1                                      -0.893634   \n","2                                       2.042683   \n","3                                      -1.103294   \n","4                                       1.824292   \n","\n","   debertav3large_lr17e-05_att_0007_content_pred  \\\n","0                                       0.880852   \n","1                                      -0.387197   \n","2                                       2.011548   \n","3                                      -0.969418   \n","4                                       2.288516   \n","\n","   debertav3large_lr18e-05_att_0007_wording_pred  \\\n","0                                      -0.025384   \n","1                                      -1.166269   \n","2                                       2.101691   \n","3                                      -1.277837   \n","4                                       2.421074   \n","\n","   debertav3large_lr18e-05_att_0007_content_pred  \\\n","0                                       0.916015   \n","1                                      -0.696791   \n","2                                       2.138166   \n","3                                      -1.192987   \n","4                                       2.707077   \n","\n","   debertav3large_lr6e-06_att_0006_wording_pred  \\\n","0                                      0.058602   \n","1                                     -0.703040   \n","2                                      2.485754   \n","3                                     -0.931144   \n","4                                      2.164462   \n","\n","   debertav3large_lr6e-06_att_0006_content_pred  \\\n","0                                      0.790532   \n","1                                     -0.181907   \n","2                                      1.965636   \n","3                                     -0.856378   \n","4                                      2.571666   \n","\n","   debertav3large_lr7e-06_att_0006_wording_pred  \\\n","0                                      0.029472   \n","1                                     -0.972083   \n","2                                      2.735226   \n","3                                     -0.955553   \n","4                                      2.270405   \n","\n","   debertav3large_lr7e-06_att_0006_content_pred  \\\n","0                                      0.575491   \n","1                                     -0.664230   \n","2                                      2.239623   \n","3                                     -0.831370   \n","4                                      2.407635   \n","\n","   debertav3large_lr8e-06_att_0006_wording_pred  \\\n","0                                      0.008973   \n","1                                     -1.004822   \n","2                                      2.414855   \n","3                                     -0.958076   \n","4                                      2.479643   \n","\n","   debertav3large_lr8e-06_att_0006_content_pred  \n","0                                      0.591914  \n","1                                     -0.483753  \n","2                                      1.586793  \n","3                                     -0.937072  \n","4                                      2.635478  \n","\n","[5 rows x 46 columns]"]},"execution_count":192,"metadata":{},"output_type":"execute_result"}],"source":["ensembling_results_val.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Find the best weight with optuna"]},{"cell_type":"code","execution_count":425,"metadata":{},"outputs":[],"source":["# weight_for_model = {}\n","# scale = 0 \n"]},{"cell_type":"code","execution_count":426,"metadata":{},"outputs":[],"source":["weight_for_model = {'upload_model/debertav3base_lr17e-05': 0.10605670751223599,\n"," 'upload_model/debertav3base_lr18e-05': 0.8151372494853444,\n"," 'upload_model/debertav3base_lr21e-05': -0.17229060080581526,\n"," 'upload_model/debertav3base_lr22e-05': 0.46124428559398767,\n"," 'upload_model/debertav3large_lr12e-05': 0.5213469207997752,\n"," 'upload_model/debertav3large_lr13e-05': 0.33587208988869377,\n"," 'debertav3large_lr8e-06_att_0007': 0.14044911254263082,\n"," 'debertav3large_lr11e-05_att_0007': 0.7504131871645143,\n"," 'debertav3large_lr13e-05_att_0007': 0.6190990620288409,\n"," 'debertav3large_lr14e-05_att_0007': -0.12134513614876408,\n"," 'debertav3large_lr17e-05_att_0007': 0.8917623536805175,\n"," 'debertav3large_lr18e-05_att_0007': -0.07388702753834085,\n"," 'debertav3large_lr7e-06_att_0006': 0.7887573166417163,\n"," 'debertav3large_lr8e-06_att_0006': 0.7195186292932556}\n","scale = sum(weight_for_model.values())\n","CFG.list_model_infer = list(weight_for_model.keys())"]},{"cell_type":"code","execution_count":427,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["upload_model/debertav3base_lr17e-05 mcrmse: 0.5543694149276929\n","upload_model/debertav3base_lr18e-05 mcrmse: 0.5526207597677534\n","upload_model/debertav3base_lr21e-05 mcrmse: 0.553756334918185\n","upload_model/debertav3base_lr22e-05 mcrmse: 0.5480178090069433\n","upload_model/debertav3large_lr12e-05 mcrmse: 0.5531885292652218\n","upload_model/debertav3large_lr13e-05 mcrmse: 0.5520867504987613\n","debertav3large_lr8e-06_att_0007 mcrmse: 0.5257363371291668\n","debertav3large_lr11e-05_att_0007 mcrmse: 0.5395042177968217\n","debertav3large_lr13e-05_att_0007 mcrmse: 0.5502121290213762\n","debertav3large_lr14e-05_att_0007 mcrmse: 0.5392729849094001\n","debertav3large_lr17e-05_att_0007 mcrmse: 0.5517737704359214\n","debertav3large_lr18e-05_att_0007 mcrmse: 0.5563119146067961\n","debertav3large_lr7e-06_att_0006 mcrmse: 0.5199209254995885\n","debertav3large_lr8e-06_att_0006 mcrmse: 0.5229320451563924\n"]}],"source":["for model in CFG.list_model_infer:\n","    results = ensembling_results_val[[f\"{model}_wording_pred\", f\"{model}_content_pred\"]]\n","    mcrmse = compute_mcrmse((results.values, train[targets].values))\n","    print(f\"{model} mcrmse: {mcrmse['mcrmse']}\")\n","    # weight_for_model[model] = 1 / mcrmse[\"mcrmse\"]\n","    # scale = scale + weight_for_model[model]\n","    "]},{"cell_type":"code","execution_count":428,"metadata":{},"outputs":[],"source":["# for model in CFG.list_model_infer:\n","#     weight_for_model[model] = 0\n","# scale = np.sum(list(weight_for_model.values()))"]},{"cell_type":"code","execution_count":429,"metadata":{},"outputs":[],"source":["train[f\"wording_pred\"] = np.sum([ensembling_results_val[f\"{model}_wording_pred\"] * weight_for_model[model] for model in CFG.list_model_infer], axis=0) / scale    \n","train[f\"content_pred\"] = np.sum([ensembling_results_val[f\"{model}_content_pred\"] * weight_for_model[model] for model in CFG.list_model_infer], axis=0) / scale\n","# test[f\"wording_pred\"] = np.sum([ensembling_results_test[f\"{model}_wording_pred\"] * weight_for_model[model] for model in CFG.list_model_infer], axis=0) / scale\n","# test[f\"content_pred\"] = np.sum([ensembling_results_test[f\"{model}_content_pred\"] * weight_for_model[model] for model in CFG.list_model_infer], axis=0) / scale\n"]},{"cell_type":"code","execution_count":430,"metadata":{},"outputs":[],"source":["# save train and test\n","# train.to_csv(\"input/train.csv\", index=False)\n","# load train and test\n","# test = pd.read_csv(\"input/test.csv\")"]},{"cell_type":"code","execution_count":431,"metadata":{},"outputs":[],"source":["# train.head()"]},{"cell_type":"code","execution_count":432,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:09.809377Z","iopub.status.busy":"2023-10-04T06:51:09.808548Z","iopub.status.idle":"2023-10-04T06:51:09.814418Z","shell.execute_reply":"2023-10-04T06:51:09.813556Z","shell.execute_reply.started":"2023-10-04T06:51:09.809343Z"},"trusted":true},"outputs":[],"source":["targets = [\"content\", \"wording\"]\n","\n","drop_columns = [\"fold\", \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n","                \"prompt_question\", \"prompt_title\", \n","                \"prompt_text\"\n","               ] + targets"]},{"cell_type":"code","execution_count":433,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:09.818456Z","iopub.status.busy":"2023-10-04T06:51:09.817994Z","iopub.status.idle":"2023-10-04T06:51:10.548361Z","shell.execute_reply":"2023-10-04T06:51:10.547516Z","shell.execute_reply.started":"2023-10-04T06:51:09.818419Z"},"trusted":true},"outputs":[],"source":["def create_model_dict(targets,train):\n","  model_dict = {}\n","  for target in targets:\n","      models = []\n","\n","      for fold in range(CFG.n_splits):\n","          X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns, inplace=False)\n","          y_train_cv = train[train[\"fold\"] != fold][target]\n","\n","          X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n","          y_eval_cv = train[train[\"fold\"] == fold][target]\n","\n","          dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n","          dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n","\n","          params = {\n","              'boosting_type': 'gbdt',\n","              'random_state': 42,\n","              'objective': 'regression',\n","              'metric': 'rmse',\n","              'learning_rate': 0.048,\n","              'max_depth': 3,\n","              'lambda_l1': 0.0,\n","              'lambda_l2': 0.011,\n","              'verbose': -1,\n","          }\n","\n","          evaluation_results = {}\n","          model = lgb.train(params,\n","                            num_boost_round=10000,\n","                            valid_names=['train', 'valid'],\n","                            train_set=dtrain,\n","                            valid_sets=dval,\n","                            callbacks=[\n","                                lgb.early_stopping(stopping_rounds=70, verbose=False),\n","                                # lgb.log_evaluation(100),\n","                                lgb.callback.record_evaluation(evaluation_results)\n","                              ],\n","                            )\n","          models.append(model)\n","\n","      model_dict[target] = models\n","  return model_dict\n","model_dict = create_model_dict(targets,train)\n"]},{"cell_type":"markdown","metadata":{},"source":["## CV Score"]},{"cell_type":"code","execution_count":434,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:10.587200Z","iopub.status.busy":"2023-10-04T06:51:10.586764Z","iopub.status.idle":"2023-10-04T06:51:10.684496Z","shell.execute_reply":"2023-10-04T06:51:10.683623Z","shell.execute_reply.started":"2023-10-04T06:51:10.587169Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["content_rmse : 0.4142110644531573\n","wording_rmse : 0.5349190424773487\n","mcrmse: 0.474565053465253\n"]}],"source":["# cv\n","# import optuna\n","def cal_mcrmse(model_dict, targets):\n","    rmses = []\n","    for target in targets:\n","        models = model_dict[target]\n","\n","        preds = []\n","        trues = []\n","        \n","        for fold, model in enumerate(models):\n","            X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns , inplace=False)\n","            y_eval_cv = train[train[\"fold\"] == fold][target]\n","\n","            pred = model.predict(X_eval_cv)\n","\n","            trues.extend(y_eval_cv)\n","            preds.extend(pred)\n","            \n","        rmse = np.sqrt(mean_squared_error(trues, preds))\n","        print(f\"{target}_rmse : {rmse}\")\n","        rmses = rmses + [rmse]\n","    return sum(rmses) / len(rmses)\n","mcrmse = cal_mcrmse(model_dict, targets)\n","print(f\"mcrmse: {mcrmse}\")\n","#mcrmse: 0.5657021985646049 after clean text"]},{"cell_type":"markdown","metadata":{},"source":["## Optuna"]},{"cell_type":"code","execution_count":435,"metadata":{},"outputs":[],"source":["import optuna\n","def objective(trial):\n","    weight_for_model = {}\n","    for model in CFG.list_model_infer:\n","        # weight_for_model[model] = trial.suggest_int(model, 0, 1)\n","        weight_for_model[model] = trial.suggest_float(model, -0.5, 1)\n","    scale = np.sum([weight_for_model[model] for model in CFG.list_model_infer])\n","    train[f\"wording_pred\"] = np.sum([ensembling_results_val[f\"{model}_wording_pred\"] * weight_for_model[model] for model in CFG.list_model_infer], axis=0) / scale    \n","    train[f\"content_pred\"] = np.sum([ensembling_results_val[f\"{model}_content_pred\"] * weight_for_model[model] for model in CFG.list_model_infer], axis=0) / scale\n","    model_dict = create_model_dict(targets,train)\n","    lost = cal_mcrmse(model_dict, targets)\n","    print(f\"mcrmse: {lost}\")\n","    # print(f\"weight_for_model: {weight_for_model}\")\n","    return lost"]},{"cell_type":"code","execution_count":436,"metadata":{},"outputs":[],"source":["# study = optuna.create_study(direction='minimize')\n","# study.optimize(objective, n_trials=100)\n","# print('Best trial:')\n","# trial_ = study.best_trial\n","\n","# print('Value: ', trial_.value)"]},{"cell_type":"code","execution_count":437,"metadata":{},"outputs":[],"source":["# for model in CFG.list_model_infer:\n","#     weight_for_model[model] = trial_.params[model]\n","# weight_for_model"]},{"cell_type":"code","execution_count":438,"metadata":{},"outputs":[],"source":["\n","# {'upload_model/debertav3base_lr17e-05': 0,\n","#  'upload_model/debertav3base_lr18e-05': 0,\n","#  'upload_model/debertav3base_lr21e-05': 0,\n","#  'upload_model/debertav3base_lr22e-05': 1,\n","#  'upload_model/debertav3large_lr12e-05': 1,\n","#  'upload_model/debertav3large_lr13e-05': 0,\n","#  'debertav3large_lr8e-06_att_0007': 1,\n","#  'debertav3large_lr9e-06_att_0007': 1,\n","#  'debertav3large_lr11e-05_att_0007': 1,\n","#  'debertav3large_lr12e-05_att_0007': 0,\n","#  'debertav3large_lr13e-05_att_0007': 0,\n","#  'debertav3large_lr14e-05_att_0007': 1,\n","#  'debertav3large_lr15e-05_att_0007': 0,\n","#  'debertav3large_lr16e-05_att_0007': 0,\n","#  'debertav3large_lr17e-05_att_0007': 0,\n","#  'debertav3large_lr18e-05_att_0007': 1,\n","#  'debertav3large_lr6e-06_att_0006': 1,\n","#  'debertav3large_lr7e-06_att_0006': 0,\n","#  'debertav3large_lr8e-06_att_0006': 1}\n","\n","\n","# Value:  0.47623294237768676\n","# Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\n","# {'upload_model/debertav3base_lr22e-05': 0.31611353255666413,\n","#  'upload_model/debertav3large_lr12e-05': 0.4865869028883723,\n","#  'debertav3large_lr8e-06_att_0007': 0.4262645629989505,\n","#  'debertav3large_lr9e-06_att_0007': 0.6016147381680528,\n","#  'debertav3large_lr11e-05_att_0007': 0.531486831481976,\n","#  'debertav3large_lr14e-05_att_0007': 0.18456428603709735,\n","#  'debertav3large_lr18e-05_att_0007': 0.5094108368580108,\n","#  'debertav3large_lr6e-06_att_0006': 0.19869967624910256,\n","#  'debertav3large_lr8e-06_att_0006': 0.7153757700914547}\n","\n","\n","# Value:  0.474565053465253\n","# {'upload_model/debertav3base_lr17e-05': 0.10605670751223599,\n","#  'upload_model/debertav3base_lr18e-05': 0.8151372494853444,\n","#  'upload_model/debertav3base_lr21e-05': -0.17229060080581526,\n","#  'upload_model/debertav3base_lr22e-05': 0.46124428559398767,\n","#  'upload_model/debertav3large_lr12e-05': 0.5213469207997752,\n","#  'upload_model/debertav3large_lr13e-05': 0.33587208988869377,\n","#  'debertav3large_lr8e-06_att_0007': 0.14044911254263082,\n","#  'debertav3large_lr11e-05_att_0007': 0.7504131871645143,\n","#  'debertav3large_lr13e-05_att_0007': 0.6190990620288409,\n","#  'debertav3large_lr14e-05_att_0007': -0.12134513614876408,\n","#  'debertav3large_lr17e-05_att_0007': 0.8917623536805175,\n","#  'debertav3large_lr18e-05_att_0007': -0.07388702753834085,\n","#  'debertav3large_lr7e-06_att_0006': 0.7887573166417163,\n","#  'debertav3large_lr8e-06_att_0006': 0.7195186292932556}"]},{"cell_type":"code","execution_count":439,"metadata":{},"outputs":[],"source":["# for model in CFG.list_model_infer:\n","#     weight_for_model[model] = trial_.params[model]\n","# weight_for_model"]},{"cell_type":"markdown","metadata":{},"source":["## Predict"]},{"cell_type":"code","execution_count":440,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:10.686383Z","iopub.status.busy":"2023-10-04T06:51:10.685913Z","iopub.status.idle":"2023-10-04T06:51:10.691848Z","shell.execute_reply":"2023-10-04T06:51:10.691034Z","shell.execute_reply.started":"2023-10-04T06:51:10.686351Z"},"trusted":true},"outputs":[],"source":["drop_columns_2 = [\n","                # \"fold\", \n","                \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n","                \"prompt_question\", \"prompt_title\", \n","                \"prompt_text\",\n","                \"input\"\n","               ] + [\n","                f\"content_pred_{i}\" for i in range(CFG.n_splits)\n","                ] + [\n","                f\"wording_pred_{i}\" for i in range(CFG.n_splits)\n","                ]"]},{"cell_type":"code","execution_count":441,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:10.693770Z","iopub.status.busy":"2023-10-04T06:51:10.693252Z","iopub.status.idle":"2023-10-04T06:51:10.714520Z","shell.execute_reply":"2023-10-04T06:51:10.713791Z","shell.execute_reply.started":"2023-10-04T06:51:10.693733Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[LightGBM] [Fatal] The number of features in data (47) is not the same as it was in training data (45).\n","You can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing.\n"]},{"ename":"LightGBMError","evalue":"The number of features in data (47) is not the same as it was in training data (45).\nYou can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)","\u001b[1;32m/home/nghiaph/nghiaph_workspace_115/CommonLit/commonlit-merge-model-infer-weight-final.ipynb Cell 55\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.166.128.115/home/nghiaph/nghiaph_workspace_115/CommonLit/commonlit-merge-model-infer-weight-final.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m X_eval_cv \u001b[39m=\u001b[39m test\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39mdrop_columns_2)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.166.128.115/home/nghiaph/nghiaph_workspace_115/CommonLit/commonlit-merge-model-infer-weight-final.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# print(X_eval_cv.head())\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.166.128.115/home/nghiaph/nghiaph_workspace_115/CommonLit/commonlit-merge-model-infer-weight-final.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(X_eval_cv)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.166.128.115/home/nghiaph/nghiaph_workspace_115/CommonLit/commonlit-merge-model-infer-weight-final.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# print('pred shape'  , pred.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.166.128.115/home/nghiaph/nghiaph_workspace_115/CommonLit/commonlit-merge-model-infer-weight-final.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m preds\u001b[39m.\u001b[39mappend(pred)\n","File \u001b[0;32m~/nghiaph_workspace_115/CommonLit/.venv/lib/python3.8/site-packages/lightgbm/basic.py:4220\u001b[0m, in \u001b[0;36mBooster.predict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features, **kwargs)\u001b[0m\n\u001b[1;32m   4218\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4219\u001b[0m         num_iteration \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 4220\u001b[0m \u001b[39mreturn\u001b[39;00m predictor\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m   4221\u001b[0m     data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m   4222\u001b[0m     start_iteration\u001b[39m=\u001b[39;49mstart_iteration,\n\u001b[1;32m   4223\u001b[0m     num_iteration\u001b[39m=\u001b[39;49mnum_iteration,\n\u001b[1;32m   4224\u001b[0m     raw_score\u001b[39m=\u001b[39;49mraw_score,\n\u001b[1;32m   4225\u001b[0m     pred_leaf\u001b[39m=\u001b[39;49mpred_leaf,\n\u001b[1;32m   4226\u001b[0m     pred_contrib\u001b[39m=\u001b[39;49mpred_contrib,\n\u001b[1;32m   4227\u001b[0m     data_has_header\u001b[39m=\u001b[39;49mdata_has_header,\n\u001b[1;32m   4228\u001b[0m     validate_features\u001b[39m=\u001b[39;49mvalidate_features\n\u001b[1;32m   4229\u001b[0m )\n","File \u001b[0;32m~/nghiaph_workspace_115/CommonLit/.venv/lib/python3.8/site-packages/lightgbm/basic.py:1047\u001b[0m, in \u001b[0;36m_InnerPredictor.predict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     preds, nrow \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__pred_for_csc(\n\u001b[1;32m   1041\u001b[0m         csc\u001b[39m=\u001b[39mdata,\n\u001b[1;32m   1042\u001b[0m         start_iteration\u001b[39m=\u001b[39mstart_iteration,\n\u001b[1;32m   1043\u001b[0m         num_iteration\u001b[39m=\u001b[39mnum_iteration,\n\u001b[1;32m   1044\u001b[0m         predict_type\u001b[39m=\u001b[39mpredict_type\n\u001b[1;32m   1045\u001b[0m     )\n\u001b[1;32m   1046\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m-> 1047\u001b[0m     preds, nrow \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__pred_for_np2d(\n\u001b[1;32m   1048\u001b[0m         mat\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m   1049\u001b[0m         start_iteration\u001b[39m=\u001b[39;49mstart_iteration,\n\u001b[1;32m   1050\u001b[0m         num_iteration\u001b[39m=\u001b[39;49mnum_iteration,\n\u001b[1;32m   1051\u001b[0m         predict_type\u001b[39m=\u001b[39;49mpredict_type\n\u001b[1;32m   1052\u001b[0m     )\n\u001b[1;32m   1053\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mlist\u001b[39m):\n\u001b[1;32m   1054\u001b[0m     \u001b[39mtry\u001b[39;00m:\n","File \u001b[0;32m~/nghiaph_workspace_115/CommonLit/.venv/lib/python3.8/site-packages/lightgbm/basic.py:1187\u001b[0m, in \u001b[0;36m_InnerPredictor.__pred_for_np2d\u001b[0;34m(self, mat, start_iteration, num_iteration, predict_type)\u001b[0m\n\u001b[1;32m   1185\u001b[0m     \u001b[39mreturn\u001b[39;00m preds, nrow\n\u001b[1;32m   1186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1187\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__inner_predict_np2d(\n\u001b[1;32m   1188\u001b[0m         mat\u001b[39m=\u001b[39;49mmat,\n\u001b[1;32m   1189\u001b[0m         start_iteration\u001b[39m=\u001b[39;49mstart_iteration,\n\u001b[1;32m   1190\u001b[0m         num_iteration\u001b[39m=\u001b[39;49mnum_iteration,\n\u001b[1;32m   1191\u001b[0m         predict_type\u001b[39m=\u001b[39;49mpredict_type,\n\u001b[1;32m   1192\u001b[0m         preds\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m   1193\u001b[0m     )\n","File \u001b[0;32m~/nghiaph_workspace_115/CommonLit/.venv/lib/python3.8/site-packages/lightgbm/basic.py:1140\u001b[0m, in \u001b[0;36m_InnerPredictor.__inner_predict_np2d\u001b[0;34m(self, mat, start_iteration, num_iteration, predict_type, preds)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mWrong length of pre-allocated predict array\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1139\u001b[0m out_num_preds \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int64(\u001b[39m0\u001b[39m)\n\u001b[0;32m-> 1140\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterPredictForMat(\n\u001b[1;32m   1141\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle,\n\u001b[1;32m   1142\u001b[0m     ptr_data,\n\u001b[1;32m   1143\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(type_ptr_data),\n\u001b[1;32m   1144\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int32(mat\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]),\n\u001b[1;32m   1145\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int32(mat\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m]),\n\u001b[1;32m   1146\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(_C_API_IS_ROW_MAJOR),\n\u001b[1;32m   1147\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(predict_type),\n\u001b[1;32m   1148\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(start_iteration),\n\u001b[1;32m   1149\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(num_iteration),\n\u001b[1;32m   1150\u001b[0m     _c_str(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpred_parameter),\n\u001b[1;32m   1151\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(out_num_preds),\n\u001b[1;32m   1152\u001b[0m     preds\u001b[39m.\u001b[39;49mctypes\u001b[39m.\u001b[39;49mdata_as(ctypes\u001b[39m.\u001b[39;49mPOINTER(ctypes\u001b[39m.\u001b[39;49mc_double))))\n\u001b[1;32m   1153\u001b[0m \u001b[39mif\u001b[39;00m n_preds \u001b[39m!=\u001b[39m out_num_preds\u001b[39m.\u001b[39mvalue:\n\u001b[1;32m   1154\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mWrong length for predict results\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/nghiaph_workspace_115/CommonLit/.venv/lib/python3.8/site-packages/lightgbm/basic.py:242\u001b[0m, in \u001b[0;36m_safe_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check the return value from C API call.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \n\u001b[1;32m    236\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[39m    The return value from C API calls.\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 242\u001b[0m     \u001b[39mraise\u001b[39;00m LightGBMError(_LIB\u001b[39m.\u001b[39mLGBM_GetLastError()\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m))\n","\u001b[0;31mLightGBMError\u001b[0m: The number of features in data (47) is not the same as it was in training data (45).\nYou can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing."]}],"source":["pred_dict = {}\n","for target in targets:\n","    models = model_dict[target]\n","    preds = []\n","\n","    for fold, model in enumerate(models):\n","        X_eval_cv = test.drop(columns=drop_columns_2)\n","        # print(X_eval_cv.head())\n","        pred = model.predict(X_eval_cv)\n","        # print('pred shape'  , pred.shape)\n","        preds.append(pred)\n","    \n","    pred_dict[target] = preds"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:10.716195Z","iopub.status.busy":"2023-10-04T06:51:10.715767Z","iopub.status.idle":"2023-10-04T06:51:10.726302Z","shell.execute_reply":"2023-10-04T06:51:10.725328Z","shell.execute_reply.started":"2023-10-04T06:51:10.716164Z"},"trusted":true},"outputs":[],"source":["for target in targets:\n","    preds = pred_dict[target]\n","    for i, pred in enumerate(preds):\n","        test[f\"{target}_pred_{i}\"] = pred\n","\n","    test[target] = test[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:10.728082Z","iopub.status.busy":"2023-10-04T06:51:10.727853Z","iopub.status.idle":"2023-10-04T06:51:10.748281Z","shell.execute_reply":"2023-10-04T06:51:10.747376Z","shell.execute_reply.started":"2023-10-04T06:51:10.728052Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>prompt_id</th>\n","      <th>text</th>\n","      <th>summary_length</th>\n","      <th>fixed_summary_text</th>\n","      <th>splling_err_num</th>\n","      <th>prompt_question</th>\n","      <th>prompt_title</th>\n","      <th>prompt_text</th>\n","      <th>prompt_length</th>\n","      <th>...</th>\n","      <th>wording_pred_1</th>\n","      <th>content_pred_1</th>\n","      <th>wording_pred_2</th>\n","      <th>content_pred_2</th>\n","      <th>wording_pred_3</th>\n","      <th>content_pred_3</th>\n","      <th>wording_pred</th>\n","      <th>content_pred</th>\n","      <th>content</th>\n","      <th>wording</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000000ffffff</td>\n","      <td>abc123</td>\n","      <td>Example text 1</td>\n","      <td>3</td>\n","      <td>Example text 1</td>\n","      <td>0</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 1</td>\n","      <td>Heading\\nText...</td>\n","      <td>3</td>\n","      <td>...</td>\n","      <td>-1.150856</td>\n","      <td>-1.73808</td>\n","      <td>-1.567721</td>\n","      <td>-1.456033</td>\n","      <td>-1.557598</td>\n","      <td>-1.396392</td>\n","      <td>-1.632939</td>\n","      <td>-1.372714</td>\n","      <td>-1.542456</td>\n","      <td>-1.433212</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>111111eeeeee</td>\n","      <td>def789</td>\n","      <td>Example text 2</td>\n","      <td>3</td>\n","      <td>Example text 2</td>\n","      <td>0</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 2</td>\n","      <td>Heading\\nText...</td>\n","      <td>3</td>\n","      <td>...</td>\n","      <td>-1.150856</td>\n","      <td>-1.73808</td>\n","      <td>-1.567721</td>\n","      <td>-1.456033</td>\n","      <td>-1.557598</td>\n","      <td>-1.396392</td>\n","      <td>-1.635602</td>\n","      <td>-1.375756</td>\n","      <td>-1.542456</td>\n","      <td>-1.433212</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>222222cccccc</td>\n","      <td>abc123</td>\n","      <td>Example text 3</td>\n","      <td>3</td>\n","      <td>Example text 3</td>\n","      <td>0</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 1</td>\n","      <td>Heading\\nText...</td>\n","      <td>3</td>\n","      <td>...</td>\n","      <td>-1.150856</td>\n","      <td>-1.73808</td>\n","      <td>-1.567721</td>\n","      <td>-1.456033</td>\n","      <td>-1.557598</td>\n","      <td>-1.396392</td>\n","      <td>-1.625720</td>\n","      <td>-1.386526</td>\n","      <td>-1.542456</td>\n","      <td>-1.433212</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>333333dddddd</td>\n","      <td>def789</td>\n","      <td>Example text 4</td>\n","      <td>3</td>\n","      <td>Example text 4</td>\n","      <td>0</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 2</td>\n","      <td>Heading\\nText...</td>\n","      <td>3</td>\n","      <td>...</td>\n","      <td>-1.150856</td>\n","      <td>-1.73808</td>\n","      <td>-1.567721</td>\n","      <td>-1.456033</td>\n","      <td>-1.557598</td>\n","      <td>-1.396392</td>\n","      <td>-1.626241</td>\n","      <td>-1.385102</td>\n","      <td>-1.542456</td>\n","      <td>-1.433212</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4 rows Ã— 63 columns</p>\n","</div>"],"text/plain":["     student_id prompt_id            text  summary_length fixed_summary_text  \\\n","0  000000ffffff    abc123  Example text 1               3     Example text 1   \n","1  111111eeeeee    def789  Example text 2               3     Example text 2   \n","2  222222cccccc    abc123  Example text 3               3     Example text 3   \n","3  333333dddddd    def789  Example text 4               3     Example text 4   \n","\n","   splling_err_num prompt_question     prompt_title       prompt_text  \\\n","0                0    Summarize...  Example Title 1  Heading\\nText...   \n","1                0    Summarize...  Example Title 2  Heading\\nText...   \n","2                0    Summarize...  Example Title 1  Heading\\nText...   \n","3                0    Summarize...  Example Title 2  Heading\\nText...   \n","\n","   prompt_length  ...  wording_pred_1  content_pred_1  wording_pred_2  \\\n","0              3  ...       -1.150856        -1.73808       -1.567721   \n","1              3  ...       -1.150856        -1.73808       -1.567721   \n","2              3  ...       -1.150856        -1.73808       -1.567721   \n","3              3  ...       -1.150856        -1.73808       -1.567721   \n","\n","   content_pred_2  wording_pred_3  content_pred_3  wording_pred  content_pred  \\\n","0       -1.456033       -1.557598       -1.396392     -1.632939     -1.372714   \n","1       -1.456033       -1.557598       -1.396392     -1.635602     -1.375756   \n","2       -1.456033       -1.557598       -1.396392     -1.625720     -1.386526   \n","3       -1.456033       -1.557598       -1.396392     -1.626241     -1.385102   \n","\n","    content   wording  \n","0 -1.542456 -1.433212  \n","1 -1.542456 -1.433212  \n","2 -1.542456 -1.433212  \n","3 -1.542456 -1.433212  \n","\n","[4 rows x 63 columns]"]},"execution_count":168,"metadata":{},"output_type":"execute_result"}],"source":["test"]},{"cell_type":"markdown","metadata":{},"source":["## Create Submission file"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:10.750060Z","iopub.status.busy":"2023-10-04T06:51:10.749559Z","iopub.status.idle":"2023-10-04T06:51:10.758788Z","shell.execute_reply":"2023-10-04T06:51:10.757827Z","shell.execute_reply.started":"2023-10-04T06:51:10.750030Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>content</th>\n","      <th>wording</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000000ffffff</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>111111eeeeee</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>222222cccccc</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>333333dddddd</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     student_id  content  wording\n","0  000000ffffff      0.0      0.0\n","1  111111eeeeee      0.0      0.0\n","2  222222cccccc      0.0      0.0\n","3  333333dddddd      0.0      0.0"]},"execution_count":169,"metadata":{},"output_type":"execute_result"}],"source":["sample_submission"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:10.760952Z","iopub.status.busy":"2023-10-04T06:51:10.759923Z","iopub.status.idle":"2023-10-04T06:51:10.770471Z","shell.execute_reply":"2023-10-04T06:51:10.769640Z","shell.execute_reply.started":"2023-10-04T06:51:10.760921Z"},"trusted":true},"outputs":[],"source":["test[[\"student_id\", \"content\", \"wording\"]].to_csv(\"submission.csv\", index=False)"]},{"cell_type":"markdown","metadata":{},"source":["## Summary\n","\n","CV result is like this.\n","\n","| | content rmse |wording rmse | mcrmse | LB| |\n","| -- | -- | -- | -- | -- | -- |\n","|baseline| 0.494 | 0.630 | 0.562 | 0.509 | [link](https://www.kaggle.com/code/tsunotsuno/debertav3-baseline-content-and-wording-models)|\n","| use title and question field | 0.476| 0.619 | 0.548 | 0.508 | [link](https://www.kaggle.com/code/tsunotsuno/debertav3-w-prompt-title-question-fields) |\n","| Debertav3 + LGBM | 0.451 | 0.591 | 0.521 | 0.461 | [link](https://www.kaggle.com/code/tsunotsuno/debertav3-lgbm-with-feature-engineering) |\n","| Debertav3 + LGBM with spell autocorrect | 0.448 | 0.581 | 0.514 | 0.459 |nogawanogawa's original code\n","| Debertav3 + LGBM with spell autocorrect and tuning | 0.442 | 0.566 | 0.504 | 0.453 | this notebook |\n","\n","The CV values improved slightly, and the LB value is improved."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
