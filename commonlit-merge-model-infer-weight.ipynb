{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# !pip install \".inputautocorrect/autocorrect-2.6.1.tar\"\n","# !pip install \".inputpyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# nltk.download(\"punkt\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from typing import List\n","import numpy as np\n","import pandas as pd\n","import warnings\n","import logging\n","import os\n","import shutil\n","import json\n","import transformers\n","from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n","from transformers import DataCollatorWithPadding\n","from datasets import Dataset,load_dataset, load_from_disk\n","from transformers import TrainingArguments, Trainer\n","from datasets import load_metric, disable_progress_bar\n","from sklearn.metrics import mean_squared_error\n","import torch\n","from sklearn.model_selection import KFold, GroupKFold\n","from tqdm import tqdm\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize.treebank import TreebankWordDetokenizer\n","from collections import Counter\n","import spacy\n","import re\n","from autocorrect import Speller\n","from spellchecker import SpellChecker\n","import lightgbm as lgb\n","warnings.simplefilter(\"ignore\")\n","logging.disable(logging.ERROR)\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n","disable_progress_bar()\n","tqdm.pandas()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def seed_everything(seed: int):\n","    import random, os\n","    import numpy as np\n","    import torch\n","    \n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True # load seed\n","    \n","seed_everything(seed=42)"]},{"cell_type":"markdown","metadata":{},"source":["## Class CFG"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class CFG:\n","    model_name=\"debertav3base\"\n","    learning_rate=0.000016\n","    weight_decay=0.03\n","    hidden_dropout_prob=0.007\n","    attention_probs_dropout_prob=0.007\n","    num_train_epochs=5\n","    n_splits=4\n","    batch_size= 128\n","    random_seed=42\n","    save_steps=100\n","    max_length= 512\n","    number_base_model = 2\n","    test_mode = False\n","    device = 'CPU'\n","    infer_mode = True\n","    list_model_infer = [\n","        # 'upload_model/debertav3base_lr15e-05',\n","        'upload_model/debertav3base_lr17e-05', #keep\n","        'upload_model/debertav3base_lr18e-05', #keep\n","        'upload_model/debertav3base_lr21e-05', # keep\n","        'upload_model/debertav3base_lr22e-05', #keep \n","        # 'upload_model/debertav3base_lr5e-05', \n","        'upload_model/debertav3large_lr12e-05', # upload\n","        'upload_model/debertav3large_lr13e-05',  # upload\n","        # 'debertav3large_lr1e-05_save',\n","        # 'debertav3large_lr1e-05_att_0007',\n","        'debertav3large_lr8e-06_att_0007', # upload \n","        'debertav3large_lr9e-06_att_0007', # upload \n","        'debertav3large_lr11e-05_att_0007',\n","        'debertav3large_lr12e-05_att_0007',\n","        'debertav3large_lr13e-05_att_0007',\n","        'debertav3large_lr14e-05_att_0007',\n","        'debertav3large_lr15e-05_att_0007', # upload \n","        'debertav3large_lr16e-05_att_0007', # upload \n","        'debertav3large_lr17e-05_att_0007', # upload \n","        'debertav3large_lr18e-05_att_0007', # upload \n","        ]\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["# print device\n","if CFG.device != 'CPU':\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # print device \n","else :\n","    device = torch.device(\"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{},"source":["## Dataload"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["DATA_DIR = \"input/commonlit-evaluate-student-summaries/\"\n","\n","# prompts_train = pd.read_csv(DATA_DIR + \"prompts_train.csv\")\n","prompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")\n","# summaries_train = pd.read_csv(DATA_DIR + \"summaries_train.csv\")\n","summaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")\n","sample_submission = pd.read_csv(DATA_DIR + \"sample_submission.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["## Exploratory Data Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# prompts_train.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocess 2\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from textblob import TextBlob\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from nltk import ne_chunk, word_tokenize, pos_tag\n","from bs4 import BeautifulSoup\n","\n","# nltk.downloader.download('vader_lexicon')\n","import pyphen\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","\n","dic = pyphen.Pyphen(lang='en')\n","sid = SentimentIntensityAnalyzer()\n","\n","class Preprocessor2:\n","    def __init__(self, \n","                model_name: str,\n","                ) -> None:\n","        self.tokenizer = AutoTokenizer.from_pretrained(f\"input/{model_name}\")\n","        self.twd = TreebankWordDetokenizer()\n","        self.STOP_WORDS = set(stopwords.words('english'))\n","        \n","        self.spacy_ner_model = spacy.load('en_core_web_sm',)\n","        self.speller = Speller(lang='en')\n","        self.spellchecker = SpellChecker() \n","        \n","    def calculate_text_similarity(self, row):\n","        vectorizer = TfidfVectorizer()\n","        tfidf_matrix = vectorizer.fit_transform([row['prompt_text'], row['text']])\n","        return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2]).flatten()[0]\n","    \n","    def sentiment_analysis(self, text):\n","        analysis = TextBlob(text)\n","        return analysis.sentiment.polarity, analysis.sentiment.subjectivity\n","    \n","    def word_overlap_count(self, row):\n","        \"\"\" intersection(prompt_text, text) \"\"\"        \n","        def check_is_stop_word(word):\n","            return word in self.STOP_WORDS\n","        \n","        prompt_words = row['prompt_tokens']\n","        summary_words = row['summary_tokens']\n","        if self.STOP_WORDS:\n","            prompt_words = list(filter(check_is_stop_word, prompt_words))\n","            summary_words = list(filter(check_is_stop_word, summary_words))\n","        return len(set(prompt_words).intersection(set(summary_words)))\n","            \n","    def ngrams(self, token, n):\n","        # Use the zip function to help us generate n-grams\n","        # Concatentate the tokens into ngrams and return\n","        ngrams = zip(*[token[i:] for i in range(n)])\n","        return [\" \".join(ngram) for ngram in ngrams]\n","\n","    def ngram_co_occurrence(self, row, n: int) -> int:\n","        # Tokenize the original text and summary into words\n","        original_tokens = row['prompt_tokens']\n","        summary_tokens = row['summary_tokens']\n","\n","        # Generate n-grams for the original text and summary\n","        original_ngrams = set(self.ngrams(original_tokens, n))\n","        summary_ngrams = set(self.ngrams(summary_tokens, n))\n","\n","        # Calculate the number of common n-grams\n","        common_ngrams = original_ngrams.intersection(summary_ngrams)\n","        return len(common_ngrams)\n","    \n","    def ner_overlap_count(self, row, mode:str):\n","        model = self.spacy_ner_model\n","        def clean_ners(ner_list):\n","            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n","        prompt = model(row['prompt_text'])\n","        summary = model(row['text'])\n","\n","        if \"spacy\" in str(model):\n","            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n","            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n","        elif \"stanza\" in str(model):\n","            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n","            summary_ner = set([(token.text, token.type) for token in summary.ents])\n","        else:\n","            raise Exception(\"Model not supported\")\n","\n","        prompt_ner = clean_ners(prompt_ner)\n","        summary_ner = clean_ners(summary_ner)\n","\n","        intersecting_ners = prompt_ner.intersection(summary_ner)\n","        \n","        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n","        \n","        if mode == \"train\":\n","            return ner_dict\n","        elif mode == \"test\":\n","            return {key: ner_dict.get(key) for key in self.ner_keys}\n","\n","    \n","    def quotes_count(self, row):\n","        summary = row['text']\n","        text = row['prompt_text']\n","        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n","        if len(quotes_from_summary)>0:\n","            return [quote in text for quote in quotes_from_summary].count(True)\n","        else:\n","            return 0\n","\n","    def spelling(self, text):\n","        \n","        wordlist=text.split()\n","        amount_miss = len(list(self.spellchecker.unknown(wordlist)))\n","\n","        return amount_miss\n","    \n","    def calculate_unique_words(self,text):\n","        unique_words = set(text.split())\n","        return len(unique_words)\n","    \n","    def add_spelling_dictionary(self, tokens: List[str]) -> List[str]:\n","        \"\"\"dictionary update for pyspell checker and autocorrect\"\"\"\n","        self.spellchecker.word_frequency.load_words(tokens)\n","        self.speller.nlp_data.update({token:1000 for token in tokens})\n","        \n","    def calculate_pos_ratios(self , text):\n","        pos_tags = pos_tag(nltk.word_tokenize(text))\n","        pos_counts = Counter(tag for word, tag in pos_tags)\n","        total_words = len(pos_tags)\n","        ratios = {tag: count / total_words for tag, count in pos_counts.items()}\n","        return ratios\n","    \n","    def calculate_punctuation_ratios(self,text):\n","        total_chars = len(text)\n","        punctuation_counts = Counter(char for char in text if char in '.,!?;:\"()[]{}')\n","        ratios = {char: count / total_chars for char, count in punctuation_counts.items()}\n","        return ratios\n","    \n","    def calculate_keyword_density(self,row):\n","        keywords = set(row['prompt_text'].split())\n","        text_words = row['text'].split()\n","        keyword_count = sum(1 for word in text_words if word in keywords)\n","        return keyword_count / len(text_words)\n","    \n","    def count_syllables(self,word):\n","        hyphenated_word = dic.inserted(word)\n","        return len(hyphenated_word.split('-'))\n","\n","    def flesch_reading_ease_manual(self,text):\n","        total_sentences = len(TextBlob(text).sentences)\n","        total_words = len(TextBlob(text).words)\n","        total_syllables = sum(self.count_syllables(word) for word in TextBlob(text).words)\n","\n","        if total_sentences == 0 or total_words == 0:\n","            return 0\n","\n","        flesch_score = 206.835 - 1.015 * (total_words / total_sentences) - 84.6 * (total_syllables / total_words)\n","        return flesch_score\n","    \n","    def flesch_kincaid_grade_level(self, text):\n","        total_sentences = len(TextBlob(text).sentences)\n","        total_words = len(TextBlob(text).words)\n","        total_syllables = sum(self.count_syllables(word) for word in TextBlob(text).words)\n","\n","        if total_sentences == 0 or total_words == 0:\n","            return 0\n","\n","        fk_grade = 0.39 * (total_words / total_sentences) + 11.8 * (total_syllables / total_words) - 15.59\n","        return fk_grade\n","    \n","    def gunning_fog(self, text):\n","        total_sentences = len(TextBlob(text).sentences)\n","        total_words = len(TextBlob(text).words)\n","        complex_words = sum(1 for word in TextBlob(text).words if self.count_syllables(word) > 2)\n","\n","        if total_sentences == 0 or total_words == 0:\n","            return 0\n","\n","        fog_index = 0.4 * ((total_words / total_sentences) + 100 * (complex_words / total_words))\n","        return fog_index\n","    \n","    def calculate_sentiment_scores(self,text):\n","        sentiment_scores = sid.polarity_scores(text)\n","        return sentiment_scores\n","    \n","    def count_difficult_words(self, text, syllable_threshold=3):\n","        words = TextBlob(text).words\n","        difficult_words_count = sum(1 for word in words if self.count_syllables(word) >= syllable_threshold)\n","        return difficult_words_count\n","\n","    def text_cleaning(self, text):\n","        '''\n","        Cleans text into a basic form for NLP. Operations include the following:-\n","        1. Remove special charecters like &, #, etc\n","        2. Removes extra spaces\n","        3. Removes embedded URL links\n","        4. Removes HTML tags\n","        5. Removes emojis\n","\n","        text - Text piece to be cleaned.\n","        '''\n","        template = re.compile(r'https?://\\S+|www\\.\\S+')  # Removes website links\n","        text = template.sub(r'', text)\n","\n","        soup = BeautifulSoup(text, 'lxml')  # Removes HTML tags\n","        only_text = soup.get_text()\n","        text = only_text\n","\n","        emoji_pattern = re.compile(\"[\"\n","                                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                                u\"\\U00002702-\\U000027B0\"\n","                                u\"\\U000024C2-\\U0001F251\"\n","                                \"]+\", flags=re.UNICODE)\n","        text = emoji_pattern.sub(r'', text)\n","\n","        text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) # Remove special Charecters\n","        text = re.sub('\\n+', '\\n', text) \n","        text = re.sub('\\.+', '.', text) \n","        text = re.sub(' +', ' ', text) # Remove Extra Spaces \n","\n","        return text\n","    \n","    def run(self, \n","            prompts: pd.DataFrame,\n","            summaries:pd.DataFrame,\n","            mode:str\n","        ) -> pd.DataFrame:\n","        \n","        # before merge preprocess\n","        prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n","            lambda x: len(word_tokenize(x))\n","        )\n","        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n","            lambda x: word_tokenize(x)\n","        )\n","\n","        summaries[\"summary_length\"] = summaries[\"text\"].apply(\n","            lambda x: len(word_tokenize(x))\n","        )\n","        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n","            lambda x: word_tokenize(x)\n","        )\n","        \n","        # Add prompt tokens into spelling checker dictionary\n","        prompts[\"prompt_tokens\"].apply(\n","            lambda x: self.add_spelling_dictionary(x)\n","        )\n","        \n","        prompts['gunning_fog_prompt'] = prompts['prompt_text'].apply(self.gunning_fog)\n","        prompts['flesch_kincaid_grade_level_prompt'] = prompts['prompt_text'].apply(self.flesch_kincaid_grade_level)\n","        prompts['flesch_reading_ease_prompt'] = prompts['prompt_text'].apply(self.flesch_reading_ease_manual)\n","\n","        \n","#         from IPython.core.debugger import Pdb; Pdb().set_trace()\n","        # fix misspelling\n","        summaries[\"fixed_summary_text\"] = summaries[\"text\"].progress_apply(\n","            lambda x: self.text_cleaning(x)\n","        )\n","        summaries[\"fixed_summary_text\"] = summaries[\"fixed_summary_text\"].progress_apply(\n","            lambda x: self.speller(x)\n","        )\n","        \n","        \n","        # count misspelling\n","        summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n","        \n","        # merge prompts and summaries\n","        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n","        input_df['flesch_reading_ease'] = input_df['text'].apply(self.flesch_reading_ease_manual)\n","        input_df['word_count'] = input_df['text'].apply(lambda x: len(x.split()))\n","        input_df['sentence_length'] = input_df['text'].apply(lambda x: len(x.split('.')))\n","        input_df['vocabulary_richness'] = input_df['text'].apply(lambda x: len(set(x.split())))\n","\n","        input_df['word_count2'] = [len(t.split(' ')) for t in input_df.text]\n","        input_df['num_unq_words']=[len(list(set(x.lower().split(' ')))) for x in input_df.text]\n","        input_df['num_chars']= [len(x) for x in input_df.text]\n","\n","        # Additional features\n","        input_df['avg_word_length'] = input_df['text'].apply(lambda x: np.mean([len(word) for word in x.split()]))\n","        input_df['comma_count'] = input_df['text'].apply(lambda x: x.count(','))\n","        input_df['semicolon_count'] = input_df['text'].apply(lambda x: x.count(';'))\n","\n","        # after merge preprocess\n","        input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n","        \n","        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n","        input_df['bigram_overlap_count'] = input_df.progress_apply(\n","            self.ngram_co_occurrence,args=(2,), axis=1 \n","        )\n","        input_df['bigram_overlap_ratio'] = input_df['bigram_overlap_count'] / (input_df['summary_length'] - 1)\n","        \n","        input_df['trigram_overlap_count'] = input_df.progress_apply(\n","            self.ngram_co_occurrence, args=(3,), axis=1\n","        )\n","        input_df['trigram_overlap_ratio'] = input_df['trigram_overlap_count'] / (input_df['summary_length'] - 2)\n","        \n","        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n","        \n","        input_df['exclamation_count'] = input_df['text'].apply(lambda x: x.count('!'))\n","        input_df['question_count'] = input_df['text'].apply(lambda x: x.count('?'))\n","        input_df['pos_ratios'] = input_df['text'].apply(self.calculate_pos_ratios)\n","\n","        # Convert the dictionary of POS ratios into a single value (mean)\n","        input_df['pos_mean'] = input_df['pos_ratios'].apply(lambda x: np.mean(list(x.values())))\n","        input_df['punctuation_ratios'] = input_df['text'].apply(self.calculate_punctuation_ratios)\n","\n","        # Convert the dictionary of punctuation ratios into a single value (sum)\n","        input_df['punctuation_sum'] = input_df['punctuation_ratios'].apply(lambda x: np.sum(list(x.values())))\n","        input_df['keyword_density'] = input_df.apply(self.calculate_keyword_density, axis=1)\n","        input_df['jaccard_similarity'] = input_df.apply(lambda row: len(set(word_tokenize(row['prompt_text'])) & set(word_tokenize(row['text']))) / len(set(word_tokenize(row['prompt_text'])) | set(word_tokenize(row['text']))), axis=1)\n","        tqdm.pandas(desc=\"Performing Sentiment Analysis\")\n","        input_df[['sentiment_polarity', 'sentiment_subjectivity']] = input_df['text'].progress_apply(\n","            lambda x: pd.Series(self.sentiment_analysis(x))\n","        )\n","        tqdm.pandas(desc=\"Calculating Text Similarity\")\n","        input_df['text_similarity'] = input_df.progress_apply(self.calculate_text_similarity, axis=1)\n","        #Calculate sentiment scores for each row\n","        input_df['sentiment_scores'] = input_df['text'].apply(self.calculate_sentiment_scores)\n","        \n","        input_df['gunning_fog'] = input_df['text'].apply(self.gunning_fog)\n","        input_df['flesch_kincaid_grade_level'] = input_df['text'].apply(self.flesch_kincaid_grade_level)\n","        input_df['count_difficult_words'] = input_df['text'].apply(self.count_difficult_words)\n","\n","        # Convert sentiment_scores into individual columns\n","        sentiment_columns = pd.DataFrame(list(input_df['sentiment_scores']))\n","        input_df = pd.concat([input_df, sentiment_columns], axis=1)\n","        input_df['sentiment_scores_prompt'] = input_df['prompt_text'].apply(self.calculate_sentiment_scores)\n","        # Convert sentiment_scores_prompt into individual columns\n","        sentiment_columns_prompt = pd.DataFrame(list(input_df['sentiment_scores_prompt']))\n","        sentiment_columns_prompt.columns = [col +'_prompt' for col in sentiment_columns_prompt.columns]\n","        input_df = pd.concat([input_df, sentiment_columns_prompt], axis=1)\n","        columns =  ['pos_ratios', 'sentiment_scores', 'punctuation_ratios', 'sentiment_scores_prompt']\n","        cols_to_drop = [col for col in columns if col in input_df.columns]\n","        if cols_to_drop:\n","            input_df = input_df.drop(columns=cols_to_drop)\n","        \n","        print(cols_to_drop)\n","        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["preprocessor = Preprocessor2(model_name=CFG.model_name)"]},{"cell_type":"markdown","metadata":{},"source":["## Create the train and test sets\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if CFG.test_mode : \n","    # prompts_train = prompts_train[:12]\n","    prompts_test = prompts_test[:12]\n","    # summaries_train = summaries_train[:12]\n","    summaries_test = summaries_test[:12]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 4/4 [00:00<00:00, 1374.28it/s]\n","100%|██████████| 4/4 [00:00<00:00, 23399.19it/s]\n","100%|██████████| 4/4 [00:00<00:00, 29279.61it/s]\n","100%|██████████| 4/4 [00:00<00:00, 6269.51it/s]\n","100%|██████████| 4/4 [00:00<00:00, 10761.52it/s]\n","100%|██████████| 4/4 [00:00<00:00, 9597.95it/s]\n","100%|██████████| 4/4 [00:00<00:00, 8603.70it/s]\n","Performing Sentiment Analysis: 100%|██████████| 4/4 [00:00<00:00, 5023.12it/s]\n","Calculating Text Similarity: 100%|██████████| 4/4 [00:00<00:00, 785.23it/s]"]},{"name":"stdout","output_type":"stream","text":["['pos_ratios', 'sentiment_scores', 'punctuation_ratios', 'sentiment_scores_prompt']\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# train = preprocessor.run(prompts_train, summaries_train, mode=\"train\")\n","test = preprocessor.run(prompts_test, summaries_test, mode=\"test\")\n","# save train and test \n","# train.to_csv(\"input/train.csv\", index=False)\n","# test.to_csv(\"input/test.csv\", index=False)\n","# load train and test\n","# train = pd.read_csv(\"input/train.csv\")\n","# test = pd.read_csv(\"input/test.csv\")\n","# train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gkf = GroupKFold(n_splits=CFG.n_splits)\n","\n","# for i, (_, val_index) in enumerate(gkf.split(train, groups=train[\"prompt_id\"])):\n","#     train.loc[val_index, \"fold\"] = i\n","\n","# train.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Model Function Definition"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    rmse = mean_squared_error(labels, predictions, squared=False)\n","    return {\"rmse\": rmse}\n","\n","def compute_mcrmse(eval_pred):\n","    \"\"\"\n","    Calculates mean columnwise root mean squared error\n","    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n","    \"\"\"\n","    preds, labels = eval_pred\n","\n","    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n","    mcrmse = np.mean(col_rmse)\n","\n","    return {\n","        \"content_rmse\": col_rmse[0],\n","        \"wording_rmse\": col_rmse[1],\n","        \"mcrmse\": mcrmse,\n","    }\n","\n","def compt_score(content_true, content_pred, wording_true, wording_pred):\n","    content_score = mean_squared_error(content_true, content_pred)**(1/2)\n","    wording_score = mean_squared_error(wording_true, wording_pred)**(1/2)\n","    \n","    return (content_score + wording_score)/2"]},{"cell_type":"markdown","metadata":{},"source":["## Deberta Regressor"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ScoreRegressor:\n","    def __init__(self, \n","                model_name: str,\n","                model_dir: str,\n","                target: list,\n","                hidden_dropout_prob: float,\n","                attention_probs_dropout_prob: float,\n","                max_length: int,\n","                ):\n","        self.inputs = [\"prompt_text\", \"prompt_title\", \"prompt_question\", \"fixed_summary_text\"] # fix summary text have prompt text in it \n","        self.input_col = \"input\"\n","        \n","        self.text_cols = [self.input_col] \n","        self.target = target\n","        self.target_cols = target\n","\n","        self.model_name = model_name\n","        lr = str(CFG.learning_rate).replace(\".\", \"\")\n","        self.model_dir = model_dir\n","        self.max_length = max_length\n","        \n","        self.tokenizer = AutoTokenizer.from_pretrained(f\"input/{model_name}\")\n","        self.model_config = AutoConfig.from_pretrained(f\"input/{model_name}\" )\n","        # print(self.model_config)\n","        self.model_config.update({\n","            \"hidden_dropout_prob\": hidden_dropout_prob,\n","            \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n","            \"num_labels\": 2,\n","            \"problem_type\": \"regression\",\n","        })\n","        seed_everything(seed=42)\n","\n","        self.data_collator = DataCollatorWithPadding(\n","            tokenizer=self.tokenizer\n","        )\n","\n","\n","    def tokenize_function(self, examples: pd.DataFrame):\n","        # labels = ['content' , 'wording']\n","        # print('labels', labels)\n","        tokenized = self.tokenizer(examples[self.input_col],\n","                         padding=False,\n","                         truncation=True,\n","                         max_length=self.max_length)\n","        return {\n","            **tokenized,\n","            \"labels\": [examples['content'], examples['wording']],\n","        }\n","    \n","    def tokenize_function_test(self, examples: pd.DataFrame):\n","        tokenized = self.tokenizer(examples[self.input_col],\n","                         padding=False,\n","                         truncation=True,\n","                         max_length=self.max_length)\n","        return tokenized\n","        \n","    def train(self, \n","            fold: int,\n","            train_df: pd.DataFrame,\n","            valid_df: pd.DataFrame,\n","            batch_size: int,\n","            learning_rate: float,\n","            weight_decay: float,\n","            num_train_epochs: float,\n","            save_steps: int,\n","        ) -> None:\n","        \"\"\"fine-tuning\"\"\"\n","        \n","        sep = self.tokenizer.sep_token\n","        # print('sep', sep)\n","        train_df[self.input_col] = (\n","                    train_df[\"prompt_title\"] + sep \n","                    + train_df[\"prompt_question\"] + sep \n","                    + train_df[\"fixed_summary_text\"]\n","                  )\n","\n","        valid_df[self.input_col] = (\n","                    valid_df[\"prompt_title\"] + sep \n","                    + valid_df[\"prompt_question\"] + sep \n","                    + valid_df[\"fixed_summary_text\"]\n","                  )\n","        # filter train_df with input_col have more than 5000 tokens\n","        # print('create train and val data frame ')\n","        # print('self.target_cols', self.target_cols)\n","        # print('self.input_col', self.input_col)\n","        train_df = train_df[[self.input_col] + self.target_cols]\n","        valid_df = valid_df[[self.input_col] + self.target_cols]\n","        \n","        model_content = AutoModelForSequenceClassification.from_pretrained(\n","            f\"input/{self.model_name}\", \n","            config=self.model_config\n","        )\n","\n","        train_dataset = Dataset.from_pandas(train_df, preserve_index=False) \n","        val_dataset = Dataset.from_pandas(valid_df, preserve_index=False) \n","        train_tokenized_datasets = train_dataset.map(self.tokenize_function, batched=True)\n","        val_tokenized_datasets = val_dataset.map(self.tokenize_function, batched=True)\n","        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n","        # print('model_fold_dir', model_fold_dir)\n","        training_args = TrainingArguments(\n","            output_dir=model_fold_dir,\n","            load_best_model_at_end=True, # select best model\n","            learning_rate=learning_rate,\n","            per_device_train_batch_size=batch_size,\n","            per_device_eval_batch_size=batch_size,\n","            num_train_epochs=num_train_epochs,\n","            weight_decay=weight_decay,\n","            report_to='none',\n","            greater_is_better=False,\n","            save_strategy=\"steps\",\n","            evaluation_strategy=\"steps\",\n","            eval_steps=save_steps,\n","            save_steps=save_steps,\n","            metric_for_best_model=\"rmse\",\n","            save_total_limit=1\n","        )\n","        # print('define trainer')\n","        trainer = Trainer(\n","            model=model_content,\n","            args=training_args,\n","            train_dataset=train_tokenized_datasets,\n","            eval_dataset=val_tokenized_datasets,\n","            tokenizer=self.tokenizer,\n","            compute_metrics=compute_metrics,\n","            data_collator=self.data_collator\n","        )\n","        print('start training')\n","        # print('trainer.train_dataset[0]' , trainer.train_dataset[0])\n","        trainer.train()\n","        print('finish training')\n","        model_content.save_pretrained(self.model_dir)\n","        self.tokenizer.save_pretrained(self.model_dir)\n","\n","        \n","    def predict(self, \n","                test_df: pd.DataFrame,\n","                fold: int,\n","               ):\n","        \"\"\"predict content score\"\"\"\n","        \n","        sep = self.tokenizer.sep_token\n","        in_text = (\n","                    test_df[\"prompt_title\"] + sep \n","                    + test_df[\"prompt_question\"] + sep \n","                    + test_df[\"fixed_summary_text\"]\n","                  )\n","        test_df[self.input_col] = in_text\n","\n","        test_ = test_df[[self.input_col]]\n","    \n","        test_dataset = Dataset.from_pandas(test_, preserve_index=False) \n","        test_tokenized_dataset = test_dataset.map(self.tokenize_function_test, batched=True)\n","\n","        model_content = AutoModelForSequenceClassification.from_pretrained(f\"{self.model_dir}\")\n","        model_content.eval()\n","        \n","        # eg. \"bert/fold_0/\"\n","        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n","        # print(\"model_fold_dir\",model_fold_dir)\n","        test_args = TrainingArguments(\n","            output_dir=model_fold_dir,\n","            do_train = False,\n","            do_predict = True,\n","            per_device_eval_batch_size = CFG.batch_size,   \n","            dataloader_drop_last = False,\n","        )\n","\n","        # init trainer\n","        infer_content = Trainer(\n","                      model = model_content, \n","                      tokenizer=self.tokenizer,\n","                      data_collator=self.data_collator,\n","                      args = test_args)\n","\n","        preds = infer_content.predict(test_tokenized_dataset)[0]\n","\n","        return preds"]},{"cell_type":"markdown","metadata":{},"source":["## Train by fold function\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# def validate(\n","#     train_df: pd.DataFrame,\n","#     target:str,\n","#     save_each_model: bool,\n","#     model_name: str,\n","#     model_dir_base: str,\n","#     hidden_dropout_prob: float,\n","#     attention_probs_dropout_prob: float,\n","#     max_length : int\n","#     ) -> pd.DataFrame:\n","#     \"\"\"predict oof data\"\"\"\n","#     for fold in range(CFG.n_splits):\n","#         # print(f\"fold {fold}:\")\n","        \n","#         valid_data = train_df[train_df[\"fold\"] == fold]\n","        \n","#         if save_each_model == True:\n","#             model_dir =  f\"{target}/{model_dir_base}/fold_{fold}\"\n","#         else: \n","#             model_dir =  f\"{model_dir_base}/fold_{fold}\"\n","#         csr = ScoreRegressor(\n","#             model_name=model_name,\n","#             target=target,\n","#             model_dir = model_dir,\n","#             hidden_dropout_prob=hidden_dropout_prob,\n","#             attention_probs_dropout_prob=attention_probs_dropout_prob,\n","#             max_length=max_length,\n","#            )\n","        \n","#         pred = csr.predict(\n","#             test_df=valid_data, \n","#             fold=fold\n","#         )\n","#         # print('pred shape', pred.shape)\n","#         train_df.loc[valid_data.index, f\"wording_pred\"] = pred[:,0]\n","#         train_df.loc[valid_data.index, f\"content_pred\"] = pred[:,1]\n","\n","#     return train_df\n","    \n","def predict(\n","    test_df: pd.DataFrame,\n","    target:str,\n","    save_each_model: bool,\n","    model_name: str,\n","    model_dir_base: str,\n","    hidden_dropout_prob: float,\n","    attention_probs_dropout_prob: float,\n","    max_length : int\n","    ):\n","    \"\"\"predict using mean folds\"\"\"\n","    for fold in range(CFG.n_splits):\n","        # print(f\"fold {fold}:\")\n","        \n","        if save_each_model == True:\n","            model_dir =  f\"{target}/{model_dir_base}/fold_{fold}\"\n","        else: \n","            model_dir =  f\"{model_dir_base}/fold_{fold}\"\n","        csr = ScoreRegressor(\n","            model_name=model_name,\n","            target=target,\n","            model_dir = model_dir, \n","            hidden_dropout_prob=hidden_dropout_prob,\n","            attention_probs_dropout_prob=attention_probs_dropout_prob,\n","            max_length=max_length,\n","           )\n","        \n","        pred = csr.predict(\n","            test_df=test_df, \n","            fold=fold\n","        )\n","        \n","        # test_df[f\"{target}_pred_{fold}\"] = pred\n","        test_df[f\"wording_pred_{fold}\"] = pred[:,0]\n","        test_df[f\"content_pred_{fold}\"] = pred[:,1]\n","        \n","    # test_df[f\"{target}\"] = test_df[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n","    test_df[f\"wording_pred\"] = test_df[[f\"wording_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n","    test_df[f\"content_pred\"] = test_df[[f\"content_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n","    return test_df\n","targets =  [\"content\", \"wording\"]\n"]},{"cell_type":"markdown","metadata":{},"source":["## Infer\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # ensembling_results_val  = pd.DataFrame()\n","# ensembling_results_test = pd.DataFrame()\n","# dem = 0\n","# if CFG.infer_mode:\n","#     for model_dir in CFG.list_model_infer:\n","#         print(\"percent of model\", dem/CFG.number_base_model)\n","#         print(model_dir)    \n","#         dem = dem +1 \n","#         if dem >= CFG.number_base_model:\n","#             CFG.batch_size = 16\n","#             CFG.max_length = 1462\n","#             CFG.model_name = \"debertav3large\"\n","#         # train = validate(\n","#         #     train,\n","#         #     target=targets,\n","#         #     save_each_model=False,\n","#         #     model_name=CFG.model_name,\n","#         #     model_dir_base = model_dir,\n","#         #     hidden_dropout_prob=CFG.hidden_dropout_prob,\n","#         #     attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n","#         #     max_length=CFG.max_length\n","#         # )\n","#         # for target in targets:\n","#         #     rmse = mean_squared_error(train[target], train[f\"{target}_pred\"], squared=False)\n","#         #     print(f\"cv {target} rmse: {rmse}\")\n","#         print('done validate')\n","#         test = predict(\n","#             test,\n","#             target=targets,\n","#             save_each_model=False,\n","#             model_name=CFG.model_name,\n","#             model_dir_base = model_dir,\n","#             hidden_dropout_prob=CFG.hidden_dropout_prob,\n","#             attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n","#             max_length=CFG.max_length\n","#         )\n","#         print('done predict')\n","#         # add wording_pred and content_pred to ensembling_results\n","#         # ensembling_results_val[f\"{model_dir}_wording_pred\"] = train[\"wording_pred\"]\n","#         # ensembling_results_val[f\"{model_dir}_content_pred\"] = train[\"content_pred\"]\n","#         ensembling_results_test[f\"{model_dir}_wording_pred\"] = test[\"wording_pred\"]\n","#         ensembling_results_test[f\"{model_dir}_content_pred\"] = test[\"content_pred\"]\n","#         # print('ensembling_results_val \\n', ensembling_results_val.head() )\n","        "]},{"cell_type":"markdown","metadata":{},"source":["## LGBM model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# save ensembling_results_val and ensembling_results_test\n","# ensembling_results_val.to_csv(\"ensembling_results_val.csv\", index=False)\n","# ensembling_results_test.to_csv(\"ensembling_results_test.csv\", index=False)\n","# load ensembling_results_val and ensembling_results_test\n","ensembling_results_val = pd.read_csv(\"ensembling_results_val.csv\")\n","# ensembling_results_test = pd.read_csv(\"ensembling_results_test.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>upload_model/debertav3base_lr5e-05_wording_pred</th>\n","      <th>upload_model/debertav3base_lr5e-05_content_pred</th>\n","      <th>upload_model/debertav3base_lr15e-05_wording_pred</th>\n","      <th>upload_model/debertav3base_lr15e-05_content_pred</th>\n","      <th>upload_model/debertav3base_lr17e-05_wording_pred</th>\n","      <th>upload_model/debertav3base_lr17e-05_content_pred</th>\n","      <th>upload_model/debertav3base_lr18e-05_wording_pred</th>\n","      <th>upload_model/debertav3base_lr18e-05_content_pred</th>\n","      <th>upload_model/debertav3base_lr21e-05_wording_pred</th>\n","      <th>upload_model/debertav3base_lr21e-05_content_pred</th>\n","      <th>...</th>\n","      <th>debertav3large_lr14e-05_att_0007_wording_pred</th>\n","      <th>debertav3large_lr14e-05_att_0007_content_pred</th>\n","      <th>debertav3large_lr15e-05_att_0007_wording_pred</th>\n","      <th>debertav3large_lr15e-05_att_0007_content_pred</th>\n","      <th>debertav3large_lr16e-05_att_0007_wording_pred</th>\n","      <th>debertav3large_lr16e-05_att_0007_content_pred</th>\n","      <th>debertav3large_lr17e-05_att_0007_wording_pred</th>\n","      <th>debertav3large_lr17e-05_att_0007_content_pred</th>\n","      <th>debertav3large_lr18e-05_att_0007_wording_pred</th>\n","      <th>debertav3large_lr18e-05_att_0007_content_pred</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.289577</td>\n","      <td>0.754494</td>\n","      <td>0.255495</td>\n","      <td>0.757381</td>\n","      <td>0.253911</td>\n","      <td>0.798586</td>\n","      <td>0.238143</td>\n","      <td>0.805690</td>\n","      <td>0.226564</td>\n","      <td>0.868932</td>\n","      <td>...</td>\n","      <td>0.050072</td>\n","      <td>0.570128</td>\n","      <td>-0.130428</td>\n","      <td>0.636849</td>\n","      <td>0.146617</td>\n","      <td>0.691859</td>\n","      <td>-0.035948</td>\n","      <td>0.880852</td>\n","      <td>-0.025384</td>\n","      <td>0.916015</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.702410</td>\n","      <td>-0.433037</td>\n","      <td>-0.801842</td>\n","      <td>-0.464458</td>\n","      <td>-0.772340</td>\n","      <td>-0.413345</td>\n","      <td>-0.665685</td>\n","      <td>-0.327464</td>\n","      <td>-0.641115</td>\n","      <td>-0.371242</td>\n","      <td>...</td>\n","      <td>-0.851928</td>\n","      <td>-0.071699</td>\n","      <td>-1.015396</td>\n","      <td>-0.227899</td>\n","      <td>-0.790502</td>\n","      <td>-0.701932</td>\n","      <td>-0.893634</td>\n","      <td>-0.387197</td>\n","      <td>-1.166269</td>\n","      <td>-0.696791</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.476874</td>\n","      <td>1.968442</td>\n","      <td>2.216656</td>\n","      <td>2.133213</td>\n","      <td>2.264356</td>\n","      <td>2.114351</td>\n","      <td>2.413981</td>\n","      <td>2.368245</td>\n","      <td>2.114522</td>\n","      <td>2.113158</td>\n","      <td>...</td>\n","      <td>2.413287</td>\n","      <td>2.190674</td>\n","      <td>2.293101</td>\n","      <td>2.366125</td>\n","      <td>1.455727</td>\n","      <td>0.879681</td>\n","      <td>2.042683</td>\n","      <td>2.011548</td>\n","      <td>2.101691</td>\n","      <td>2.138166</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-1.046302</td>\n","      <td>-0.884815</td>\n","      <td>-1.117746</td>\n","      <td>-0.960289</td>\n","      <td>-1.006900</td>\n","      <td>-0.931899</td>\n","      <td>-0.985633</td>\n","      <td>-0.782978</td>\n","      <td>-1.084898</td>\n","      <td>-0.928009</td>\n","      <td>...</td>\n","      <td>-1.108393</td>\n","      <td>-0.970286</td>\n","      <td>-1.204046</td>\n","      <td>-1.049266</td>\n","      <td>-0.896619</td>\n","      <td>-0.799949</td>\n","      <td>-1.103294</td>\n","      <td>-0.969418</td>\n","      <td>-1.277837</td>\n","      <td>-1.192987</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2.218238</td>\n","      <td>1.984394</td>\n","      <td>2.125614</td>\n","      <td>2.388381</td>\n","      <td>2.147007</td>\n","      <td>2.516407</td>\n","      <td>2.084431</td>\n","      <td>2.448008</td>\n","      <td>2.143604</td>\n","      <td>2.350667</td>\n","      <td>...</td>\n","      <td>2.187430</td>\n","      <td>2.335758</td>\n","      <td>2.327338</td>\n","      <td>2.474953</td>\n","      <td>2.385828</td>\n","      <td>2.251376</td>\n","      <td>1.824292</td>\n","      <td>2.288516</td>\n","      <td>2.421074</td>\n","      <td>2.707077</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7160</th>\n","      <td>-0.084784</td>\n","      <td>-0.080262</td>\n","      <td>-0.192514</td>\n","      <td>-0.029484</td>\n","      <td>-0.218292</td>\n","      <td>-0.110078</td>\n","      <td>-0.208576</td>\n","      <td>-0.138276</td>\n","      <td>-0.080752</td>\n","      <td>-0.022244</td>\n","      <td>...</td>\n","      <td>-0.128036</td>\n","      <td>0.122814</td>\n","      <td>-0.145221</td>\n","      <td>-0.106024</td>\n","      <td>-0.461274</td>\n","      <td>-0.669521</td>\n","      <td>-0.157177</td>\n","      <td>0.020587</td>\n","      <td>-0.237773</td>\n","      <td>-0.200248</td>\n","    </tr>\n","    <tr>\n","      <th>7161</th>\n","      <td>-0.433700</td>\n","      <td>-0.205239</td>\n","      <td>-0.335272</td>\n","      <td>-0.031181</td>\n","      <td>-0.366315</td>\n","      <td>-0.163821</td>\n","      <td>-0.431232</td>\n","      <td>-0.058000</td>\n","      <td>-0.449126</td>\n","      <td>-0.166257</td>\n","      <td>...</td>\n","      <td>-0.467294</td>\n","      <td>-0.368578</td>\n","      <td>-0.369047</td>\n","      <td>-0.174953</td>\n","      <td>-0.417107</td>\n","      <td>-0.321235</td>\n","      <td>-0.416164</td>\n","      <td>-0.077031</td>\n","      <td>-0.374409</td>\n","      <td>-0.275327</td>\n","    </tr>\n","    <tr>\n","      <th>7162</th>\n","      <td>-1.026422</td>\n","      <td>-1.043301</td>\n","      <td>-0.790768</td>\n","      <td>-0.539467</td>\n","      <td>-0.878920</td>\n","      <td>-0.593079</td>\n","      <td>-1.000468</td>\n","      <td>-0.721421</td>\n","      <td>-0.949790</td>\n","      <td>-0.683982</td>\n","      <td>...</td>\n","      <td>-0.897805</td>\n","      <td>-0.603563</td>\n","      <td>-1.005552</td>\n","      <td>-0.796761</td>\n","      <td>-0.633984</td>\n","      <td>-0.617932</td>\n","      <td>-1.140137</td>\n","      <td>-0.791222</td>\n","      <td>-0.894914</td>\n","      <td>-0.417379</td>\n","    </tr>\n","    <tr>\n","      <th>7163</th>\n","      <td>-0.099472</td>\n","      <td>0.458754</td>\n","      <td>0.150375</td>\n","      <td>0.407218</td>\n","      <td>0.054952</td>\n","      <td>0.478801</td>\n","      <td>-0.008228</td>\n","      <td>0.426154</td>\n","      <td>0.015571</td>\n","      <td>0.472524</td>\n","      <td>...</td>\n","      <td>-0.040969</td>\n","      <td>0.603768</td>\n","      <td>0.022594</td>\n","      <td>0.560333</td>\n","      <td>0.020776</td>\n","      <td>0.301249</td>\n","      <td>-0.103226</td>\n","      <td>0.218670</td>\n","      <td>-0.003400</td>\n","      <td>0.486490</td>\n","    </tr>\n","    <tr>\n","      <th>7164</th>\n","      <td>0.756706</td>\n","      <td>0.402938</td>\n","      <td>0.648505</td>\n","      <td>0.655756</td>\n","      <td>0.571203</td>\n","      <td>0.563511</td>\n","      <td>0.656650</td>\n","      <td>0.665906</td>\n","      <td>0.649003</td>\n","      <td>0.647377</td>\n","      <td>...</td>\n","      <td>0.905071</td>\n","      <td>0.387474</td>\n","      <td>0.823531</td>\n","      <td>0.858222</td>\n","      <td>0.850111</td>\n","      <td>0.038124</td>\n","      <td>0.989619</td>\n","      <td>0.582919</td>\n","      <td>0.700234</td>\n","      <td>0.151528</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7165 rows × 40 columns</p>\n","</div>"],"text/plain":["      upload_model/debertav3base_lr5e-05_wording_pred  \\\n","0                                            0.289577   \n","1                                           -0.702410   \n","2                                            2.476874   \n","3                                           -1.046302   \n","4                                            2.218238   \n","...                                               ...   \n","7160                                        -0.084784   \n","7161                                        -0.433700   \n","7162                                        -1.026422   \n","7163                                        -0.099472   \n","7164                                         0.756706   \n","\n","      upload_model/debertav3base_lr5e-05_content_pred  \\\n","0                                            0.754494   \n","1                                           -0.433037   \n","2                                            1.968442   \n","3                                           -0.884815   \n","4                                            1.984394   \n","...                                               ...   \n","7160                                        -0.080262   \n","7161                                        -0.205239   \n","7162                                        -1.043301   \n","7163                                         0.458754   \n","7164                                         0.402938   \n","\n","      upload_model/debertav3base_lr15e-05_wording_pred  \\\n","0                                             0.255495   \n","1                                            -0.801842   \n","2                                             2.216656   \n","3                                            -1.117746   \n","4                                             2.125614   \n","...                                                ...   \n","7160                                         -0.192514   \n","7161                                         -0.335272   \n","7162                                         -0.790768   \n","7163                                          0.150375   \n","7164                                          0.648505   \n","\n","      upload_model/debertav3base_lr15e-05_content_pred  \\\n","0                                             0.757381   \n","1                                            -0.464458   \n","2                                             2.133213   \n","3                                            -0.960289   \n","4                                             2.388381   \n","...                                                ...   \n","7160                                         -0.029484   \n","7161                                         -0.031181   \n","7162                                         -0.539467   \n","7163                                          0.407218   \n","7164                                          0.655756   \n","\n","      upload_model/debertav3base_lr17e-05_wording_pred  \\\n","0                                             0.253911   \n","1                                            -0.772340   \n","2                                             2.264356   \n","3                                            -1.006900   \n","4                                             2.147007   \n","...                                                ...   \n","7160                                         -0.218292   \n","7161                                         -0.366315   \n","7162                                         -0.878920   \n","7163                                          0.054952   \n","7164                                          0.571203   \n","\n","      upload_model/debertav3base_lr17e-05_content_pred  \\\n","0                                             0.798586   \n","1                                            -0.413345   \n","2                                             2.114351   \n","3                                            -0.931899   \n","4                                             2.516407   \n","...                                                ...   \n","7160                                         -0.110078   \n","7161                                         -0.163821   \n","7162                                         -0.593079   \n","7163                                          0.478801   \n","7164                                          0.563511   \n","\n","      upload_model/debertav3base_lr18e-05_wording_pred  \\\n","0                                             0.238143   \n","1                                            -0.665685   \n","2                                             2.413981   \n","3                                            -0.985633   \n","4                                             2.084431   \n","...                                                ...   \n","7160                                         -0.208576   \n","7161                                         -0.431232   \n","7162                                         -1.000468   \n","7163                                         -0.008228   \n","7164                                          0.656650   \n","\n","      upload_model/debertav3base_lr18e-05_content_pred  \\\n","0                                             0.805690   \n","1                                            -0.327464   \n","2                                             2.368245   \n","3                                            -0.782978   \n","4                                             2.448008   \n","...                                                ...   \n","7160                                         -0.138276   \n","7161                                         -0.058000   \n","7162                                         -0.721421   \n","7163                                          0.426154   \n","7164                                          0.665906   \n","\n","      upload_model/debertav3base_lr21e-05_wording_pred  \\\n","0                                             0.226564   \n","1                                            -0.641115   \n","2                                             2.114522   \n","3                                            -1.084898   \n","4                                             2.143604   \n","...                                                ...   \n","7160                                         -0.080752   \n","7161                                         -0.449126   \n","7162                                         -0.949790   \n","7163                                          0.015571   \n","7164                                          0.649003   \n","\n","      upload_model/debertav3base_lr21e-05_content_pred  ...  \\\n","0                                             0.868932  ...   \n","1                                            -0.371242  ...   \n","2                                             2.113158  ...   \n","3                                            -0.928009  ...   \n","4                                             2.350667  ...   \n","...                                                ...  ...   \n","7160                                         -0.022244  ...   \n","7161                                         -0.166257  ...   \n","7162                                         -0.683982  ...   \n","7163                                          0.472524  ...   \n","7164                                          0.647377  ...   \n","\n","      debertav3large_lr14e-05_att_0007_wording_pred  \\\n","0                                          0.050072   \n","1                                         -0.851928   \n","2                                          2.413287   \n","3                                         -1.108393   \n","4                                          2.187430   \n","...                                             ...   \n","7160                                      -0.128036   \n","7161                                      -0.467294   \n","7162                                      -0.897805   \n","7163                                      -0.040969   \n","7164                                       0.905071   \n","\n","      debertav3large_lr14e-05_att_0007_content_pred  \\\n","0                                          0.570128   \n","1                                         -0.071699   \n","2                                          2.190674   \n","3                                         -0.970286   \n","4                                          2.335758   \n","...                                             ...   \n","7160                                       0.122814   \n","7161                                      -0.368578   \n","7162                                      -0.603563   \n","7163                                       0.603768   \n","7164                                       0.387474   \n","\n","      debertav3large_lr15e-05_att_0007_wording_pred  \\\n","0                                         -0.130428   \n","1                                         -1.015396   \n","2                                          2.293101   \n","3                                         -1.204046   \n","4                                          2.327338   \n","...                                             ...   \n","7160                                      -0.145221   \n","7161                                      -0.369047   \n","7162                                      -1.005552   \n","7163                                       0.022594   \n","7164                                       0.823531   \n","\n","      debertav3large_lr15e-05_att_0007_content_pred  \\\n","0                                          0.636849   \n","1                                         -0.227899   \n","2                                          2.366125   \n","3                                         -1.049266   \n","4                                          2.474953   \n","...                                             ...   \n","7160                                      -0.106024   \n","7161                                      -0.174953   \n","7162                                      -0.796761   \n","7163                                       0.560333   \n","7164                                       0.858222   \n","\n","      debertav3large_lr16e-05_att_0007_wording_pred  \\\n","0                                          0.146617   \n","1                                         -0.790502   \n","2                                          1.455727   \n","3                                         -0.896619   \n","4                                          2.385828   \n","...                                             ...   \n","7160                                      -0.461274   \n","7161                                      -0.417107   \n","7162                                      -0.633984   \n","7163                                       0.020776   \n","7164                                       0.850111   \n","\n","      debertav3large_lr16e-05_att_0007_content_pred  \\\n","0                                          0.691859   \n","1                                         -0.701932   \n","2                                          0.879681   \n","3                                         -0.799949   \n","4                                          2.251376   \n","...                                             ...   \n","7160                                      -0.669521   \n","7161                                      -0.321235   \n","7162                                      -0.617932   \n","7163                                       0.301249   \n","7164                                       0.038124   \n","\n","      debertav3large_lr17e-05_att_0007_wording_pred  \\\n","0                                         -0.035948   \n","1                                         -0.893634   \n","2                                          2.042683   \n","3                                         -1.103294   \n","4                                          1.824292   \n","...                                             ...   \n","7160                                      -0.157177   \n","7161                                      -0.416164   \n","7162                                      -1.140137   \n","7163                                      -0.103226   \n","7164                                       0.989619   \n","\n","      debertav3large_lr17e-05_att_0007_content_pred  \\\n","0                                          0.880852   \n","1                                         -0.387197   \n","2                                          2.011548   \n","3                                         -0.969418   \n","4                                          2.288516   \n","...                                             ...   \n","7160                                       0.020587   \n","7161                                      -0.077031   \n","7162                                      -0.791222   \n","7163                                       0.218670   \n","7164                                       0.582919   \n","\n","      debertav3large_lr18e-05_att_0007_wording_pred  \\\n","0                                         -0.025384   \n","1                                         -1.166269   \n","2                                          2.101691   \n","3                                         -1.277837   \n","4                                          2.421074   \n","...                                             ...   \n","7160                                      -0.237773   \n","7161                                      -0.374409   \n","7162                                      -0.894914   \n","7163                                      -0.003400   \n","7164                                       0.700234   \n","\n","      debertav3large_lr18e-05_att_0007_content_pred  \n","0                                          0.916015  \n","1                                         -0.696791  \n","2                                          2.138166  \n","3                                         -1.192987  \n","4                                          2.707077  \n","...                                             ...  \n","7160                                      -0.200248  \n","7161                                      -0.275327  \n","7162                                      -0.417379  \n","7163                                       0.486490  \n","7164                                       0.151528  \n","\n","[7165 rows x 40 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["ensembling_results_val"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# replace upload_model/ with \"\"\n","# ensembling_results_val = ensembling_results_val.rename(columns=lambda x: x.replace(\"upload_model/\", \"\"))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>upload_model/debertav3base_lr5e-05_wording_pred</th>\n","      <th>upload_model/debertav3base_lr5e-05_content_pred</th>\n","      <th>upload_model/debertav3base_lr15e-05_wording_pred</th>\n","      <th>upload_model/debertav3base_lr15e-05_content_pred</th>\n","      <th>upload_model/debertav3base_lr17e-05_wording_pred</th>\n","      <th>upload_model/debertav3base_lr17e-05_content_pred</th>\n","      <th>upload_model/debertav3base_lr18e-05_wording_pred</th>\n","      <th>upload_model/debertav3base_lr18e-05_content_pred</th>\n","      <th>upload_model/debertav3base_lr21e-05_wording_pred</th>\n","      <th>upload_model/debertav3base_lr21e-05_content_pred</th>\n","      <th>...</th>\n","      <th>debertav3large_lr14e-05_att_0007_wording_pred</th>\n","      <th>debertav3large_lr14e-05_att_0007_content_pred</th>\n","      <th>debertav3large_lr15e-05_att_0007_wording_pred</th>\n","      <th>debertav3large_lr15e-05_att_0007_content_pred</th>\n","      <th>debertav3large_lr16e-05_att_0007_wording_pred</th>\n","      <th>debertav3large_lr16e-05_att_0007_content_pred</th>\n","      <th>debertav3large_lr17e-05_att_0007_wording_pred</th>\n","      <th>debertav3large_lr17e-05_att_0007_content_pred</th>\n","      <th>debertav3large_lr18e-05_att_0007_wording_pred</th>\n","      <th>debertav3large_lr18e-05_att_0007_content_pred</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.289577</td>\n","      <td>0.754494</td>\n","      <td>0.255495</td>\n","      <td>0.757381</td>\n","      <td>0.253911</td>\n","      <td>0.798586</td>\n","      <td>0.238143</td>\n","      <td>0.805690</td>\n","      <td>0.226564</td>\n","      <td>0.868932</td>\n","      <td>...</td>\n","      <td>0.050072</td>\n","      <td>0.570128</td>\n","      <td>-0.130428</td>\n","      <td>0.636849</td>\n","      <td>0.146617</td>\n","      <td>0.691859</td>\n","      <td>-0.035948</td>\n","      <td>0.880852</td>\n","      <td>-0.025384</td>\n","      <td>0.916015</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.702410</td>\n","      <td>-0.433037</td>\n","      <td>-0.801842</td>\n","      <td>-0.464458</td>\n","      <td>-0.772340</td>\n","      <td>-0.413345</td>\n","      <td>-0.665685</td>\n","      <td>-0.327464</td>\n","      <td>-0.641115</td>\n","      <td>-0.371242</td>\n","      <td>...</td>\n","      <td>-0.851928</td>\n","      <td>-0.071699</td>\n","      <td>-1.015396</td>\n","      <td>-0.227899</td>\n","      <td>-0.790502</td>\n","      <td>-0.701932</td>\n","      <td>-0.893634</td>\n","      <td>-0.387197</td>\n","      <td>-1.166269</td>\n","      <td>-0.696791</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.476874</td>\n","      <td>1.968442</td>\n","      <td>2.216656</td>\n","      <td>2.133213</td>\n","      <td>2.264356</td>\n","      <td>2.114351</td>\n","      <td>2.413981</td>\n","      <td>2.368245</td>\n","      <td>2.114522</td>\n","      <td>2.113158</td>\n","      <td>...</td>\n","      <td>2.413287</td>\n","      <td>2.190674</td>\n","      <td>2.293101</td>\n","      <td>2.366125</td>\n","      <td>1.455727</td>\n","      <td>0.879681</td>\n","      <td>2.042683</td>\n","      <td>2.011548</td>\n","      <td>2.101691</td>\n","      <td>2.138166</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-1.046302</td>\n","      <td>-0.884815</td>\n","      <td>-1.117746</td>\n","      <td>-0.960289</td>\n","      <td>-1.006900</td>\n","      <td>-0.931899</td>\n","      <td>-0.985633</td>\n","      <td>-0.782978</td>\n","      <td>-1.084898</td>\n","      <td>-0.928009</td>\n","      <td>...</td>\n","      <td>-1.108393</td>\n","      <td>-0.970286</td>\n","      <td>-1.204046</td>\n","      <td>-1.049266</td>\n","      <td>-0.896619</td>\n","      <td>-0.799949</td>\n","      <td>-1.103294</td>\n","      <td>-0.969418</td>\n","      <td>-1.277837</td>\n","      <td>-1.192987</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2.218238</td>\n","      <td>1.984394</td>\n","      <td>2.125614</td>\n","      <td>2.388381</td>\n","      <td>2.147007</td>\n","      <td>2.516407</td>\n","      <td>2.084431</td>\n","      <td>2.448008</td>\n","      <td>2.143604</td>\n","      <td>2.350667</td>\n","      <td>...</td>\n","      <td>2.187430</td>\n","      <td>2.335758</td>\n","      <td>2.327338</td>\n","      <td>2.474953</td>\n","      <td>2.385828</td>\n","      <td>2.251376</td>\n","      <td>1.824292</td>\n","      <td>2.288516</td>\n","      <td>2.421074</td>\n","      <td>2.707077</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 40 columns</p>\n","</div>"],"text/plain":["   upload_model/debertav3base_lr5e-05_wording_pred  \\\n","0                                         0.289577   \n","1                                        -0.702410   \n","2                                         2.476874   \n","3                                        -1.046302   \n","4                                         2.218238   \n","\n","   upload_model/debertav3base_lr5e-05_content_pred  \\\n","0                                         0.754494   \n","1                                        -0.433037   \n","2                                         1.968442   \n","3                                        -0.884815   \n","4                                         1.984394   \n","\n","   upload_model/debertav3base_lr15e-05_wording_pred  \\\n","0                                          0.255495   \n","1                                         -0.801842   \n","2                                          2.216656   \n","3                                         -1.117746   \n","4                                          2.125614   \n","\n","   upload_model/debertav3base_lr15e-05_content_pred  \\\n","0                                          0.757381   \n","1                                         -0.464458   \n","2                                          2.133213   \n","3                                         -0.960289   \n","4                                          2.388381   \n","\n","   upload_model/debertav3base_lr17e-05_wording_pred  \\\n","0                                          0.253911   \n","1                                         -0.772340   \n","2                                          2.264356   \n","3                                         -1.006900   \n","4                                          2.147007   \n","\n","   upload_model/debertav3base_lr17e-05_content_pred  \\\n","0                                          0.798586   \n","1                                         -0.413345   \n","2                                          2.114351   \n","3                                         -0.931899   \n","4                                          2.516407   \n","\n","   upload_model/debertav3base_lr18e-05_wording_pred  \\\n","0                                          0.238143   \n","1                                         -0.665685   \n","2                                          2.413981   \n","3                                         -0.985633   \n","4                                          2.084431   \n","\n","   upload_model/debertav3base_lr18e-05_content_pred  \\\n","0                                          0.805690   \n","1                                         -0.327464   \n","2                                          2.368245   \n","3                                         -0.782978   \n","4                                          2.448008   \n","\n","   upload_model/debertav3base_lr21e-05_wording_pred  \\\n","0                                          0.226564   \n","1                                         -0.641115   \n","2                                          2.114522   \n","3                                         -1.084898   \n","4                                          2.143604   \n","\n","   upload_model/debertav3base_lr21e-05_content_pred  ...  \\\n","0                                          0.868932  ...   \n","1                                         -0.371242  ...   \n","2                                          2.113158  ...   \n","3                                         -0.928009  ...   \n","4                                          2.350667  ...   \n","\n","   debertav3large_lr14e-05_att_0007_wording_pred  \\\n","0                                       0.050072   \n","1                                      -0.851928   \n","2                                       2.413287   \n","3                                      -1.108393   \n","4                                       2.187430   \n","\n","   debertav3large_lr14e-05_att_0007_content_pred  \\\n","0                                       0.570128   \n","1                                      -0.071699   \n","2                                       2.190674   \n","3                                      -0.970286   \n","4                                       2.335758   \n","\n","   debertav3large_lr15e-05_att_0007_wording_pred  \\\n","0                                      -0.130428   \n","1                                      -1.015396   \n","2                                       2.293101   \n","3                                      -1.204046   \n","4                                       2.327338   \n","\n","   debertav3large_lr15e-05_att_0007_content_pred  \\\n","0                                       0.636849   \n","1                                      -0.227899   \n","2                                       2.366125   \n","3                                      -1.049266   \n","4                                       2.474953   \n","\n","   debertav3large_lr16e-05_att_0007_wording_pred  \\\n","0                                       0.146617   \n","1                                      -0.790502   \n","2                                       1.455727   \n","3                                      -0.896619   \n","4                                       2.385828   \n","\n","   debertav3large_lr16e-05_att_0007_content_pred  \\\n","0                                       0.691859   \n","1                                      -0.701932   \n","2                                       0.879681   \n","3                                      -0.799949   \n","4                                       2.251376   \n","\n","   debertav3large_lr17e-05_att_0007_wording_pred  \\\n","0                                      -0.035948   \n","1                                      -0.893634   \n","2                                       2.042683   \n","3                                      -1.103294   \n","4                                       1.824292   \n","\n","   debertav3large_lr17e-05_att_0007_content_pred  \\\n","0                                       0.880852   \n","1                                      -0.387197   \n","2                                       2.011548   \n","3                                      -0.969418   \n","4                                       2.288516   \n","\n","   debertav3large_lr18e-05_att_0007_wording_pred  \\\n","0                                      -0.025384   \n","1                                      -1.166269   \n","2                                       2.101691   \n","3                                      -1.277837   \n","4                                       2.421074   \n","\n","   debertav3large_lr18e-05_att_0007_content_pred  \n","0                                       0.916015  \n","1                                      -0.696791  \n","2                                       2.138166  \n","3                                      -1.192987  \n","4                                       2.707077  \n","\n","[5 rows x 40 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["ensembling_results_val.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Find the best weight with optuna"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# CFG.list_model_infer = [\n","#         # 'upload_model/debertav3base_lr15e-05',\n","#         'upload_model/debertav3base_lr17e-05', #keep\n","#         'upload_model/debertav3base_lr18e-05', #keep\n","#         'upload_model/debertav3base_lr21e-05', # keep\n","#         'upload_model/debertav3base_lr22e-05', #keep \n","#         # 'upload_model/debertav3base_lr5e-05', \n","#         'upload_model/debertav3large_lr12e-05', # upload\n","#         'upload_model/debertav3large_lr13e-05',  # upload\n","#         # 'debertav3large_lr1e-05_save',\n","#         # 'debertav3large_lr1e-05_att_0007',\n","#         'debertav3large_lr8e-06_att_0007', # upload \n","#         'debertav3large_lr9e-06_att_0007', # upload \n","#         'debertav3large_lr11e-05_att_0007',\n","#         'debertav3large_lr12e-05_att_0007',\n","#         'debertav3large_lr13e-05_att_0007',\n","#         'debertav3large_lr14e-05_att_0007',\n","#         'debertav3large_lr15e-05_att_0007', # upload \n","#         'debertav3large_lr16e-05_att_0007', # upload \n","#         'debertav3large_lr17e-05_att_0007', # upload \n","#         'debertav3large_lr18e-05_att_0007', # upload \n","#         ]\n","# print(len(CFG.list_model_infer))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# weight_for_model = {}\n","# scale = 0 \n","# for model in CFG.list_model_infer:\n","#     results = ensembling_results_val[[f\"{model}_wording_pred\", f\"{model}_content_pred\"]]\n","#     mcrmse = compute_mcrmse((results.values, train[targets].values))\n","#     print(f\"{model} mcrmse: {mcrmse['mcrmse']}\")\n","#     # weight_for_model[model] = 1 / mcrmse[\"mcrmse\"]\n","#     # scale = scale + weight_for_model[model]\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["16\n","scale 6.7863581942530455\n"]}],"source":["weight_for_model =  {       \n","    \"upload_model/debertav3base_lr17e-05\": 0.212909408976402    ,\n","    \"upload_model/debertav3base_lr18e-05\": 0.1421852032796326,\n","    \"upload_model/debertav3base_lr21e-05\": 0.8953672127134387,\n","    \"upload_model/debertav3base_lr22e-05\": 0.36047950541959095,\n","    \"upload_model/debertav3large_lr12e-05\": 0.9413883882391965,\n","    \"upload_model/debertav3large_lr13e-05\": 0.48278638692252185,\n","    \"debertav3large_lr8e-06_att_0007\": 0.30564935544213445,\n","    \"debertav3large_lr9e-06_att_0007\": 0.40560485262757806,\n","    \"debertav3large_lr11e-05_att_0007\": 0.882846667614644,\n","    \"debertav3large_lr12e-05_att_0007\": 0.09507155361092456,\n","    \"debertav3large_lr13e-05_att_0007\": 0.6241475562582361,\n","    \"debertav3large_lr14e-05_att_0007\": 0.03402452758512511,\n","    \"debertav3large_lr15e-05_att_0007\": 0.11628723374671882,\n","    \"debertav3large_lr16e-05_att_0007\": 0.0001281844830617676,\n","    \"debertav3large_lr17e-05_att_0007\": 0.969431755664122,\n","    \"debertav3large_lr18e-05_att_0007\": 0.318050401669718,\n","}\n","print(len(weight_for_model))\n","scale = np.sum(list(weight_for_model.values()))\n","print('scale', scale)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train = pd.read_csv(\"input/train.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for model in CFG.list_model_infer:\n","    weight_for_model[model] = 1\n","scale = np.sum(list(weight_for_model.values()))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train[f\"wording_pred\"] = np.sum([ensembling_results_val[f\"{model}_wording_pred\"] * weight_for_model[model] for model in CFG.list_model_infer], axis=0) / scale    \n","train[f\"content_pred\"] = np.sum([ensembling_results_val[f\"{model}_content_pred\"] * weight_for_model[model] for model in CFG.list_model_infer], axis=0) / scale\n","# test[f\"wording_pred\"] = np.sum([ensembling_results_test[f\"{model}_wording_pred\"] * weight_for_model[model] for model in CFG.list_model_infer], axis=0) / scale\n","# test[f\"content_pred\"] = np.sum([ensembling_results_test[f\"{model}_content_pred\"] * weight_for_model[model] for model in CFG.list_model_infer], axis=0) / scale\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save train and test\n","# train.to_csv(\"input/train.csv\", index=False)\n","# load train and test\n","# test = pd.read_csv(\"input/test.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["targets = [\"content\", \"wording\"]\n","\n","drop_columns = [\"fold\", \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n","                \"prompt_question\", \"prompt_title\", \n","                \"prompt_text\"\n","               ] + targets"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def create_model_dict(targets,train):\n","  model_dict = {}\n","  for target in targets:\n","      models = []\n","\n","      for fold in range(CFG.n_splits):\n","          X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns, inplace=False)\n","          y_train_cv = train[train[\"fold\"] != fold][target]\n","\n","          X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n","          y_eval_cv = train[train[\"fold\"] == fold][target]\n","\n","          dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n","          dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n","\n","          params = {\n","              'boosting_type': 'gbdt',\n","              'random_state': 42,\n","              'objective': 'regression',\n","              'metric': 'rmse',\n","              'learning_rate': 0.048,\n","              'max_depth': 3,\n","              'lambda_l1': 0.0,\n","              'lambda_l2': 0.011,\n","              'verbose': -1,\n","          }\n","\n","          evaluation_results = {}\n","          model = lgb.train(params,\n","                            num_boost_round=10000,\n","                            valid_names=['train', 'valid'],\n","                            train_set=dtrain,\n","                            valid_sets=dval,\n","                            callbacks=[\n","                                lgb.early_stopping(stopping_rounds=70, verbose=False),\n","                                # lgb.log_evaluation(100),\n","                                lgb.callback.record_evaluation(evaluation_results)\n","                              ],\n","                            )\n","          models.append(model)\n","\n","      model_dict[target] = models\n","  return model_dict\n","model_dict = create_model_dict(targets,train)\n"]},{"cell_type":"markdown","metadata":{},"source":["## CV Score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["mcrmse: 1.0641848929195725\n"]}],"source":["# cv\n","# import optuna\n","def cal_mcrmse(model_dict, targets):\n","    rmses = []\n","    for target in targets:\n","        models = model_dict[target]\n","\n","        preds = []\n","        trues = []\n","        \n","        for fold, model in enumerate(models):\n","            X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns , inplace=False)\n","            y_eval_cv = train[train[\"fold\"] == fold][target]\n","\n","            pred = model.predict(X_eval_cv)\n","\n","            trues.extend(y_eval_cv)\n","            preds.extend(pred)\n","            \n","        rmse = np.sqrt(mean_squared_error(trues, preds))\n","        # print(f\"{target}_rmse : {rmse}\")\n","        rmses = rmses + [rmse]\n","    return sum(rmses) / len(rmses)\n","mcrmse = cal_mcrmse(model_dict, targets)\n","print(f\"mcrmse: {mcrmse}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Optuna"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import optuna\n","def objective(trial):\n","    weight_for_model = {}\n","    for model in CFG.list_model_infer:\n","        weight_for_model[model] = trial.suggest_float(model, -0.5, 1.0)\n","    scale = np.sum([weight_for_model[model] for model in CFG.list_model_infer])\n","    train[f\"wording_pred\"] = np.sum([ensembling_results_val[f\"{model}_wording_pred\"] * weight_for_model[model] for model in CFG.list_model_infer], axis=0) / scale    \n","    train[f\"content_pred\"] = np.sum([ensembling_results_val[f\"{model}_content_pred\"] * weight_for_model[model] for model in CFG.list_model_infer], axis=0) / scale\n","    model_dict = create_model_dict(targets,train)\n","    lost = cal_mcrmse(model_dict, targets)\n","    print(f\"mcrmse: {lost}\")\n","    # print(f\"weight_for_model: {weight_for_model}\")\n","    return lost"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["mcrmse: 0.4842790883422752\n","mcrmse: 0.48193215160166303\n","mcrmse: 0.4882895789147218\n","mcrmse: 0.48268891130391006\n","mcrmse: 0.5065932850412581\n","mcrmse: 0.4885858896374236\n","mcrmse: 0.48480572815055534\n","mcrmse: 0.49032664563749934\n","mcrmse: 0.4823207042880988\n","mcrmse: 0.49948450434247676\n","mcrmse: 0.4841628917664917\n","mcrmse: 0.48341097959343043\n","mcrmse: 0.48673718746294226\n","mcrmse: 0.4829622375030531\n","mcrmse: 0.4820785899612382\n","mcrmse: 0.48156687710072676\n","mcrmse: 0.4836273295673663\n","mcrmse: 0.4794385294815756\n","mcrmse: 0.4802068429308679\n","mcrmse: 0.48025715570661986\n","mcrmse: 0.48046003812444016\n","mcrmse: 0.4801421647134472\n","mcrmse: 0.4802825342619865\n","mcrmse: 0.47932986118615\n","mcrmse: 0.48054872612151317\n","mcrmse: 0.4792728334449834\n","mcrmse: 0.47886721433626933\n","mcrmse: 0.47893867061690254\n","mcrmse: 0.4804847763696122\n","mcrmse: 0.4794998538312514\n","mcrmse: 0.480440017151915\n","mcrmse: 0.4794425253592045\n","mcrmse: 0.48012817063668956\n","mcrmse: 0.48063366520338335\n","mcrmse: 0.4793793351820571\n","mcrmse: 0.48027276279550557\n","mcrmse: 0.48113745563370003\n","mcrmse: 0.48039912254654504\n","mcrmse: 0.48047693116714973\n","mcrmse: 0.4799177023561783\n","mcrmse: 0.4840187036058745\n","mcrmse: 0.48002541253044023\n","mcrmse: 0.4786468742968768\n","mcrmse: 0.47879983798001213\n","mcrmse: 0.47996528082602774\n","mcrmse: 0.4797168492370901\n","mcrmse: 0.4788539543503084\n","mcrmse: 0.4798269924942885\n","mcrmse: 0.48020280760246437\n","mcrmse: 0.4801350171596309\n","mcrmse: 0.4808212277851359\n","mcrmse: 0.4789573902649635\n","mcrmse: 0.4793413801939532\n","mcrmse: 0.4793050388236159\n","mcrmse: 0.47940834716000724\n","mcrmse: 0.4799566249678438\n","mcrmse: 0.47925565300205364\n","mcrmse: 0.4801861431851371\n","mcrmse: 0.47972717932486675\n","mcrmse: 0.48053744726711733\n","mcrmse: 0.4802846656748152\n","mcrmse: 0.48002697558510543\n","mcrmse: 0.4786214863755811\n","mcrmse: 0.4793454826661919\n","mcrmse: 0.47960248995490395\n","mcrmse: 0.47881716997223106\n","mcrmse: 0.4796420132611858\n","mcrmse: 0.4796386643129031\n","mcrmse: 0.4785306519505583\n","mcrmse: 0.47898568149324267\n","mcrmse: 0.4789284153082657\n","mcrmse: 0.47863846509148844\n","mcrmse: 0.47902014181722163\n","mcrmse: 0.47857297567034596\n","mcrmse: 0.47959008141077064\n","mcrmse: 0.4785185843255557\n","mcrmse: 0.48003532487022454\n","mcrmse: 0.4786927346450244\n","mcrmse: 0.4794580728668126\n","mcrmse: 0.479660950520031\n","mcrmse: 0.48072317886230875\n","mcrmse: 0.4794766396343968\n","mcrmse: 0.4790973775522486\n","mcrmse: 0.4801211460299427\n","mcrmse: 0.47929657494948585\n","mcrmse: 0.4799348001776652\n","mcrmse: 0.47953410013760445\n","mcrmse: 0.4799469525079173\n","mcrmse: 0.4790494102053765\n","mcrmse: 0.4795421238798495\n","mcrmse: 0.4791073877684784\n","mcrmse: 0.4792445043653375\n","mcrmse: 0.4787971584369444\n","mcrmse: 0.479556351139509\n","mcrmse: 0.47891837528900694\n","mcrmse: 0.47889474909790597\n","mcrmse: 0.4787599124816533\n","mcrmse: 0.4800399384807285\n","mcrmse: 0.4794005614795165\n","mcrmse: 0.47870933870180327\n","mcrmse: 0.47850594991669376\n","mcrmse: 0.47990613338920685\n","mcrmse: 0.479860202089533\n","mcrmse: 0.47902282019080455\n","mcrmse: 0.4796514067139565\n","mcrmse: 0.4795884357638971\n","mcrmse: 0.4805161365864744\n","mcrmse: 0.47814747868455043\n","mcrmse: 0.4794722188596994\n","mcrmse: 0.47963789033982734\n","mcrmse: 0.4794839733885357\n","mcrmse: 0.47962403183532293\n","mcrmse: 0.47922253136251247\n","mcrmse: 0.47930134427500126\n","mcrmse: 0.47982684765236006\n","mcrmse: 0.4795400507783391\n","mcrmse: 0.4801730859754263\n","mcrmse: 0.4792029917553497\n","mcrmse: 0.47916685483070864\n","mcrmse: 0.47957717233771463\n","mcrmse: 0.4792051449722498\n","mcrmse: 0.47930130789886427\n","mcrmse: 0.47899795692178404\n","mcrmse: 0.47849531288999997\n","mcrmse: 0.47918680205098496\n","mcrmse: 0.47879109165927036\n","mcrmse: 0.4781400435034817\n","mcrmse: 0.4791634298091597\n","mcrmse: 0.47864070956193117\n","mcrmse: 0.47939477632198024\n","mcrmse: 0.4788806252859891\n","mcrmse: 0.4797355230570679\n","mcrmse: 0.4784580430612465\n","mcrmse: 0.47886471392215524\n","mcrmse: 0.47921284979276485\n","mcrmse: 0.4789129021760573\n","mcrmse: 0.479828969844167\n","mcrmse: 0.4791021980961595\n","mcrmse: 0.4799999029839893\n","mcrmse: 0.4800044279476809\n","mcrmse: 0.47871447711642323\n","mcrmse: 0.47806199977527747\n","mcrmse: 0.47922857824585874\n","mcrmse: 0.47910178520615443\n","mcrmse: 0.47824306313642245\n","mcrmse: 0.47907294713765614\n","mcrmse: 0.4788128510028052\n","mcrmse: 0.47816284869335046\n","mcrmse: 0.4792756102872439\n","mcrmse: 0.4803029488586522\n","mcrmse: 0.4788500820207172\n","mcrmse: 0.4789554211998339\n","mcrmse: 0.47990796043219963\n","mcrmse: 0.47854759752562803\n","mcrmse: 0.4790207980492752\n","mcrmse: 0.4787032302154672\n","mcrmse: 0.47883587409428074\n","mcrmse: 0.4795290780251654\n","mcrmse: 0.4795685897133266\n","mcrmse: 0.4796697721624005\n","mcrmse: 0.47937540480239554\n","mcrmse: 0.4795167853793154\n","mcrmse: 0.4794468029264336\n","mcrmse: 0.4784261545979901\n","mcrmse: 0.4786153511376132\n","mcrmse: 0.4797212723163264\n","mcrmse: 0.4799581413617148\n","mcrmse: 0.4785493040171004\n","mcrmse: 0.478512178586635\n","mcrmse: 0.4799588353971033\n","mcrmse: 0.47897735190046475\n","mcrmse: 0.47842330935884564\n","mcrmse: 0.4792322075415595\n","mcrmse: 0.4789564807625628\n","mcrmse: 0.47796189776901377\n","mcrmse: 0.4800025370169462\n","mcrmse: 0.4791270307856459\n","mcrmse: 0.47945734513709903\n","mcrmse: 0.4786197845991611\n","mcrmse: 0.47979590339451006\n","mcrmse: 0.4781373397460258\n","mcrmse: 0.4792862665524281\n","mcrmse: 0.47934697760863443\n","mcrmse: 0.47857859050127716\n","mcrmse: 0.4791710135878464\n","mcrmse: 0.4797195825549065\n","mcrmse: 0.4784082388617834\n","mcrmse: 0.4781326085347852\n","mcrmse: 0.4797413169253202\n","mcrmse: 0.47862172548681214\n","mcrmse: 0.478732562199391\n","mcrmse: 0.4777914572452091\n","mcrmse: 0.47827324482829403\n","mcrmse: 0.4780524364663118\n","mcrmse: 0.47917097737240844\n","mcrmse: 0.47890718983494984\n","mcrmse: 0.47921925849546043\n","mcrmse: 0.47975054247378257\n","mcrmse: 0.4784166629218909\n","mcrmse: 0.4788305555907572\n","mcrmse: 0.47915276923250616\n","mcrmse: 0.47941534428032984\n","mcrmse: 0.47874289729730646\n","mcrmse: 0.4787413333413787\n","mcrmse: 0.47857440791861305\n","mcrmse: 0.47894536917460595\n","mcrmse: 0.4788311364927377\n","mcrmse: 0.47943521023621644\n","mcrmse: 0.47951278691265264\n","mcrmse: 0.4795004690317717\n","mcrmse: 0.4782648484991813\n","mcrmse: 0.4784940414031551\n","mcrmse: 0.47837483738388775\n","mcrmse: 0.47937707882913183\n","mcrmse: 0.4786237581122923\n","mcrmse: 0.479959848757952\n","mcrmse: 0.4808068881039792\n","mcrmse: 0.47996818755601045\n","mcrmse: 0.4792859241211246\n","mcrmse: 0.47943326588611\n","mcrmse: 0.47927935713663417\n","mcrmse: 0.479879303486193\n","mcrmse: 0.4792229366631326\n","mcrmse: 0.479410891679407\n","mcrmse: 0.4802162396813451\n","mcrmse: 0.4787455663327244\n","mcrmse: 0.4801777308552623\n","mcrmse: 0.4797332958480871\n","mcrmse: 0.48152443428010083\n","mcrmse: 0.48042142352653266\n","mcrmse: 0.4806685368048247\n","mcrmse: 0.47864273698622595\n","mcrmse: 0.47980869071271764\n","mcrmse: 0.47975775978044155\n","mcrmse: 0.48004959844131256\n","mcrmse: 0.4782624852386924\n","mcrmse: 0.4801218764299737\n","mcrmse: 0.4784120578023696\n","mcrmse: 0.4804518616989107\n","mcrmse: 0.4794911906952454\n","mcrmse: 0.47916830511098973\n","mcrmse: 0.47807866986658654\n","mcrmse: 0.4791121215701703\n","mcrmse: 0.47951037637952476\n","mcrmse: 0.47934423971439494\n","mcrmse: 0.48048748552013987\n","mcrmse: 0.4815290177233519\n","mcrmse: 0.47886102773052874\n","mcrmse: 0.4788554498857259\n","mcrmse: 0.4793290366445745\n","mcrmse: 0.47778869759031795\n","mcrmse: 0.4782953732902397\n","mcrmse: 0.4780383522981626\n","mcrmse: 0.4783063724191403\n","mcrmse: 0.479551197524837\n","mcrmse: 0.47826444866008666\n","mcrmse: 0.4788515096593874\n","mcrmse: 0.478780341115809\n","mcrmse: 0.4796740863819748\n","mcrmse: 0.47871254841895405\n","mcrmse: 0.47889096176114776\n","mcrmse: 0.47861782597372926\n","mcrmse: 0.47961145035015595\n","mcrmse: 0.480343898431677\n","mcrmse: 0.48118940678906197\n","mcrmse: 0.478406458172005\n","mcrmse: 0.47940777901532305\n","mcrmse: 0.47763943087001764\n","mcrmse: 0.478569682973746\n","mcrmse: 0.479334872320553\n","mcrmse: 0.4805193436105377\n","mcrmse: 0.4786897492234993\n","mcrmse: 0.4800529791168115\n","mcrmse: 0.47967595676620217\n","mcrmse: 0.47908195004291154\n","mcrmse: 0.47838326747251225\n","mcrmse: 0.4791407244910658\n","mcrmse: 0.4777535539102905\n","mcrmse: 0.47829411263061256\n","mcrmse: 0.4778029776386681\n","mcrmse: 0.4779556113237057\n","mcrmse: 0.47835221703450803\n","mcrmse: 0.47902218813786723\n","mcrmse: 0.47866325232968143\n","mcrmse: 0.4778566532651043\n","mcrmse: 0.4782205702521508\n","mcrmse: 0.4780360080399544\n","mcrmse: 0.4778127152143252\n","mcrmse: 0.4784376513790875\n","mcrmse: 0.47905979085681805\n","mcrmse: 0.4779092333256937\n","mcrmse: 0.47987791100392585\n","mcrmse: 0.4783856937364961\n","mcrmse: 0.48007388103897847\n","mcrmse: 0.4799870683766325\n","mcrmse: 0.4794894978827372\n","mcrmse: 0.4782837026486779\n","mcrmse: 0.4778480812409235\n","mcrmse: 0.4791032004410076\n","mcrmse: 0.4793052255666708\n","mcrmse: 0.479507595278446\n","mcrmse: 0.4773347139866894\n","mcrmse: 0.479199154073069\n","mcrmse: 0.4783848826881091\n","mcrmse: 0.47901978110832494\n","mcrmse: 0.48282414442407817\n","mcrmse: 0.4791611116186074\n","mcrmse: 0.47844769853527946\n","mcrmse: 0.4804061010792281\n","mcrmse: 0.478588336273474\n","mcrmse: 0.4990462629605338\n","mcrmse: 0.4839520762175532\n","mcrmse: 0.478586938558543\n","mcrmse: 0.48166696985372154\n","mcrmse: 0.47988633202201547\n","mcrmse: 0.4830866228243306\n","mcrmse: 0.48051938759754365\n","mcrmse: 0.4839893286341246\n","mcrmse: 0.48161183474755775\n","mcrmse: 0.4792025370428523\n","mcrmse: 0.48307258722554125\n","mcrmse: 0.4825378978449267\n","mcrmse: 0.4783243999894571\n","mcrmse: 0.48069696820540675\n","mcrmse: 0.48301293982537635\n","mcrmse: 0.4794945179801039\n","mcrmse: 0.4843809553998325\n","mcrmse: 0.47961663771614504\n","mcrmse: 0.4824513491877573\n","mcrmse: 0.4784698027047587\n","mcrmse: 0.4809051699513738\n","mcrmse: 0.47972488854950845\n","mcrmse: 0.479444005403586\n","mcrmse: 0.4788143939383752\n","mcrmse: 0.47947890735610565\n","mcrmse: 0.4783117933914993\n","mcrmse: 0.4812402883011054\n","mcrmse: 0.48012626339389053\n","mcrmse: 0.47929170345266825\n","mcrmse: 0.48036446127430266\n","mcrmse: 0.48402384546865374\n","mcrmse: 0.48142357298490596\n","mcrmse: 0.47934745022436326\n","mcrmse: 0.4845991330582995\n","mcrmse: 0.47800195445995497\n","mcrmse: 0.4782101332404578\n","mcrmse: 0.4789597364152688\n","mcrmse: 0.4782660687560647\n","mcrmse: 0.4781558935888389\n","mcrmse: 0.4786148418248718\n","mcrmse: 0.4815411551423243\n","mcrmse: 0.47814284621409187\n","mcrmse: 0.47744204214531616\n","mcrmse: 0.47834019639002634\n","mcrmse: 0.4785529700614607\n","mcrmse: 0.478532258359699\n","mcrmse: 0.4778483753906059\n","mcrmse: 0.48009784885211754\n","mcrmse: 0.47933572846046457\n","mcrmse: 0.4796041138560228\n","mcrmse: 0.4790313633116519\n","mcrmse: 0.4789411784695258\n","mcrmse: 0.4804669251313677\n","mcrmse: 0.4790305703059196\n","mcrmse: 0.47833177160158\n","mcrmse: 0.47852999061478696\n","mcrmse: 0.47941522739611664\n","mcrmse: 0.47959812509863553\n","mcrmse: 0.481802632811551\n","mcrmse: 0.47933722040416726\n","mcrmse: 0.4815354092931159\n","mcrmse: 0.48205587450002774\n","mcrmse: 0.47982237879430406\n","mcrmse: 0.48151092971424375\n","mcrmse: 0.4780886078487655\n","mcrmse: 0.47825121912822066\n","mcrmse: 0.4782481049910292\n","mcrmse: 0.4792468918659878\n","mcrmse: 0.4814163571612531\n","mcrmse: 0.4782502864570607\n","mcrmse: 0.48041705513499006\n","mcrmse: 0.4791250292762108\n","mcrmse: 0.4799153412656987\n","mcrmse: 0.4802867331433255\n","mcrmse: 0.4795657598925266\n","mcrmse: 0.48228082040599934\n","mcrmse: 0.4790444441300903\n","mcrmse: 0.47724243965045127\n","mcrmse: 0.4771354845817847\n","mcrmse: 0.4767011222034463\n","mcrmse: 0.47778129019813376\n","mcrmse: 0.4804866032385555\n","mcrmse: 0.47748765671943066\n","mcrmse: 0.47818029909735593\n","mcrmse: 0.4781473526367107\n","mcrmse: 0.4788113102707029\n","mcrmse: 0.4770952511246528\n","mcrmse: 0.477461807131346\n","mcrmse: 0.47795913268703516\n","mcrmse: 0.47755243381161816\n","mcrmse: 0.4775922249087031\n","mcrmse: 0.47777600807947157\n","mcrmse: 0.47754570743250274\n","mcrmse: 0.4777648060407076\n","mcrmse: 0.4787458787250122\n","mcrmse: 0.4779988006344992\n","mcrmse: 0.47807742506826423\n","mcrmse: 0.4775170839030856\n","mcrmse: 0.47768434904219215\n","mcrmse: 0.47758387064171737\n","mcrmse: 0.4776581301404913\n","mcrmse: 0.4771457735043756\n","mcrmse: 0.47773892215699454\n","mcrmse: 0.4772926321929012\n","mcrmse: 0.4773823831341255\n","mcrmse: 0.47807511686810655\n","mcrmse: 0.478840970014032\n","mcrmse: 0.4786878165880781\n","mcrmse: 0.47833953648302074\n","mcrmse: 0.47786672515863726\n","mcrmse: 0.47770001145947116\n","mcrmse: 0.4786507295022629\n","mcrmse: 0.4783322073005063\n","mcrmse: 0.4786090510372015\n","mcrmse: 0.47746691409374464\n","mcrmse: 0.47815118291712777\n","mcrmse: 0.4789247097601453\n","mcrmse: 0.47877677960869436\n","mcrmse: 0.4775975724735367\n","mcrmse: 0.47833533865997646\n","mcrmse: 0.477402718242522\n","mcrmse: 0.47759883774336653\n","mcrmse: 0.4777773547827273\n","mcrmse: 0.47784307629519657\n","mcrmse: 0.47756379093980295\n","mcrmse: 0.4785097104583055\n","mcrmse: 0.4783763195258603\n","mcrmse: 0.47859989082398413\n","mcrmse: 0.4782480644783378\n","mcrmse: 0.4773961380128761\n","mcrmse: 0.47765519938300216\n","mcrmse: 0.47806732172087707\n","mcrmse: 0.4790195335687988\n","mcrmse: 0.4780302083450799\n","mcrmse: 0.47705475572752876\n","mcrmse: 0.4772301784645808\n","mcrmse: 0.4777723387184416\n","mcrmse: 0.47831906487910547\n","mcrmse: 0.47809592009039026\n","mcrmse: 0.4780303968933395\n","mcrmse: 0.47773281653407607\n","mcrmse: 0.4772213580262763\n","mcrmse: 0.47786748693540987\n","mcrmse: 0.47797472486723885\n","mcrmse: 0.4778301155588368\n","mcrmse: 0.47820268412082634\n","mcrmse: 0.4787829437230525\n","mcrmse: 0.47748040564322347\n","mcrmse: 0.4781137697794954\n","mcrmse: 0.4775411273141983\n","mcrmse: 0.47726935499804934\n","mcrmse: 0.4771938940712869\n","mcrmse: 0.4764391446022248\n","mcrmse: 0.4793086005557966\n","mcrmse: 0.4777855204760504\n","mcrmse: 0.4780060834773573\n","mcrmse: 0.47934801571598307\n","mcrmse: 0.477651846136968\n","mcrmse: 0.47794202846190853\n","mcrmse: 0.478895319035199\n","mcrmse: 0.47771301511646297\n","mcrmse: 0.481189128277135\n","mcrmse: 0.4780900599763809\n","mcrmse: 0.4788278539907741\n","mcrmse: 0.4789280859507324\n","mcrmse: 0.478871986800283\n","mcrmse: 0.4792418517130137\n","mcrmse: 0.47869626648551367\n","mcrmse: 0.4780660864530688\n","mcrmse: 0.4779000246437745\n","mcrmse: 0.4803382426186469\n","mcrmse: 0.47934804007339943\n","mcrmse: 0.47963904410865116\n","mcrmse: 0.48078472174166353\n","mcrmse: 0.47760912002405803\n","mcrmse: 0.47882695557922283\n","mcrmse: 0.48042088769149116\n","mcrmse: 0.4785016317560905\n","mcrmse: 0.4789318013432846\n","mcrmse: 0.48477973112546313\n","mcrmse: 0.4788979019614177\n","mcrmse: 0.4824259241598831\n","mcrmse: 0.4781078859514558\n","mcrmse: 0.4795376030557986\n","mcrmse: 0.4876738143078069\n","mcrmse: 0.4803266755777309\n","mcrmse: 0.48106422339266564\n","mcrmse: 0.4773380152300325\n","mcrmse: 0.4785828544635715\n","mcrmse: 0.47959388605404896\n"]}],"source":["study = optuna.create_study(direction='minimize')\n","study.optimize(objective, n_trials=500)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Best trial:\n","Value:  0.4764391446022248\n"]},{"data":{"text/plain":["{'upload_model/debertav3base_lr17e-05': 0.050140453759609765,\n"," 'upload_model/debertav3base_lr18e-05': 0.07052951309192787,\n"," 'upload_model/debertav3base_lr21e-05': -0.2737032505249057,\n"," 'upload_model/debertav3base_lr22e-05': 0.7323608145720258,\n"," 'upload_model/debertav3large_lr12e-05': 0.17739633753942907,\n"," 'upload_model/debertav3large_lr13e-05': 0.3487763171688307,\n"," 'debertav3large_lr8e-06_att_0007': 0.40208654403767585,\n"," 'debertav3large_lr9e-06_att_0007': 0.6324542274221961,\n"," 'debertav3large_lr11e-05_att_0007': 0.5861786287038867,\n"," 'debertav3large_lr12e-05_att_0007': -0.2614732225436932,\n"," 'debertav3large_lr13e-05_att_0007': 0.3223536795999506,\n"," 'debertav3large_lr14e-05_att_0007': -0.05707090804260281,\n"," 'debertav3large_lr15e-05_att_0007': 0.30642565495970764,\n"," 'debertav3large_lr16e-05_att_0007': -0.025179008462274423,\n"," 'debertav3large_lr17e-05_att_0007': 0.28841239875231883,\n"," 'debertav3large_lr18e-05_att_0007': 0.15050522437804395}"]},"metadata":{},"output_type":"display_data"}],"source":["\n","print('Best trial:')\n","trial_ = study.best_trial\n","\n","print('Value: ', trial_.value)\n","\n","for model in CFG.list_model_infer:\n","    weight_for_model[model] = trial_.params[model]\n","weight_for_model\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["{'upload_model/debertav3base_lr17e-05': 0.050140453759609765,\n"," 'upload_model/debertav3base_lr18e-05': 0.07052951309192787,\n"," 'upload_model/debertav3base_lr21e-05': -0.2737032505249057,\n"," 'upload_model/debertav3base_lr22e-05': 0.7323608145720258,\n"," 'upload_model/debertav3large_lr12e-05': 0.17739633753942907,\n"," 'upload_model/debertav3large_lr13e-05': 0.3487763171688307,\n"," 'debertav3large_lr8e-06_att_0007': 0.40208654403767585,\n"," 'debertav3large_lr9e-06_att_0007': 0.6324542274221961,\n"," 'debertav3large_lr11e-05_att_0007': 0.5861786287038867,\n"," 'debertav3large_lr12e-05_att_0007': -0.2614732225436932,\n"," 'debertav3large_lr13e-05_att_0007': 0.3223536795999506,\n"," 'debertav3large_lr14e-05_att_0007': -0.05707090804260281,\n"," 'debertav3large_lr15e-05_att_0007': 0.30642565495970764,\n"," 'debertav3large_lr16e-05_att_0007': -0.025179008462274423,\n"," 'debertav3large_lr17e-05_att_0007': 0.28841239875231883,\n"," 'debertav3large_lr18e-05_att_0007': 0.15050522437804395}"]},"metadata":{},"output_type":"display_data"}],"source":["for model in CFG.list_model_infer:\n","    weight_for_model[model] = trial_.params[model]\n","weight_for_model"]},{"cell_type":"markdown","metadata":{},"source":["## Predict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Best trial:\n","# Value:  0.4764391446022248\n","# Params: \n","#     upload_model/debertav3base_lr17e-05: 0.050140453759609765\n","#     upload_model/debertav3base_lr18e-05: 0.07052951309192787\n","#     upload_model/debertav3base_lr21e-05: -0.2737032505249057\n","#     upload_model/debertav3base_lr22e-05: 0.7323608145720258\n","#     upload_model/debertav3large_lr12e-05: 0.17739633753942907\n","#     upload_model/debertav3large_lr13e-05: 0.3487763171688307\n","#     debertav3large_lr8e-06_att_0007: 0.40208654403767585\n","#     debertav3large_lr9e-06_att_0007: 0.6324542274221961\n","#     debertav3large_lr11e-05_att_0007: 0.5861786287038867\n","#     debertav3large_lr12e-05_att_0007: -0.2614732225436932\n","#     debertav3large_lr13e-05_att_0007: 0.3223536795999506\n","#     debertav3large_lr14e-05_att_0007: -0.05707090804260281\n","#     debertav3large_lr15e-05_att_0007: 0.30642565495970764\n","#     debertav3large_lr16e-05_att_0007: -0.025179008462274423\n","#     debertav3large_lr17e-05_att_0007: 0.28841239875231883\n","#     debertav3large_lr18e-05_att_0007: 0.15050522437804395"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["drop_columns_2 = [\n","                # \"fold\", \n","                \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n","                \"prompt_question\", \"prompt_title\", \n","                \"prompt_text\",\n","                \"input\"\n","               ] + [\n","                f\"content_pred_{i}\" for i in range(CFG.n_splits)\n","                ] + [\n","                f\"wording_pred_{i}\" for i in range(CFG.n_splits)\n","                ]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred_dict = {}\n","for target in targets:\n","    models = model_dict[target]\n","    preds = []\n","\n","    for fold, model in enumerate(models):\n","        X_eval_cv = test.drop(columns=drop_columns_2)\n","        # print(X_eval_cv.head())\n","        pred = model.predict(X_eval_cv)\n","        # print('pred shape'  , pred.shape)\n","        preds.append(pred)\n","    \n","    pred_dict[target] = preds"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for target in targets:\n","    preds = pred_dict[target]\n","    for i, pred in enumerate(preds):\n","        test[f\"{target}_pred_{i}\"] = pred\n","\n","    test[target] = test[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>prompt_id</th>\n","      <th>text</th>\n","      <th>summary_length</th>\n","      <th>fixed_summary_text</th>\n","      <th>splling_err_num</th>\n","      <th>prompt_question</th>\n","      <th>prompt_title</th>\n","      <th>prompt_text</th>\n","      <th>prompt_length</th>\n","      <th>...</th>\n","      <th>wording_pred_1</th>\n","      <th>content_pred_1</th>\n","      <th>wording_pred_2</th>\n","      <th>content_pred_2</th>\n","      <th>wording_pred_3</th>\n","      <th>content_pred_3</th>\n","      <th>wording_pred</th>\n","      <th>content_pred</th>\n","      <th>content</th>\n","      <th>wording</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000000ffffff</td>\n","      <td>abc123</td>\n","      <td>Example text 1</td>\n","      <td>3</td>\n","      <td>Example text 1</td>\n","      <td>0</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 1</td>\n","      <td>Heading\\nText...</td>\n","      <td>3</td>\n","      <td>...</td>\n","      <td>-1.251299</td>\n","      <td>-1.578506</td>\n","      <td>-1.230973</td>\n","      <td>-1.311372</td>\n","      <td>-1.472722</td>\n","      <td>-1.607912</td>\n","      <td>-1.539744</td>\n","      <td>-1.335907</td>\n","      <td>-1.517739</td>\n","      <td>-1.354057</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>111111eeeeee</td>\n","      <td>def789</td>\n","      <td>Example text 2</td>\n","      <td>3</td>\n","      <td>Example text 2</td>\n","      <td>0</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 2</td>\n","      <td>Heading\\nText...</td>\n","      <td>3</td>\n","      <td>...</td>\n","      <td>-1.251299</td>\n","      <td>-1.578506</td>\n","      <td>-1.230973</td>\n","      <td>-1.311372</td>\n","      <td>-1.472722</td>\n","      <td>-1.607912</td>\n","      <td>-1.542108</td>\n","      <td>-1.336610</td>\n","      <td>-1.517739</td>\n","      <td>-1.354057</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>222222cccccc</td>\n","      <td>abc123</td>\n","      <td>Example text 3</td>\n","      <td>3</td>\n","      <td>Example text 3</td>\n","      <td>0</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 1</td>\n","      <td>Heading\\nText...</td>\n","      <td>3</td>\n","      <td>...</td>\n","      <td>-1.251299</td>\n","      <td>-1.578506</td>\n","      <td>-1.230973</td>\n","      <td>-1.311372</td>\n","      <td>-1.472722</td>\n","      <td>-1.607912</td>\n","      <td>-1.541584</td>\n","      <td>-1.344446</td>\n","      <td>-1.517739</td>\n","      <td>-1.354057</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>333333dddddd</td>\n","      <td>def789</td>\n","      <td>Example text 4</td>\n","      <td>3</td>\n","      <td>Example text 4</td>\n","      <td>0</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 2</td>\n","      <td>Heading\\nText...</td>\n","      <td>3</td>\n","      <td>...</td>\n","      <td>-1.251299</td>\n","      <td>-1.578506</td>\n","      <td>-1.230973</td>\n","      <td>-1.311372</td>\n","      <td>-1.472722</td>\n","      <td>-1.607912</td>\n","      <td>-1.544137</td>\n","      <td>-1.341062</td>\n","      <td>-1.517739</td>\n","      <td>-1.354057</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4 rows × 63 columns</p>\n","</div>"],"text/plain":["     student_id prompt_id            text  summary_length fixed_summary_text  \\\n","0  000000ffffff    abc123  Example text 1               3     Example text 1   \n","1  111111eeeeee    def789  Example text 2               3     Example text 2   \n","2  222222cccccc    abc123  Example text 3               3     Example text 3   \n","3  333333dddddd    def789  Example text 4               3     Example text 4   \n","\n","   splling_err_num prompt_question     prompt_title       prompt_text  \\\n","0                0    Summarize...  Example Title 1  Heading\\nText...   \n","1                0    Summarize...  Example Title 2  Heading\\nText...   \n","2                0    Summarize...  Example Title 1  Heading\\nText...   \n","3                0    Summarize...  Example Title 2  Heading\\nText...   \n","\n","   prompt_length  ...  wording_pred_1  content_pred_1  wording_pred_2  \\\n","0              3  ...       -1.251299       -1.578506       -1.230973   \n","1              3  ...       -1.251299       -1.578506       -1.230973   \n","2              3  ...       -1.251299       -1.578506       -1.230973   \n","3              3  ...       -1.251299       -1.578506       -1.230973   \n","\n","   content_pred_2  wording_pred_3  content_pred_3  wording_pred  content_pred  \\\n","0       -1.311372       -1.472722       -1.607912     -1.539744     -1.335907   \n","1       -1.311372       -1.472722       -1.607912     -1.542108     -1.336610   \n","2       -1.311372       -1.472722       -1.607912     -1.541584     -1.344446   \n","3       -1.311372       -1.472722       -1.607912     -1.544137     -1.341062   \n","\n","    content   wording  \n","0 -1.517739 -1.354057  \n","1 -1.517739 -1.354057  \n","2 -1.517739 -1.354057  \n","3 -1.517739 -1.354057  \n","\n","[4 rows x 63 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["test"]},{"cell_type":"markdown","metadata":{},"source":["## Create Submission file"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>content</th>\n","      <th>wording</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000000ffffff</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>111111eeeeee</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>222222cccccc</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>333333dddddd</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     student_id  content  wording\n","0  000000ffffff      0.0      0.0\n","1  111111eeeeee      0.0      0.0\n","2  222222cccccc      0.0      0.0\n","3  333333dddddd      0.0      0.0"]},"metadata":{},"output_type":"display_data"}],"source":["sample_submission"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test[[\"student_id\", \"content\", \"wording\"]].to_csv(\"submission.csv\", index=False)"]},{"cell_type":"markdown","metadata":{},"source":["## Summary\n","\n","CV result is like this.\n","\n","| | content rmse |wording rmse | mcrmse | LB| |\n","| -- | -- | -- | -- | -- | -- |\n","|baseline| 0.494 | 0.630 | 0.562 | 0.509 | [link](https://www.kaggle.com/code/tsunotsuno/debertav3-baseline-content-and-wording-models)|\n","| use title and question field | 0.476| 0.619 | 0.548 | 0.508 | [link](https://www.kaggle.com/code/tsunotsuno/debertav3-w-prompt-title-question-fields) |\n","| Debertav3 + LGBM | 0.451 | 0.591 | 0.521 | 0.461 | [link](https://www.kaggle.com/code/tsunotsuno/debertav3-lgbm-with-feature-engineering) |\n","| Debertav3 + LGBM with spell autocorrect | 0.448 | 0.581 | 0.514 | 0.459 |nogawanogawa's original code\n","| Debertav3 + LGBM with spell autocorrect and tuning | 0.442 | 0.566 | 0.504 | 0.453 | this notebook |\n","\n","The CV values improved slightly, and the LB value is improved."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
