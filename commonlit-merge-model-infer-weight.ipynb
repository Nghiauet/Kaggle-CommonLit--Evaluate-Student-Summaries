{"cells":[{"cell_type":"code","execution_count":328,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-10-04T06:40:30.055491Z","iopub.status.busy":"2023-10-04T06:40:30.055032Z","iopub.status.idle":"2023-10-04T06:40:48.625658Z","shell.execute_reply":"2023-10-04T06:40:48.624671Z","shell.execute_reply.started":"2023-10-04T06:40:30.055452Z"},"trusted":true},"outputs":[],"source":["# !pip install \".inputautocorrect/autocorrect-2.6.1.tar\"\n","# !pip install \".inputpyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\"\n"]},{"cell_type":"code","execution_count":329,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:40:48.628788Z","iopub.status.busy":"2023-10-04T06:40:48.628441Z","iopub.status.idle":"2023-10-04T06:40:48.633138Z","shell.execute_reply":"2023-10-04T06:40:48.632140Z","shell.execute_reply.started":"2023-10-04T06:40:48.628747Z"},"trusted":true},"outputs":[],"source":["# nltk.download(\"punkt\")"]},{"cell_type":"code","execution_count":330,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:40:48.635088Z","iopub.status.busy":"2023-10-04T06:40:48.634616Z","iopub.status.idle":"2023-10-04T06:40:48.653149Z","shell.execute_reply":"2023-10-04T06:40:48.652357Z","shell.execute_reply.started":"2023-10-04T06:40:48.635053Z"},"trusted":true},"outputs":[],"source":["from typing import List\n","import numpy as np\n","import pandas as pd\n","import warnings\n","import logging\n","import os\n","import shutil\n","import json\n","import transformers\n","from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n","from transformers import DataCollatorWithPadding\n","from datasets import Dataset,load_dataset, load_from_disk\n","from transformers import TrainingArguments, Trainer\n","from datasets import load_metric, disable_progress_bar\n","from sklearn.metrics import mean_squared_error\n","import torch\n","from sklearn.model_selection import KFold, GroupKFold\n","from tqdm import tqdm\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize.treebank import TreebankWordDetokenizer\n","from collections import Counter\n","import spacy\n","import re\n","from autocorrect import Speller\n","from spellchecker import SpellChecker\n","import lightgbm as lgb\n","warnings.simplefilter(\"ignore\")\n","logging.disable(logging.ERROR)\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n","disable_progress_bar()\n","tqdm.pandas()"]},{"cell_type":"code","execution_count":331,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:40:48.654853Z","iopub.status.busy":"2023-10-04T06:40:48.654636Z","iopub.status.idle":"2023-10-04T06:40:48.664885Z","shell.execute_reply":"2023-10-04T06:40:48.663979Z","shell.execute_reply.started":"2023-10-04T06:40:48.654792Z"},"trusted":true},"outputs":[],"source":["def seed_everything(seed: int):\n","    import random, os\n","    import numpy as np\n","    import torch\n","    \n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True # load seed\n","    \n","seed_everything(seed=42)"]},{"cell_type":"markdown","metadata":{},"source":["## Class CFG"]},{"cell_type":"code","execution_count":332,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:42:32.950196Z","iopub.status.busy":"2023-10-04T06:42:32.949946Z","iopub.status.idle":"2023-10-04T06:42:32.956667Z","shell.execute_reply":"2023-10-04T06:42:32.954789Z","shell.execute_reply.started":"2023-10-04T06:42:32.950170Z"},"trusted":true},"outputs":[],"source":["class CFG:\n","    model_name=\"debertav3base\"\n","    learning_rate=0.000016\n","    weight_decay=0.03\n","    hidden_dropout_prob=0.007\n","    attention_probs_dropout_prob=0.007\n","    num_train_epochs=5\n","    n_splits=4\n","    batch_size= 128\n","    random_seed=42\n","    save_steps=100\n","    max_length= 512\n","    number_base_model = 2\n","    test_mode = False\n","    device = 'CPU'\n","    infer_mode = True\n","    list_model_infer = [\n","        'upload_model/debertav3base_lr5e-05',\n","        'upload_model/debertav3base_lr15e-05',\n","        'upload_model/debertav3base_lr17e-05',\n","        'upload_model/debertav3base_lr18e-05',\n","        'upload_model/debertav3base_lr21e-05',\n","        'upload_model/debertav3base_lr22e-05',\n","        'upload_model/debertav3large_lr13e-05',\n","        'upload_model/debertav3large_lr12e-05',\n","        'debertav3large_lr1e-05_att_0007',\n","        'debertav3large_lr1e-05_save',\n","        'debertav3large_lr8e-06_att_0007',\n","        'debertav3large_lr9e-06_att_0007',\n","        'debertav3large_lr11e-05_att_0007',\n","        'debertav3large_lr12e-05_att_0007',\n","        'debertav3large_lr13e-05_att_0007',\n","        'debertav3large_lr14e-05_att_0007',\n","        'debertav3large_lr15e-05_att_0007',\n","        'debertav3large_lr16e-05_att_0007',\n","        'debertav3large_lr17e-05_att_0007',\n","        'debertav3large_lr18e-05_att_0007',\n","        ]\n","    "]},{"cell_type":"code","execution_count":333,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:42:33.445629Z","iopub.status.busy":"2023-10-04T06:42:33.445350Z","iopub.status.idle":"2023-10-04T06:42:33.450949Z","shell.execute_reply":"2023-10-04T06:42:33.450072Z","shell.execute_reply.started":"2023-10-04T06:42:33.445577Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["# print device\n","if CFG.device != 'CPU':\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # print device \n","else :\n","    device = torch.device(\"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{},"source":["## Dataload"]},{"cell_type":"code","execution_count":334,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:42:33.519669Z","iopub.status.busy":"2023-10-04T06:42:33.519426Z","iopub.status.idle":"2023-10-04T06:42:33.580911Z","shell.execute_reply":"2023-10-04T06:42:33.580042Z","shell.execute_reply.started":"2023-10-04T06:42:33.519644Z"},"trusted":true},"outputs":[],"source":["DATA_DIR = \"input/commonlit-evaluate-student-summaries/\"\n","\n","prompts_train = pd.read_csv(DATA_DIR + \"prompts_train.csv\")\n","prompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")\n","summaries_train = pd.read_csv(DATA_DIR + \"summaries_train.csv\")\n","summaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")\n","sample_submission = pd.read_csv(DATA_DIR + \"sample_submission.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["## Exploratory Data Analysis"]},{"cell_type":"code","execution_count":335,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:42:33.610090Z","iopub.status.busy":"2023-10-04T06:42:33.609903Z","iopub.status.idle":"2023-10-04T06:42:33.618920Z","shell.execute_reply":"2023-10-04T06:42:33.617939Z","shell.execute_reply.started":"2023-10-04T06:42:33.610070Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>prompt_id</th>\n","      <th>prompt_question</th>\n","      <th>prompt_title</th>\n","      <th>prompt_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>39c16e</td>\n","      <td>Summarize at least 3 elements of an ideal trag...</td>\n","      <td>On Tragedy</td>\n","      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3b9047</td>\n","      <td>In complete sentences, summarize the structure...</td>\n","      <td>Egyptian Social Structure</td>\n","      <td>Egyptian society was structured like a pyramid...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>814d6b</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ebad26</td>\n","      <td>Summarize the various ways the factory would u...</td>\n","      <td>Excerpt from The Jungle</td>\n","      <td>With one member trimming beef in a cannery, an...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  prompt_id                                    prompt_question  \\\n","0    39c16e  Summarize at least 3 elements of an ideal trag...   \n","1    3b9047  In complete sentences, summarize the structure...   \n","2    814d6b  Summarize how the Third Wave developed over su...   \n","3    ebad26  Summarize the various ways the factory would u...   \n","\n","                prompt_title  \\\n","0                 On Tragedy   \n","1  Egyptian Social Structure   \n","2             The Third Wave   \n","3    Excerpt from The Jungle   \n","\n","                                         prompt_text  \n","0  Chapter 13 \\r\\nAs the sequel to what has alrea...  \n","1  Egyptian society was structured like a pyramid...  \n","2  Background \\r\\nThe Third Wave experiment took ...  \n","3  With one member trimming beef in a cannery, an...  "]},"execution_count":335,"metadata":{},"output_type":"execute_result"}],"source":["prompts_train.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocess\n","\n","[Using features]\n","\n","- Text Length\n","- Length Ratio\n","- Word Overlap\n","- N-grams Co-occurrence\n","  - count\n","  - ratio\n","- Quotes Overlap\n","- Grammar Check\n","  - spelling: pyspellchecker\n"]},{"cell_type":"code","execution_count":336,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:42:33.675573Z","iopub.status.busy":"2023-10-04T06:42:33.675062Z","iopub.status.idle":"2023-10-04T06:42:35.228470Z","shell.execute_reply":"2023-10-04T06:42:35.227551Z","shell.execute_reply.started":"2023-10-04T06:42:33.675532Z"},"trusted":true},"outputs":[],"source":["class Preprocessor:\n","    def __init__(self, \n","                model_name: str,\n","                ) -> None:\n","        self.tokenizer = AutoTokenizer.from_pretrained(f\"input/{model_name}\")\n","        self.twd = TreebankWordDetokenizer()\n","        self.STOP_WORDS = set(stopwords.words('english'))\n","        \n","        self.spacy_ner_model = spacy.load('en_core_web_sm',)\n","        self.speller = Speller(lang='en')\n","        self.spellchecker = SpellChecker() \n","        \n","    def word_overlap_count(self, row):\n","        \"\"\" intersection(prompt_text, text) \"\"\"        \n","        def check_is_stop_word(word):\n","            return word in self.STOP_WORDS\n","        \n","        prompt_words = row['prompt_tokens']\n","        summary_words = row['summary_tokens']\n","        if self.STOP_WORDS:\n","            prompt_words = list(filter(check_is_stop_word, prompt_words))\n","            summary_words = list(filter(check_is_stop_word, summary_words))\n","        return len(set(prompt_words).intersection(set(summary_words)))\n","            \n","    def ngrams(self, token, n):\n","        # Use the zip function to help us generate n-grams\n","        # Concatentate the tokens into ngrams and return\n","        ngrams = zip(*[token[i:] for i in range(n)])\n","        return [\" \".join(ngram) for ngram in ngrams]\n","\n","    def ngram_co_occurrence(self, row, n: int) -> int:\n","        # Tokenize the original text and summary into words\n","        original_tokens = row['prompt_tokens']\n","        summary_tokens = row['summary_tokens']\n","\n","        # Generate n-grams for the original text and summary\n","        original_ngrams = set(self.ngrams(original_tokens, n))\n","        summary_ngrams = set(self.ngrams(summary_tokens, n))\n","\n","        # Calculate the number of common n-grams\n","        common_ngrams = original_ngrams.intersection(summary_ngrams)\n","        return len(common_ngrams)\n","    \n","    def ner_overlap_count(self, row, mode:str):\n","        model = self.spacy_ner_model\n","        def clean_ners(ner_list):\n","            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n","        prompt = model(row['prompt_text'])\n","        summary = model(row['text'])\n","\n","        if \"spacy\" in str(model):\n","            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n","            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n","        elif \"stanza\" in str(model):\n","            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n","            summary_ner = set([(token.text, token.type) for token in summary.ents])\n","        else:\n","            raise Exception(\"Model not supported\")\n","\n","        prompt_ner = clean_ners(prompt_ner)\n","        summary_ner = clean_ners(summary_ner)\n","\n","        intersecting_ners = prompt_ner.intersection(summary_ner)\n","        \n","        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n","        \n","        if mode == \"train\":\n","            return ner_dict\n","        elif mode == \"test\":\n","            return {key: ner_dict.get(key) for key in self.ner_keys}\n","\n","    \n","    def quotes_count(self, row):\n","        summary = row['text']\n","        text = row['prompt_text']\n","        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n","        if len(quotes_from_summary)>0:\n","            return [quote in text for quote in quotes_from_summary].count(True)\n","        else:\n","            return 0\n","\n","    def spelling(self, text):\n","        \n","        wordlist=text.split()\n","        amount_miss = len(list(self.spellchecker.unknown(wordlist)))\n","\n","        return amount_miss\n","    \n","    def add_spelling_dictionary(self, tokens: List[str]) -> List[str]:\n","        \"\"\"dictionary update for pyspell checker and autocorrect\"\"\"\n","        self.spellchecker.word_frequency.load_words(tokens)\n","        self.speller.nlp_data.update({token:1000 for token in tokens})\n","    \n","    def run(self, \n","            prompts: pd.DataFrame,\n","            summaries:pd.DataFrame,\n","            mode:str\n","        ) -> pd.DataFrame:\n","        \n","        # before merge preprocess\n","        prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n","            lambda x: len(word_tokenize(x))\n","        )\n","        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n","            lambda x: word_tokenize(x)\n","        )\n","\n","        summaries[\"summary_length\"] = summaries[\"text\"].apply(\n","            lambda x: len(word_tokenize(x))\n","        )\n","        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n","            lambda x: word_tokenize(x)\n","        )\n","        \n","        # Add prompt tokens into spelling checker dictionary\n","        prompts[\"prompt_tokens\"].apply(\n","            lambda x: self.add_spelling_dictionary(x)\n","        )\n","        \n","        #         from IPython.core.debugger import Pdb; Pdb().set_trace()\n","        # fix misspelling\n","        summaries[\"fixed_summary_text\"] = summaries[\"text\"].progress_apply(\n","            lambda x: self.speller(x)\n","        ) # fix mission spelling\n","        \n","        # count misspelling\n","        summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling) # count number of misspelling \n","        \n","        # merge prompts and summaries\n","        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\") \n","\n","        # after merge preprocess\n","        # input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n","        \n","        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n","        input_df['bigram_overlap_count'] = input_df.progress_apply(\n","            self.ngram_co_occurrence,args=(2,), axis=1 \n","        )\n","        input_df['bigram_overlap_ratio'] = input_df['bigram_overlap_count'] / (input_df['summary_length'] - 1)\n","        \n","        input_df['trigram_overlap_count'] = input_df.progress_apply(\n","            self.ngram_co_occurrence, args=(3,), axis=1\n","        )\n","        input_df['trigram_overlap_ratio'] = input_df['trigram_overlap_count'] / (input_df['summary_length'] - 2)\n","        \n","        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n","        \n","        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n","    \n"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocess 2\n"]},{"cell_type":"code","execution_count":337,"metadata":{},"outputs":[],"source":["from textblob import TextBlob\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from nltk import ne_chunk, word_tokenize, pos_tag\n","# nltk.downloader.download('vader_lexicon')\n","import pyphen\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","\n","dic = pyphen.Pyphen(lang='en')\n","sid = SentimentIntensityAnalyzer()\n","\n","class Preprocessor2:\n","    def __init__(self, \n","                model_name: str,\n","                ) -> None:\n","        self.tokenizer = AutoTokenizer.from_pretrained(f\"input/{model_name}\")\n","        self.twd = TreebankWordDetokenizer()\n","        self.STOP_WORDS = set(stopwords.words('english'))\n","        \n","        self.spacy_ner_model = spacy.load('en_core_web_sm',)\n","        self.speller = Speller(lang='en')\n","        self.spellchecker = SpellChecker() \n","        \n","    def calculate_text_similarity(self, row):\n","        vectorizer = TfidfVectorizer()\n","        tfidf_matrix = vectorizer.fit_transform([row['prompt_text'], row['text']])\n","        return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2]).flatten()[0]\n","    \n","    def sentiment_analysis(self, text):\n","        analysis = TextBlob(text)\n","        return analysis.sentiment.polarity, analysis.sentiment.subjectivity\n","    \n","    def word_overlap_count(self, row):\n","        \"\"\" intersection(prompt_text, text) \"\"\"        \n","        def check_is_stop_word(word):\n","            return word in self.STOP_WORDS\n","        \n","        prompt_words = row['prompt_tokens']\n","        summary_words = row['summary_tokens']\n","        if self.STOP_WORDS:\n","            prompt_words = list(filter(check_is_stop_word, prompt_words))\n","            summary_words = list(filter(check_is_stop_word, summary_words))\n","        return len(set(prompt_words).intersection(set(summary_words)))\n","            \n","    def ngrams(self, token, n):\n","        # Use the zip function to help us generate n-grams\n","        # Concatentate the tokens into ngrams and return\n","        ngrams = zip(*[token[i:] for i in range(n)])\n","        return [\" \".join(ngram) for ngram in ngrams]\n","\n","    def ngram_co_occurrence(self, row, n: int) -> int:\n","        # Tokenize the original text and summary into words\n","        original_tokens = row['prompt_tokens']\n","        summary_tokens = row['summary_tokens']\n","\n","        # Generate n-grams for the original text and summary\n","        original_ngrams = set(self.ngrams(original_tokens, n))\n","        summary_ngrams = set(self.ngrams(summary_tokens, n))\n","\n","        # Calculate the number of common n-grams\n","        common_ngrams = original_ngrams.intersection(summary_ngrams)\n","        return len(common_ngrams)\n","    \n","    def ner_overlap_count(self, row, mode:str):\n","        model = self.spacy_ner_model\n","        def clean_ners(ner_list):\n","            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n","        prompt = model(row['prompt_text'])\n","        summary = model(row['text'])\n","\n","        if \"spacy\" in str(model):\n","            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n","            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n","        elif \"stanza\" in str(model):\n","            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n","            summary_ner = set([(token.text, token.type) for token in summary.ents])\n","        else:\n","            raise Exception(\"Model not supported\")\n","\n","        prompt_ner = clean_ners(prompt_ner)\n","        summary_ner = clean_ners(summary_ner)\n","\n","        intersecting_ners = prompt_ner.intersection(summary_ner)\n","        \n","        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n","        \n","        if mode == \"train\":\n","            return ner_dict\n","        elif mode == \"test\":\n","            return {key: ner_dict.get(key) for key in self.ner_keys}\n","\n","    \n","    def quotes_count(self, row):\n","        summary = row['text']\n","        text = row['prompt_text']\n","        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n","        if len(quotes_from_summary)>0:\n","            return [quote in text for quote in quotes_from_summary].count(True)\n","        else:\n","            return 0\n","\n","    def spelling(self, text):\n","        \n","        wordlist=text.split()\n","        amount_miss = len(list(self.spellchecker.unknown(wordlist)))\n","\n","        return amount_miss\n","    \n","    def calculate_unique_words(self,text):\n","        unique_words = set(text.split())\n","        return len(unique_words)\n","    \n","    def add_spelling_dictionary(self, tokens: List[str]) -> List[str]:\n","        \"\"\"dictionary update for pyspell checker and autocorrect\"\"\"\n","        self.spellchecker.word_frequency.load_words(tokens)\n","        self.speller.nlp_data.update({token:1000 for token in tokens})\n","        \n","    def calculate_pos_ratios(self , text):\n","        pos_tags = pos_tag(nltk.word_tokenize(text))\n","        pos_counts = Counter(tag for word, tag in pos_tags)\n","        total_words = len(pos_tags)\n","        ratios = {tag: count / total_words for tag, count in pos_counts.items()}\n","        return ratios\n","    \n","    def calculate_punctuation_ratios(self,text):\n","        total_chars = len(text)\n","        punctuation_counts = Counter(char for char in text if char in '.,!?;:\"()[]{}')\n","        ratios = {char: count / total_chars for char, count in punctuation_counts.items()}\n","        return ratios\n","    \n","    def calculate_keyword_density(self,row):\n","        keywords = set(row['prompt_text'].split())\n","        text_words = row['text'].split()\n","        keyword_count = sum(1 for word in text_words if word in keywords)\n","        return keyword_count / len(text_words)\n","    \n","    def count_syllables(self,word):\n","        hyphenated_word = dic.inserted(word)\n","        return len(hyphenated_word.split('-'))\n","\n","    def flesch_reading_ease_manual(self,text):\n","        total_sentences = len(TextBlob(text).sentences)\n","        total_words = len(TextBlob(text).words)\n","        total_syllables = sum(self.count_syllables(word) for word in TextBlob(text).words)\n","\n","        if total_sentences == 0 or total_words == 0:\n","            return 0\n","\n","        flesch_score = 206.835 - 1.015 * (total_words / total_sentences) - 84.6 * (total_syllables / total_words)\n","        return flesch_score\n","    \n","    def flesch_kincaid_grade_level(self, text):\n","        total_sentences = len(TextBlob(text).sentences)\n","        total_words = len(TextBlob(text).words)\n","        total_syllables = sum(self.count_syllables(word) for word in TextBlob(text).words)\n","\n","        if total_sentences == 0 or total_words == 0:\n","            return 0\n","\n","        fk_grade = 0.39 * (total_words / total_sentences) + 11.8 * (total_syllables / total_words) - 15.59\n","        return fk_grade\n","    \n","    def gunning_fog(self, text):\n","        total_sentences = len(TextBlob(text).sentences)\n","        total_words = len(TextBlob(text).words)\n","        complex_words = sum(1 for word in TextBlob(text).words if self.count_syllables(word) > 2)\n","\n","        if total_sentences == 0 or total_words == 0:\n","            return 0\n","\n","        fog_index = 0.4 * ((total_words / total_sentences) + 100 * (complex_words / total_words))\n","        return fog_index\n","    \n","    def calculate_sentiment_scores(self,text):\n","        sentiment_scores = sid.polarity_scores(text)\n","        return sentiment_scores\n","    \n","    def count_difficult_words(self, text, syllable_threshold=3):\n","        words = TextBlob(text).words\n","        difficult_words_count = sum(1 for word in words if self.count_syllables(word) >= syllable_threshold)\n","        return difficult_words_count\n","\n","\n","    \n","    def run(self, \n","            prompts: pd.DataFrame,\n","            summaries:pd.DataFrame,\n","            mode:str\n","        ) -> pd.DataFrame:\n","        \n","        # before merge preprocess\n","        prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n","            lambda x: len(word_tokenize(x))\n","        )\n","        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n","            lambda x: word_tokenize(x)\n","        )\n","\n","        summaries[\"summary_length\"] = summaries[\"text\"].apply(\n","            lambda x: len(word_tokenize(x))\n","        )\n","        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n","            lambda x: word_tokenize(x)\n","        )\n","        \n","        # Add prompt tokens into spelling checker dictionary\n","        prompts[\"prompt_tokens\"].apply(\n","            lambda x: self.add_spelling_dictionary(x)\n","        )\n","        \n","        prompts['gunning_fog_prompt'] = prompts['prompt_text'].apply(self.gunning_fog)\n","        prompts['flesch_kincaid_grade_level_prompt'] = prompts['prompt_text'].apply(self.flesch_kincaid_grade_level)\n","        prompts['flesch_reading_ease_prompt'] = prompts['prompt_text'].apply(self.flesch_reading_ease_manual)\n","\n","        \n","#         from IPython.core.debugger import Pdb; Pdb().set_trace()\n","        # fix misspelling\n","        summaries[\"fixed_summary_text\"] = summaries[\"text\"].progress_apply(\n","            lambda x: self.speller(x)\n","        )\n","        \n","        \n","        # count misspelling\n","        summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n","        \n","        # merge prompts and summaries\n","        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n","        input_df['flesch_reading_ease'] = input_df['text'].apply(self.flesch_reading_ease_manual)\n","        input_df['word_count'] = input_df['text'].apply(lambda x: len(x.split()))\n","        input_df['sentence_length'] = input_df['text'].apply(lambda x: len(x.split('.')))\n","        input_df['vocabulary_richness'] = input_df['text'].apply(lambda x: len(set(x.split())))\n","\n","        input_df['word_count2'] = [len(t.split(' ')) for t in input_df.text]\n","        input_df['num_unq_words']=[len(list(set(x.lower().split(' ')))) for x in input_df.text]\n","        input_df['num_chars']= [len(x) for x in input_df.text]\n","\n","        # Additional features\n","        input_df['avg_word_length'] = input_df['text'].apply(lambda x: np.mean([len(word) for word in x.split()]))\n","        input_df['comma_count'] = input_df['text'].apply(lambda x: x.count(','))\n","        input_df['semicolon_count'] = input_df['text'].apply(lambda x: x.count(';'))\n","\n","        # after merge preprocess\n","        input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n","        \n","        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n","        input_df['bigram_overlap_count'] = input_df.progress_apply(\n","            self.ngram_co_occurrence,args=(2,), axis=1 \n","        )\n","        input_df['bigram_overlap_ratio'] = input_df['bigram_overlap_count'] / (input_df['summary_length'] - 1)\n","        \n","        input_df['trigram_overlap_count'] = input_df.progress_apply(\n","            self.ngram_co_occurrence, args=(3,), axis=1\n","        )\n","        input_df['trigram_overlap_ratio'] = input_df['trigram_overlap_count'] / (input_df['summary_length'] - 2)\n","        \n","        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n","        \n","        input_df['exclamation_count'] = input_df['text'].apply(lambda x: x.count('!'))\n","        input_df['question_count'] = input_df['text'].apply(lambda x: x.count('?'))\n","        input_df['pos_ratios'] = input_df['text'].apply(self.calculate_pos_ratios)\n","\n","        # Convert the dictionary of POS ratios into a single value (mean)\n","        input_df['pos_mean'] = input_df['pos_ratios'].apply(lambda x: np.mean(list(x.values())))\n","        input_df['punctuation_ratios'] = input_df['text'].apply(self.calculate_punctuation_ratios)\n","\n","        # Convert the dictionary of punctuation ratios into a single value (sum)\n","        input_df['punctuation_sum'] = input_df['punctuation_ratios'].apply(lambda x: np.sum(list(x.values())))\n","        input_df['keyword_density'] = input_df.apply(self.calculate_keyword_density, axis=1)\n","        input_df['jaccard_similarity'] = input_df.apply(lambda row: len(set(word_tokenize(row['prompt_text'])) & set(word_tokenize(row['text']))) / len(set(word_tokenize(row['prompt_text'])) | set(word_tokenize(row['text']))), axis=1)\n","        tqdm.pandas(desc=\"Performing Sentiment Analysis\")\n","        input_df[['sentiment_polarity', 'sentiment_subjectivity']] = input_df['text'].progress_apply(\n","            lambda x: pd.Series(self.sentiment_analysis(x))\n","        )\n","        tqdm.pandas(desc=\"Calculating Text Similarity\")\n","        input_df['text_similarity'] = input_df.progress_apply(self.calculate_text_similarity, axis=1)\n","        #Calculate sentiment scores for each row\n","        input_df['sentiment_scores'] = input_df['text'].apply(self.calculate_sentiment_scores)\n","        \n","        input_df['gunning_fog'] = input_df['text'].apply(self.gunning_fog)\n","        input_df['flesch_kincaid_grade_level'] = input_df['text'].apply(self.flesch_kincaid_grade_level)\n","        input_df['count_difficult_words'] = input_df['text'].apply(self.count_difficult_words)\n","\n","        # Convert sentiment_scores into individual columns\n","        sentiment_columns = pd.DataFrame(list(input_df['sentiment_scores']))\n","        input_df = pd.concat([input_df, sentiment_columns], axis=1)\n","        input_df['sentiment_scores_prompt'] = input_df['prompt_text'].apply(self.calculate_sentiment_scores)\n","        # Convert sentiment_scores_prompt into individual columns\n","        sentiment_columns_prompt = pd.DataFrame(list(input_df['sentiment_scores_prompt']))\n","        sentiment_columns_prompt.columns = [col +'_prompt' for col in sentiment_columns_prompt.columns]\n","        input_df = pd.concat([input_df, sentiment_columns_prompt], axis=1)\n","        columns =  ['pos_ratios', 'sentiment_scores', 'punctuation_ratios', 'sentiment_scores_prompt']\n","        cols_to_drop = [col for col in columns if col in input_df.columns]\n","        if cols_to_drop:\n","            input_df = input_df.drop(columns=cols_to_drop)\n","        \n","        print(cols_to_drop)\n","        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n","    "]},{"cell_type":"code","execution_count":338,"metadata":{},"outputs":[],"source":["preprocessor = Preprocessor(model_name=CFG.model_name)"]},{"cell_type":"markdown","metadata":{},"source":["## Create the train and test sets\n"]},{"cell_type":"code","execution_count":339,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:42:35.231162Z","iopub.status.busy":"2023-10-04T06:42:35.230619Z","iopub.status.idle":"2023-10-04T06:42:35.236187Z","shell.execute_reply":"2023-10-04T06:42:35.235261Z","shell.execute_reply.started":"2023-10-04T06:42:35.231130Z"},"trusted":true},"outputs":[],"source":["if CFG.test_mode : \n","    prompts_train = prompts_train[:12]\n","    prompts_test = prompts_test[:12]\n","    summaries_train = summaries_train[:12]\n","    summaries_test = summaries_test[:12]"]},{"cell_type":"code","execution_count":341,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:42:35.237834Z","iopub.status.busy":"2023-10-04T06:42:35.237577Z","iopub.status.idle":"2023-10-04T06:49:18.913211Z","shell.execute_reply":"2023-10-04T06:49:18.912332Z","shell.execute_reply.started":"2023-10-04T06:42:35.237803Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 7165/7165 [02:17<00:00, 52.06it/s]\n","100%|██████████| 7165/7165 [00:00<00:00, 20155.88it/s]\n","100%|██████████| 7165/7165 [00:00<00:00, 21078.41it/s]\n","100%|██████████| 7165/7165 [00:00<00:00, 12506.47it/s]\n","100%|██████████| 7165/7165 [00:00<00:00, 11009.14it/s]\n","100%|██████████| 7165/7165 [00:00<00:00, 207185.03it/s]\n","100%|██████████| 4/4 [00:00<00:00, 23530.46it/s]\n","100%|██████████| 4/4 [00:00<00:00, 29485.44it/s]\n","100%|██████████| 4/4 [00:00<00:00, 9782.63it/s]\n","100%|██████████| 4/4 [00:00<00:00, 11335.96it/s]\n","100%|██████████| 4/4 [00:00<00:00, 11244.78it/s]\n","100%|██████████| 4/4 [00:00<00:00, 5562.74it/s]\n"]}],"source":["train = preprocessor.run(prompts_train, summaries_train, mode=\"train\")\n","test = preprocessor.run(prompts_test, summaries_test, mode=\"test\")\n","# save train and test \n","# train.to_csv(\"input/train.csv\", index=False)\n","# test.to_csv(\"input/test.csv\", index=False)\n","# load train and test\n","# train = pd.read_csv(\"input/train.csv\")\n","# test = pd.read_csv(\"input/test.csv\")\n","# train.head()"]},{"cell_type":"code","execution_count":342,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:49:18.916276Z","iopub.status.busy":"2023-10-04T06:49:18.915510Z","iopub.status.idle":"2023-10-04T06:49:18.943400Z","shell.execute_reply":"2023-10-04T06:49:18.942559Z","shell.execute_reply.started":"2023-10-04T06:49:18.916242Z"},"trusted":true},"outputs":[],"source":["gkf = GroupKFold(n_splits=CFG.n_splits)\n","\n","for i, (_, val_index) in enumerate(gkf.split(train, groups=train[\"prompt_id\"])):\n","    train.loc[val_index, \"fold\"] = i\n","\n","# train.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Model Function Definition"]},{"cell_type":"code","execution_count":343,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:49:18.944706Z","iopub.status.busy":"2023-10-04T06:49:18.944482Z","iopub.status.idle":"2023-10-04T06:49:18.951510Z","shell.execute_reply":"2023-10-04T06:49:18.950756Z","shell.execute_reply.started":"2023-10-04T06:49:18.944677Z"},"trusted":true},"outputs":[],"source":["def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    rmse = mean_squared_error(labels, predictions, squared=False)\n","    return {\"rmse\": rmse}\n","\n","def compute_mcrmse(eval_pred):\n","    \"\"\"\n","    Calculates mean columnwise root mean squared error\n","    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n","    \"\"\"\n","    preds, labels = eval_pred\n","\n","    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n","    mcrmse = np.mean(col_rmse)\n","\n","    return {\n","        \"content_rmse\": col_rmse[0],\n","        \"wording_rmse\": col_rmse[1],\n","        \"mcrmse\": mcrmse,\n","    }\n","\n","def compt_score(content_true, content_pred, wording_true, wording_pred):\n","    content_score = mean_squared_error(content_true, content_pred)**(1/2)\n","    wording_score = mean_squared_error(wording_true, wording_pred)**(1/2)\n","    \n","    return (content_score + wording_score)/2"]},{"cell_type":"markdown","metadata":{},"source":["## Deberta Regressor"]},{"cell_type":"code","execution_count":344,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:49:18.953587Z","iopub.status.busy":"2023-10-04T06:49:18.953198Z","iopub.status.idle":"2023-10-04T06:49:18.971775Z","shell.execute_reply":"2023-10-04T06:49:18.970910Z","shell.execute_reply.started":"2023-10-04T06:49:18.953557Z"},"trusted":true},"outputs":[],"source":["class ScoreRegressor:\n","    def __init__(self, \n","                model_name: str,\n","                model_dir: str,\n","                target: list,\n","                hidden_dropout_prob: float,\n","                attention_probs_dropout_prob: float,\n","                max_length: int,\n","                ):\n","        self.inputs = [\"prompt_text\", \"prompt_title\", \"prompt_question\", \"fixed_summary_text\"] # fix summary text have prompt text in it \n","        self.input_col = \"input\"\n","        \n","        self.text_cols = [self.input_col] \n","        self.target = target\n","        self.target_cols = target\n","\n","        self.model_name = model_name\n","        lr = str(CFG.learning_rate).replace(\".\", \"\")\n","        self.model_dir = model_dir\n","        self.max_length = max_length\n","        \n","        self.tokenizer = AutoTokenizer.from_pretrained(f\"input/{model_name}\")\n","        self.model_config = AutoConfig.from_pretrained(f\"input/{model_name}\" )\n","        # print(self.model_config)\n","        self.model_config.update({\n","            \"hidden_dropout_prob\": hidden_dropout_prob,\n","            \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n","            \"num_labels\": 2,\n","            \"problem_type\": \"regression\",\n","        })\n","        seed_everything(seed=42)\n","\n","        self.data_collator = DataCollatorWithPadding(\n","            tokenizer=self.tokenizer\n","        )\n","\n","\n","    def tokenize_function(self, examples: pd.DataFrame):\n","        # labels = ['content' , 'wording']\n","        # print('labels', labels)\n","        tokenized = self.tokenizer(examples[self.input_col],\n","                         padding=False,\n","                         truncation=True,\n","                         max_length=self.max_length)\n","        return {\n","            **tokenized,\n","            \"labels\": [examples['content'], examples['wording']],\n","        }\n","    \n","    def tokenize_function_test(self, examples: pd.DataFrame):\n","        tokenized = self.tokenizer(examples[self.input_col],\n","                         padding=False,\n","                         truncation=True,\n","                         max_length=self.max_length)\n","        return tokenized\n","        \n","    def train(self, \n","            fold: int,\n","            train_df: pd.DataFrame,\n","            valid_df: pd.DataFrame,\n","            batch_size: int,\n","            learning_rate: float,\n","            weight_decay: float,\n","            num_train_epochs: float,\n","            save_steps: int,\n","        ) -> None:\n","        \"\"\"fine-tuning\"\"\"\n","        \n","        sep = self.tokenizer.sep_token\n","        # print('sep', sep)\n","        train_df[self.input_col] = (\n","                    train_df[\"prompt_title\"] + sep \n","                    + train_df[\"prompt_question\"] + sep \n","                    + train_df[\"fixed_summary_text\"]\n","                  )\n","\n","        valid_df[self.input_col] = (\n","                    valid_df[\"prompt_title\"] + sep \n","                    + valid_df[\"prompt_question\"] + sep \n","                    + valid_df[\"fixed_summary_text\"]\n","                  )\n","        # filter train_df with input_col have more than 5000 tokens\n","        # print('create train and val data frame ')\n","        # print('self.target_cols', self.target_cols)\n","        # print('self.input_col', self.input_col)\n","        train_df = train_df[[self.input_col] + self.target_cols]\n","        valid_df = valid_df[[self.input_col] + self.target_cols]\n","        \n","        model_content = AutoModelForSequenceClassification.from_pretrained(\n","            f\"input/{self.model_name}\", \n","            config=self.model_config\n","        )\n","\n","        train_dataset = Dataset.from_pandas(train_df, preserve_index=False) \n","        val_dataset = Dataset.from_pandas(valid_df, preserve_index=False) \n","        train_tokenized_datasets = train_dataset.map(self.tokenize_function, batched=True)\n","        val_tokenized_datasets = val_dataset.map(self.tokenize_function, batched=True)\n","        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n","        # print('model_fold_dir', model_fold_dir)\n","        training_args = TrainingArguments(\n","            output_dir=model_fold_dir,\n","            load_best_model_at_end=True, # select best model\n","            learning_rate=learning_rate,\n","            per_device_train_batch_size=batch_size,\n","            per_device_eval_batch_size=batch_size,\n","            num_train_epochs=num_train_epochs,\n","            weight_decay=weight_decay,\n","            report_to='none',\n","            greater_is_better=False,\n","            save_strategy=\"steps\",\n","            evaluation_strategy=\"steps\",\n","            eval_steps=save_steps,\n","            save_steps=save_steps,\n","            metric_for_best_model=\"rmse\",\n","            save_total_limit=1\n","        )\n","        # print('define trainer')\n","        trainer = Trainer(\n","            model=model_content,\n","            args=training_args,\n","            train_dataset=train_tokenized_datasets,\n","            eval_dataset=val_tokenized_datasets,\n","            tokenizer=self.tokenizer,\n","            compute_metrics=compute_metrics,\n","            data_collator=self.data_collator\n","        )\n","        print('start training')\n","        # print('trainer.train_dataset[0]' , trainer.train_dataset[0])\n","        trainer.train()\n","        print('finish training')\n","        model_content.save_pretrained(self.model_dir)\n","        self.tokenizer.save_pretrained(self.model_dir)\n","\n","        \n","    def predict(self, \n","                test_df: pd.DataFrame,\n","                fold: int,\n","               ):\n","        \"\"\"predict content score\"\"\"\n","        \n","        sep = self.tokenizer.sep_token\n","        in_text = (\n","                    test_df[\"prompt_title\"] + sep \n","                    + test_df[\"prompt_question\"] + sep \n","                    + test_df[\"fixed_summary_text\"]\n","                  )\n","        test_df[self.input_col] = in_text\n","\n","        test_ = test_df[[self.input_col]]\n","    \n","        test_dataset = Dataset.from_pandas(test_, preserve_index=False) \n","        test_tokenized_dataset = test_dataset.map(self.tokenize_function_test, batched=True)\n","\n","        model_content = AutoModelForSequenceClassification.from_pretrained(f\"{self.model_dir}\")\n","        model_content.eval()\n","        \n","        # eg. \"bert/fold_0/\"\n","        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n","        # print(\"model_fold_dir\",model_fold_dir)\n","        test_args = TrainingArguments(\n","            output_dir=model_fold_dir,\n","            do_train = False,\n","            do_predict = True,\n","            per_device_eval_batch_size = CFG.batch_size,   \n","            dataloader_drop_last = False,\n","        )\n","\n","        # init trainer\n","        infer_content = Trainer(\n","                      model = model_content, \n","                      tokenizer=self.tokenizer,\n","                      data_collator=self.data_collator,\n","                      args = test_args)\n","\n","        preds = infer_content.predict(test_tokenized_dataset)[0]\n","\n","        return preds"]},{"cell_type":"markdown","metadata":{},"source":["## Train by fold function\n"]},{"cell_type":"code","execution_count":345,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:49:18.973809Z","iopub.status.busy":"2023-10-04T06:49:18.973053Z","iopub.status.idle":"2023-10-04T06:49:18.989797Z","shell.execute_reply":"2023-10-04T06:49:18.988950Z","shell.execute_reply.started":"2023-10-04T06:49:18.973779Z"},"trusted":true},"outputs":[],"source":["def validate(\n","    train_df: pd.DataFrame,\n","    target:str,\n","    save_each_model: bool,\n","    model_name: str,\n","    model_dir_base: str,\n","    hidden_dropout_prob: float,\n","    attention_probs_dropout_prob: float,\n","    max_length : int\n","    ) -> pd.DataFrame:\n","    \"\"\"predict oof data\"\"\"\n","    for fold in range(CFG.n_splits):\n","        # print(f\"fold {fold}:\")\n","        \n","        valid_data = train_df[train_df[\"fold\"] == fold]\n","        \n","        if save_each_model == True:\n","            model_dir =  f\"{target}/{model_dir_base}/fold_{fold}\"\n","        else: \n","            model_dir =  f\"{model_dir_base}/fold_{fold}\"\n","        csr = ScoreRegressor(\n","            model_name=model_name,\n","            target=target,\n","            model_dir = model_dir,\n","            hidden_dropout_prob=hidden_dropout_prob,\n","            attention_probs_dropout_prob=attention_probs_dropout_prob,\n","            max_length=max_length,\n","           )\n","        \n","        pred = csr.predict(\n","            test_df=valid_data, \n","            fold=fold\n","        )\n","        # print('pred shape', pred.shape)\n","        train_df.loc[valid_data.index, f\"wording_pred\"] = pred[:,0]\n","        train_df.loc[valid_data.index, f\"content_pred\"] = pred[:,1]\n","\n","    return train_df\n","    \n","def predict(\n","    test_df: pd.DataFrame,\n","    target:str,\n","    save_each_model: bool,\n","    model_name: str,\n","    model_dir_base: str,\n","    hidden_dropout_prob: float,\n","    attention_probs_dropout_prob: float,\n","    max_length : int\n","    ):\n","    \"\"\"predict using mean folds\"\"\"\n","    for fold in range(CFG.n_splits):\n","        # print(f\"fold {fold}:\")\n","        \n","        if save_each_model == True:\n","            model_dir =  f\"{target}/{model_dir_base}/fold_{fold}\"\n","        else: \n","            model_dir =  f\"{model_dir_base}/fold_{fold}\"\n","        csr = ScoreRegressor(\n","            model_name=model_name,\n","            target=target,\n","            model_dir = model_dir, \n","            hidden_dropout_prob=hidden_dropout_prob,\n","            attention_probs_dropout_prob=attention_probs_dropout_prob,\n","            max_length=max_length,\n","           )\n","        \n","        pred = csr.predict(\n","            test_df=test_df, \n","            fold=fold\n","        )\n","        \n","        # test_df[f\"{target}_pred_{fold}\"] = pred\n","        test_df[f\"wording_pred_{fold}\"] = pred[:,0]\n","        test_df[f\"content_pred_{fold}\"] = pred[:,1]\n","        \n","    # test_df[f\"{target}\"] = test_df[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n","    test_df[f\"wording_pred\"] = test_df[[f\"wording_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n","    test_df[f\"content_pred\"] = test_df[[f\"content_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n","    return test_df\n","targets =  [\"content\", \"wording\"]\n"]},{"cell_type":"markdown","metadata":{},"source":["## Infer\n","\n"]},{"cell_type":"code","execution_count":347,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:49:18.991423Z","iopub.status.busy":"2023-10-04T06:49:18.990863Z","iopub.status.idle":"2023-10-04T06:51:09.807207Z","shell.execute_reply":"2023-10-04T06:51:09.806306Z","shell.execute_reply.started":"2023-10-04T06:49:18.991394Z"},"trusted":true},"outputs":[],"source":["# # ensembling_results_val  = pd.DataFrame()\n","# # ensembling_results_test = pd.DataFrame()\n","# dem = 0\n","# if CFG.infer_mode:\n","#     for model_dir in CFG.list_model_infer:\n","#         print(\"percent of model\", dem/CFG.number_base_model)\n","#         print(model_dir)    \n","#         dem = dem +1 \n","#         if dem >= CFG.number_base_model:\n","#             CFG.batch_size = 16\n","#             CFG.max_length = 1462\n","#             CFG.model_name = \"debertav3large\"\n","#         train = validate(\n","#             train,\n","#             target=targets,\n","#             save_each_model=False,\n","#             model_name=CFG.model_name,\n","#             model_dir_base = model_dir,\n","#             hidden_dropout_prob=CFG.hidden_dropout_prob,\n","#             attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n","#             max_length=CFG.max_length\n","#         )\n","#         # for target in targets:\n","#         #     rmse = mean_squared_error(train[target], train[f\"{target}_pred\"], squared=False)\n","#         #     print(f\"cv {target} rmse: {rmse}\")\n","#         print('done validate')\n","#         test = predict(\n","#             test,\n","#             target=targets,\n","#             save_each_model=False,\n","#             model_name=CFG.model_name,\n","#             model_dir_base = model_dir,\n","#             hidden_dropout_prob=CFG.hidden_dropout_prob,\n","#             attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n","#             max_length=CFG.max_length\n","#         )\n","#         print('done predict')\n","#         # add wording_pred and content_pred to ensembling_results\n","#         ensembling_results_val[f\"{model_dir}_wording_pred\"] = train[\"wording_pred\"]\n","#         ensembling_results_val[f\"{model_dir}_content_pred\"] = train[\"content_pred\"]\n","#         ensembling_results_test[f\"{model_dir}_wording_pred\"] = test[\"wording_pred\"]\n","#         ensembling_results_test[f\"{model_dir}_content_pred\"] = test[\"content_pred\"]\n","#         # print('ensembling_results_val \\n', ensembling_results_val.head() )\n","        "]},{"cell_type":"markdown","metadata":{},"source":["## LGBM model"]},{"cell_type":"code","execution_count":348,"metadata":{},"outputs":[],"source":["\n","# save ensembling_results_val and ensembling_results_test\n","# ensembling_results_val.to_csv(\"ensembling_results_val.csv\", index=False)\n","# ensembling_results_test.to_csv(\"ensembling_results_test.csv\", index=False)\n","# load ensembling_results_val and ensembling_results_test\n","ensembling_results_val = pd.read_csv(\"ensembling_results_val.csv\")\n","ensembling_results_test = pd.read_csv(\"ensembling_results_test.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["## Find the best weight with optuna"]},{"cell_type":"code","execution_count":349,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["16\n"]}],"source":["CFG.list_model_infer = [\n","        # 'upload_model/debertav3base_lr15e-05',\n","        'upload_model/debertav3base_lr17e-05', #keep\n","        'upload_model/debertav3base_lr18e-05', #keep\n","        'upload_model/debertav3base_lr21e-05', # keep\n","        'upload_model/debertav3base_lr22e-05', #keep \n","        # 'upload_model/debertav3base_lr5e-05', \n","        'upload_model/debertav3large_lr12e-05', # upload\n","        'upload_model/debertav3large_lr13e-05',  # upload\n","        # 'debertav3large_lr1e-05_save',\n","        # 'debertav3large_lr1e-05_att_0007',\n","        'debertav3large_lr8e-06_att_0007', # upload \n","        'debertav3large_lr9e-06_att_0007', # upload \n","        'debertav3large_lr11e-05_att_0007',\n","        'debertav3large_lr12e-05_att_0007',\n","        'debertav3large_lr13e-05_att_0007',\n","        'debertav3large_lr14e-05_att_0007',\n","        'debertav3large_lr15e-05_att_0007', # upload \n","        'debertav3large_lr16e-05_att_0007', # upload \n","        'debertav3large_lr17e-05_att_0007', # upload \n","        'debertav3large_lr18e-05_att_0007', # upload \n","        ]\n","print(len(CFG.list_model_infer))"]},{"cell_type":"code","execution_count":384,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["upload_model/debertav3base_lr17e-05 mcrmse: 0.5543694149276929\n","upload_model/debertav3base_lr18e-05 mcrmse: 0.5526207597677534\n","upload_model/debertav3base_lr21e-05 mcrmse: 0.553756334918185\n","upload_model/debertav3base_lr22e-05 mcrmse: 0.5480178090069433\n","upload_model/debertav3large_lr12e-05 mcrmse: 0.5531885292652218\n","upload_model/debertav3large_lr13e-05 mcrmse: 0.5520867504987613\n","debertav3large_lr8e-06_att_0007 mcrmse: 0.5257363371291668\n","debertav3large_lr9e-06_att_0007 mcrmse: 0.5391015003980028\n","debertav3large_lr11e-05_att_0007 mcrmse: 0.5395042177968217\n","debertav3large_lr12e-05_att_0007 mcrmse: 0.5454541263489372\n","debertav3large_lr13e-05_att_0007 mcrmse: 0.5502121290213762\n","debertav3large_lr14e-05_att_0007 mcrmse: 0.5392729849094001\n","debertav3large_lr15e-05_att_0007 mcrmse: 0.5388472988905048\n","debertav3large_lr16e-05_att_0007 mcrmse: 0.5568832398581905\n","debertav3large_lr17e-05_att_0007 mcrmse: 0.5517737704359214\n","debertav3large_lr18e-05_att_0007 mcrmse: 0.5563119146067961\n"]}],"source":["weight_for_model = {}\n","scale = 0 \n","for model in CFG.list_model_infer:\n","    results = ensembling_results_val[[f\"{model}_wording_pred\", f\"{model}_content_pred\"]]\n","    mcrmse = compute_mcrmse((results.values, train[targets].values))\n","    print(f\"{model} mcrmse: {mcrmse['mcrmse']}\")\n","    # weight_for_model[model] = 1 / mcrmse[\"mcrmse\"]\n","    # scale = scale + weight_for_model[model]\n","    "]},{"cell_type":"code","execution_count":385,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["16\n","scale 6.7863581942530455\n"]}],"source":["weight_for_model =  {       \n","    \"upload_model/debertav3base_lr17e-05\": 0.212909408976402    ,\n","    \"upload_model/debertav3base_lr18e-05\": 0.1421852032796326,\n","    \"upload_model/debertav3base_lr21e-05\": 0.8953672127134387,\n","    \"upload_model/debertav3base_lr22e-05\": 0.36047950541959095,\n","    \"upload_model/debertav3large_lr12e-05\": 0.9413883882391965,\n","    \"upload_model/debertav3large_lr13e-05\": 0.48278638692252185,\n","    \"debertav3large_lr8e-06_att_0007\": 0.30564935544213445,\n","    \"debertav3large_lr9e-06_att_0007\": 0.40560485262757806,\n","    \"debertav3large_lr11e-05_att_0007\": 0.882846667614644,\n","    \"debertav3large_lr12e-05_att_0007\": 0.09507155361092456,\n","    \"debertav3large_lr13e-05_att_0007\": 0.6241475562582361,\n","    \"debertav3large_lr14e-05_att_0007\": 0.03402452758512511,\n","    \"debertav3large_lr15e-05_att_0007\": 0.11628723374671882,\n","    \"debertav3large_lr16e-05_att_0007\": 0.0001281844830617676,\n","    \"debertav3large_lr17e-05_att_0007\": 0.969431755664122,\n","    \"debertav3large_lr18e-05_att_0007\": 0.318050401669718,\n","}\n","print(len(weight_for_model))\n","scale = np.sum(list(weight_for_model.values()))\n","print('scale', scale)"]},{"cell_type":"code","execution_count":386,"metadata":{},"outputs":[],"source":["# weight_for_model =  {'upload_model/debertav3base_lr5e-05': 0.051723147603719516,\n","#  'upload_model/debertav3base_lr15e-05': 0.713986390124981,\n","#  'upload_model/debertav3base_lr17e-05': 0.21585195973104482,\n","#  'upload_model/debertav3base_lr18e-05': 0.4991352617947008,\n","#  'upload_model/debertav3base_lr21e-05': 0.004646508278479511,\n","#  'upload_model/debertav3base_lr22e-05': 0.11250582743280035,\n","#  'upload_model/debertav3large_lr12e-05': 0.6062309604440405,\n","#  'upload_model/debertav3large_lr13e-05': 0.41563814657040926,\n","#  'debertav3large_lr1e-05_save': 0.8788672880708215,\n","#  'debertav3large_lr1e-05_att_0007': 0.882589383882597,\n","#  'debertav3large_lr8e-06_att_0007': 0.7194970862228134,\n","#  'debertav3large_lr9e-06_att_0007': 0.6181074749297657,\n","#  'debertav3large_lr11e-05_att_0007': 0.9809193274307263,\n","#  'debertav3large_lr12e-05_att_0007': 0.17214549469931723,\n","#  'debertav3large_lr13e-05_att_0007': 0.4328122227545144,\n","#  'debertav3large_lr14e-05_att_0007': 0.3284035971778506,\n","#  'debertav3large_lr15e-05_att_0007': 0.9392121766363538,\n","#  'debertav3large_lr16e-05_att_0007': 0.033996448553002434,\n","#  'debertav3large_lr17e-05_att_0007': 0.8327105838357515,\n","#  'debertav3large_lr18e-05_att_0007': 0.8773681981314573}\n","# print(len(weight_for_model))\n","# scale = np.sum(list(weight_for_model.values()))\n","# print('scale', scale)"]},{"cell_type":"code","execution_count":401,"metadata":{},"outputs":[],"source":["train = pd.read_csv(\"input/train.csv\")"]},{"cell_type":"code","execution_count":402,"metadata":{},"outputs":[],"source":["train[f\"wording_pred\"] = np.sum([ensembling_results_val[f\"{model}_wording_pred\"] * weight_for_model[model] for model in CFG.list_model_infer], axis=0) / scale    \n","train[f\"content_pred\"] = np.sum([ensembling_results_val[f\"{model}_content_pred\"] * weight_for_model[model] for model in CFG.list_model_infer], axis=0) / scale\n","test[f\"wording_pred\"] = np.sum([ensembling_results_test[f\"{model}_wording_pred\"] * weight_for_model[model] for model in CFG.list_model_infer], axis=0) / scale\n","test[f\"content_pred\"] = np.sum([ensembling_results_test[f\"{model}_content_pred\"] * weight_for_model[model] for model in CFG.list_model_infer], axis=0) / scale\n"]},{"cell_type":"code","execution_count":403,"metadata":{},"outputs":[],"source":["# save train and test\n","# train.to_csv(\"input/train.csv\", index=False)\n","# load train and test\n","# test = pd.read_csv(\"input/test.csv\")"]},{"cell_type":"code","execution_count":404,"metadata":{},"outputs":[],"source":["# train.head()"]},{"cell_type":"code","execution_count":405,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:09.809377Z","iopub.status.busy":"2023-10-04T06:51:09.808548Z","iopub.status.idle":"2023-10-04T06:51:09.814418Z","shell.execute_reply":"2023-10-04T06:51:09.813556Z","shell.execute_reply.started":"2023-10-04T06:51:09.809343Z"},"trusted":true},"outputs":[],"source":["targets = [\"content\", \"wording\"]\n","\n","drop_columns = [\"fold\", \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n","                \"prompt_question\", \"prompt_title\", \n","                \"prompt_text\"\n","               ] + targets"]},{"cell_type":"code","execution_count":406,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:09.818456Z","iopub.status.busy":"2023-10-04T06:51:09.817994Z","iopub.status.idle":"2023-10-04T06:51:10.548361Z","shell.execute_reply":"2023-10-04T06:51:10.547516Z","shell.execute_reply.started":"2023-10-04T06:51:09.818419Z"},"trusted":true},"outputs":[],"source":["def create_model_dict(targets,train):\n","  model_dict = {}\n","  for target in targets:\n","      models = []\n","\n","      for fold in range(CFG.n_splits):\n","          X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns, inplace=False)\n","          y_train_cv = train[train[\"fold\"] != fold][target]\n","\n","          X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n","          y_eval_cv = train[train[\"fold\"] == fold][target]\n","\n","          dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n","          dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n","\n","          params = {\n","              'boosting_type': 'gbdt',\n","              'random_state': 42,\n","              'objective': 'regression',\n","              'metric': 'rmse',\n","              'learning_rate': 0.048,\n","              'max_depth': 3,\n","              'lambda_l1': 0.0,\n","              'lambda_l2': 0.011,\n","              'verbose': -1,\n","          }\n","\n","          evaluation_results = {}\n","          model = lgb.train(params,\n","                            num_boost_round=10000,\n","                            valid_names=['train', 'valid'],\n","                            train_set=dtrain,\n","                            valid_sets=dval,\n","                            callbacks=[\n","                                lgb.early_stopping(stopping_rounds=70, verbose=False),\n","                                # lgb.log_evaluation(100),\n","                                lgb.callback.record_evaluation(evaluation_results)\n","                              ],\n","                            )\n","          models.append(model)\n","\n","      model_dict[target] = models\n","  return model_dict\n","model_dict = create_model_dict(targets,train)\n"]},{"cell_type":"markdown","metadata":{},"source":["## CV Score"]},{"cell_type":"code","execution_count":407,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:10.587200Z","iopub.status.busy":"2023-10-04T06:51:10.586764Z","iopub.status.idle":"2023-10-04T06:51:10.684496Z","shell.execute_reply":"2023-10-04T06:51:10.683623Z","shell.execute_reply.started":"2023-10-04T06:51:10.587169Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["content_rmse : 0.4131673554646339\n","wording_rmse : 0.5406696561968247\n","mcrmse: 0.4769185058307293\n"]}],"source":["# cv\n","import optuna\n","def cal_mcrmse(model_dict, targets):\n","    rmses = []\n","    for target in targets:\n","        models = model_dict[target]\n","\n","        preds = []\n","        trues = []\n","        \n","        for fold, model in enumerate(models):\n","            X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns , inplace=False)\n","            y_eval_cv = train[train[\"fold\"] == fold][target]\n","\n","            pred = model.predict(X_eval_cv)\n","\n","            trues.extend(y_eval_cv)\n","            preds.extend(pred)\n","            \n","        rmse = np.sqrt(mean_squared_error(trues, preds))\n","        print(f\"{target}_rmse : {rmse}\")\n","        rmses = rmses + [rmse]\n","    return sum(rmses) / len(rmses)\n","mcrmse = cal_mcrmse(model_dict, targets)\n","print(f\"mcrmse: {mcrmse}\")"]},{"cell_type":"code","execution_count":408,"metadata":{},"outputs":[],"source":["def objective(trial):\n","    weight_for_model = {}\n","    for model in CFG.list_model_infer:\n","        weight_for_model[model] = trial.suggest_float(model, 0.0, 1.0)\n","    scale = np.sum([weight_for_model[model] for model in CFG.list_model_infer])\n","    train[f\"wording_pred\"] = np.sum([ensembling_results_val[f\"{model}_wording_pred\"] * weight_for_model[model] for model in CFG.list_model_infer], axis=0) / scale    \n","    train[f\"content_pred\"] = np.sum([ensembling_results_val[f\"{model}_content_pred\"] * weight_for_model[model] for model in CFG.list_model_infer], axis=0) / scale\n","    model_dict = create_model_dict(targets,train)\n","    lost = cal_mcrmse(model_dict, targets)\n","    # print(f\"mcrmse: {lost}\")\n","    # print(f\"weight_for_model: {weight_for_model}\")\n","    return lost"]},{"cell_type":"code","execution_count":409,"metadata":{},"outputs":[],"source":["# study = optuna.create_study(direction='minimize')\n","# study.optimize(objective, n_trials=100)"]},{"cell_type":"code","execution_count":410,"metadata":{},"outputs":[],"source":["\n","# print('Best trial:')\n","# trial_ = study.best_trial\n","\n","# print('Value: ', trial_.value)\n","\n","# print('Params: ')\n","# for key, value in trial_.params.items():\n","#     print('    {}: {}'.format(key, value)) \n","\n","# content_rmse : 0.4131169758305616\n","# wording_rmse : 0.5421836129991084\n","# mcrmse: 0.47765029441483503\n"]},{"cell_type":"code","execution_count":411,"metadata":{},"outputs":[],"source":["\n","# Best trial:\n","# Value:  0.4769185058307293\n","# Params: \n","#     upload_model/debertav3base_lr17e-05: 0.212909408976402\n","#     upload_model/debertav3base_lr18e-05: 0.1421852032796326\n","#     upload_model/debertav3base_lr21e-05: 0.8953672127134387\n","#     upload_model/debertav3base_lr22e-05: 0.36047950541959095\n","#     upload_model/debertav3large_lr12e-05: 0.9413883882391965\n","#     upload_model/debertav3large_lr13e-05: 0.48278638692252185\n","#     debertav3large_lr8e-06_att_0007: 0.30564935544213445\n","#     debertav3large_lr9e-06_att_0007: 0.40560485262757806\n","#     debertav3large_lr11e-05_att_0007: 0.882846667614644\n","#     debertav3large_lr12e-05_att_0007: 0.09507155361092456\n","#     debertav3large_lr13e-05_att_0007: 0.6241475562582361\n","#     debertav3large_lr14e-05_att_0007: 0.03402452758512511\n","#     debertav3large_lr15e-05_att_0007: 0.11628723374671882\n","#     debertav3large_lr16e-05_att_0007: 0.0001281844830617676\n","#     debertav3large_lr17e-05_att_0007: 0.969431755664122\n","#     debertav3large_lr18e-05_att_0007: 0.318050401669718\n","\n","# Best trial:\n","# Value:  0.4772940289625304\n","# Params: \n","#     upload_model/debertav3base_lr5e-05: 0.09347891502416127\n","#     upload_model/debertav3base_lr15e-05: 0.591661405535349\n","#     upload_model/debertav3base_lr17e-05: 0.2899256756091853\n","#     upload_model/debertav3base_lr18e-05: 0.5247554121196624\n","#     upload_model/debertav3base_lr21e-05: 0.06827852930559399\n","#     upload_model/debertav3base_lr22e-05: 0.386669811226943\n","#     upload_model/debertav3large_lr12e-05: 0.4819735230934519\n","#     upload_model/debertav3large_lr13e-05: 0.6277811009078926\n","#     debertav3large_lr1e-05_save: 0.8800293681553476\n","#     debertav3large_lr1e-05_att_0007: 0.907620186268483\n","#     debertav3large_lr8e-06_att_0007: 0.20560270128817018\n","#     debertav3large_lr9e-06_att_0007: 0.8283922073344201\n","#     debertav3large_lr11e-05_att_0007: 0.9434490377699455\n","#     debertav3large_lr12e-05_att_0007: 0.9568517774138272\n","#     debertav3large_lr13e-05_att_0007: 0.5510754606259485\n","#     debertav3large_lr14e-05_att_0007: 0.0505932089216867\n","#     debertav3large_lr15e-05_att_0007: 0.020756011719650247\n","#     debertav3large_lr16e-05_att_0007: 0.019564460078885\n","#     debertav3large_lr17e-05_att_0007: 0.9065148193079343\n","#     debertav3large_lr18e-05_att_0007: 0.3463884928585018\n","\n","# Best trial:\n","# Value:  0.4771132861010911\n","# Params: \n","#     upload_model/debertav3base_lr5e-05: 0.051723147603719516\n","#     upload_model/debertav3base_lr15e-05: 0.713986390124981\n","#     upload_model/debertav3base_lr17e-05: 0.21585195973104482\n","#     upload_model/debertav3base_lr18e-05: 0.4991352617947008\n","#     upload_model/debertav3base_lr21e-05: 0.004646508278479511\n","#     upload_model/debertav3base_lr22e-05: 0.11250582743280035\n","#     upload_model/debertav3large_lr12e-05: 0.6062309604440405\n","#     upload_model/debertav3large_lr13e-05: 0.41563814657040926\n","#     debertav3large_lr1e-05_save: 0.8788672880708215\n","#     debertav3large_lr1e-05_att_0007: 0.882589383882597\n","#     debertav3large_lr8e-06_att_0007: 0.7194970862228134\n","#     debertav3large_lr9e-06_att_0007: 0.6181074749297657\n","#     debertav3large_lr11e-05_att_0007: 0.9809193274307263\n","#     debertav3large_lr12e-05_att_0007: 0.17214549469931723\n","#     debertav3large_lr13e-05_att_0007: 0.4328122227545144\n","#     debertav3large_lr14e-05_att_0007: 0.3284035971778506\n","#     debertav3large_lr15e-05_att_0007: 0.9392121766363538\n","#     debertav3large_lr16e-05_att_0007: 0.033996448553002434\n","#     debertav3large_lr17e-05_att_0007: 0.8327105838357515\n","#     debertav3large_lr18e-05_att_0007: 0.8773681981314573"]},{"cell_type":"code","execution_count":412,"metadata":{},"outputs":[],"source":["# content_rmse : 0.41518188998433053\n","# wording_rmse : 0.5438935328374503\n","# mcrmse : 0.4795377114108904 WITH WEIGHT \n","# use 22 modell \n"]},{"cell_type":"code","execution_count":413,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["16\n"]}],"source":["print(len(CFG.list_model_infer))"]},{"cell_type":"markdown","metadata":{},"source":["## Predict"]},{"cell_type":"code","execution_count":414,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:10.686383Z","iopub.status.busy":"2023-10-04T06:51:10.685913Z","iopub.status.idle":"2023-10-04T06:51:10.691848Z","shell.execute_reply":"2023-10-04T06:51:10.691034Z","shell.execute_reply.started":"2023-10-04T06:51:10.686351Z"},"trusted":true},"outputs":[],"source":["drop_columns_2 = [\n","                # \"fold\", \n","                \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n","                \"prompt_question\", \"prompt_title\", \n","                \"prompt_text\",\n","                \"input\"\n","               ] + [\n","                f\"content_pred_{i}\" for i in range(CFG.n_splits)\n","                ] + [\n","                f\"wording_pred_{i}\" for i in range(CFG.n_splits)\n","                ]"]},{"cell_type":"code","execution_count":415,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:10.693770Z","iopub.status.busy":"2023-10-04T06:51:10.693252Z","iopub.status.idle":"2023-10-04T06:51:10.714520Z","shell.execute_reply":"2023-10-04T06:51:10.713791Z","shell.execute_reply.started":"2023-10-04T06:51:10.693733Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[LightGBM] [Fatal] The number of features in data (13) is not the same as it was in training data (45).\n","You can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing.\n"]},{"ename":"LightGBMError","evalue":"The number of features in data (13) is not the same as it was in training data (45).\nYou can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)","\u001b[1;32m/home/nghiaph/nghiaph_workspace_115/CommonLit/commonlit-merge-model-infer-weight.ipynb Cell 52\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.166.128.115/home/nghiaph/nghiaph_workspace_115/CommonLit/commonlit-merge-model-infer-weight.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m X_eval_cv \u001b[39m=\u001b[39m test\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39mdrop_columns_2)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.166.128.115/home/nghiaph/nghiaph_workspace_115/CommonLit/commonlit-merge-model-infer-weight.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# print(X_eval_cv.head())\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.166.128.115/home/nghiaph/nghiaph_workspace_115/CommonLit/commonlit-merge-model-infer-weight.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(X_eval_cv)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.166.128.115/home/nghiaph/nghiaph_workspace_115/CommonLit/commonlit-merge-model-infer-weight.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# print('pred shape'  , pred.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.166.128.115/home/nghiaph/nghiaph_workspace_115/CommonLit/commonlit-merge-model-infer-weight.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m preds\u001b[39m.\u001b[39mappend(pred)\n","File \u001b[0;32m~/nghiaph_workspace_115/CommonLit/.venv/lib/python3.8/site-packages/lightgbm/basic.py:4220\u001b[0m, in \u001b[0;36mBooster.predict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features, **kwargs)\u001b[0m\n\u001b[1;32m   4218\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4219\u001b[0m         num_iteration \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 4220\u001b[0m \u001b[39mreturn\u001b[39;00m predictor\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m   4221\u001b[0m     data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m   4222\u001b[0m     start_iteration\u001b[39m=\u001b[39;49mstart_iteration,\n\u001b[1;32m   4223\u001b[0m     num_iteration\u001b[39m=\u001b[39;49mnum_iteration,\n\u001b[1;32m   4224\u001b[0m     raw_score\u001b[39m=\u001b[39;49mraw_score,\n\u001b[1;32m   4225\u001b[0m     pred_leaf\u001b[39m=\u001b[39;49mpred_leaf,\n\u001b[1;32m   4226\u001b[0m     pred_contrib\u001b[39m=\u001b[39;49mpred_contrib,\n\u001b[1;32m   4227\u001b[0m     data_has_header\u001b[39m=\u001b[39;49mdata_has_header,\n\u001b[1;32m   4228\u001b[0m     validate_features\u001b[39m=\u001b[39;49mvalidate_features\n\u001b[1;32m   4229\u001b[0m )\n","File \u001b[0;32m~/nghiaph_workspace_115/CommonLit/.venv/lib/python3.8/site-packages/lightgbm/basic.py:1047\u001b[0m, in \u001b[0;36m_InnerPredictor.predict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     preds, nrow \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__pred_for_csc(\n\u001b[1;32m   1041\u001b[0m         csc\u001b[39m=\u001b[39mdata,\n\u001b[1;32m   1042\u001b[0m         start_iteration\u001b[39m=\u001b[39mstart_iteration,\n\u001b[1;32m   1043\u001b[0m         num_iteration\u001b[39m=\u001b[39mnum_iteration,\n\u001b[1;32m   1044\u001b[0m         predict_type\u001b[39m=\u001b[39mpredict_type\n\u001b[1;32m   1045\u001b[0m     )\n\u001b[1;32m   1046\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m-> 1047\u001b[0m     preds, nrow \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__pred_for_np2d(\n\u001b[1;32m   1048\u001b[0m         mat\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m   1049\u001b[0m         start_iteration\u001b[39m=\u001b[39;49mstart_iteration,\n\u001b[1;32m   1050\u001b[0m         num_iteration\u001b[39m=\u001b[39;49mnum_iteration,\n\u001b[1;32m   1051\u001b[0m         predict_type\u001b[39m=\u001b[39;49mpredict_type\n\u001b[1;32m   1052\u001b[0m     )\n\u001b[1;32m   1053\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mlist\u001b[39m):\n\u001b[1;32m   1054\u001b[0m     \u001b[39mtry\u001b[39;00m:\n","File \u001b[0;32m~/nghiaph_workspace_115/CommonLit/.venv/lib/python3.8/site-packages/lightgbm/basic.py:1187\u001b[0m, in \u001b[0;36m_InnerPredictor.__pred_for_np2d\u001b[0;34m(self, mat, start_iteration, num_iteration, predict_type)\u001b[0m\n\u001b[1;32m   1185\u001b[0m     \u001b[39mreturn\u001b[39;00m preds, nrow\n\u001b[1;32m   1186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1187\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__inner_predict_np2d(\n\u001b[1;32m   1188\u001b[0m         mat\u001b[39m=\u001b[39;49mmat,\n\u001b[1;32m   1189\u001b[0m         start_iteration\u001b[39m=\u001b[39;49mstart_iteration,\n\u001b[1;32m   1190\u001b[0m         num_iteration\u001b[39m=\u001b[39;49mnum_iteration,\n\u001b[1;32m   1191\u001b[0m         predict_type\u001b[39m=\u001b[39;49mpredict_type,\n\u001b[1;32m   1192\u001b[0m         preds\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m   1193\u001b[0m     )\n","File \u001b[0;32m~/nghiaph_workspace_115/CommonLit/.venv/lib/python3.8/site-packages/lightgbm/basic.py:1140\u001b[0m, in \u001b[0;36m_InnerPredictor.__inner_predict_np2d\u001b[0;34m(self, mat, start_iteration, num_iteration, predict_type, preds)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mWrong length of pre-allocated predict array\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1139\u001b[0m out_num_preds \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int64(\u001b[39m0\u001b[39m)\n\u001b[0;32m-> 1140\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterPredictForMat(\n\u001b[1;32m   1141\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle,\n\u001b[1;32m   1142\u001b[0m     ptr_data,\n\u001b[1;32m   1143\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(type_ptr_data),\n\u001b[1;32m   1144\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int32(mat\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]),\n\u001b[1;32m   1145\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int32(mat\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m]),\n\u001b[1;32m   1146\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(_C_API_IS_ROW_MAJOR),\n\u001b[1;32m   1147\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(predict_type),\n\u001b[1;32m   1148\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(start_iteration),\n\u001b[1;32m   1149\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(num_iteration),\n\u001b[1;32m   1150\u001b[0m     _c_str(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpred_parameter),\n\u001b[1;32m   1151\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(out_num_preds),\n\u001b[1;32m   1152\u001b[0m     preds\u001b[39m.\u001b[39;49mctypes\u001b[39m.\u001b[39;49mdata_as(ctypes\u001b[39m.\u001b[39;49mPOINTER(ctypes\u001b[39m.\u001b[39;49mc_double))))\n\u001b[1;32m   1153\u001b[0m \u001b[39mif\u001b[39;00m n_preds \u001b[39m!=\u001b[39m out_num_preds\u001b[39m.\u001b[39mvalue:\n\u001b[1;32m   1154\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mWrong length for predict results\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/nghiaph_workspace_115/CommonLit/.venv/lib/python3.8/site-packages/lightgbm/basic.py:242\u001b[0m, in \u001b[0;36m_safe_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check the return value from C API call.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \n\u001b[1;32m    236\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[39m    The return value from C API calls.\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 242\u001b[0m     \u001b[39mraise\u001b[39;00m LightGBMError(_LIB\u001b[39m.\u001b[39mLGBM_GetLastError()\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m))\n","\u001b[0;31mLightGBMError\u001b[0m: The number of features in data (13) is not the same as it was in training data (45).\nYou can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing."]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["pred_dict = {}\n","for target in targets:\n","    models = model_dict[target]\n","    preds = []\n","\n","    for fold, model in enumerate(models):\n","        X_eval_cv = test.drop(columns=drop_columns_2)\n","        # print(X_eval_cv.head())\n","        pred = model.predict(X_eval_cv)\n","        # print('pred shape'  , pred.shape)\n","        preds.append(pred)\n","    \n","    pred_dict[target] = preds"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:10.716195Z","iopub.status.busy":"2023-10-04T06:51:10.715767Z","iopub.status.idle":"2023-10-04T06:51:10.726302Z","shell.execute_reply":"2023-10-04T06:51:10.725328Z","shell.execute_reply.started":"2023-10-04T06:51:10.716164Z"},"trusted":true},"outputs":[],"source":["for target in targets:\n","    preds = pred_dict[target]\n","    for i, pred in enumerate(preds):\n","        test[f\"{target}_pred_{i}\"] = pred\n","\n","    test[target] = test[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:10.728082Z","iopub.status.busy":"2023-10-04T06:51:10.727853Z","iopub.status.idle":"2023-10-04T06:51:10.748281Z","shell.execute_reply":"2023-10-04T06:51:10.747376Z","shell.execute_reply.started":"2023-10-04T06:51:10.728052Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>prompt_id</th>\n","      <th>text</th>\n","      <th>summary_length</th>\n","      <th>fixed_summary_text</th>\n","      <th>splling_err_num</th>\n","      <th>prompt_question</th>\n","      <th>prompt_title</th>\n","      <th>prompt_text</th>\n","      <th>prompt_length</th>\n","      <th>...</th>\n","      <th>wording_pred_1</th>\n","      <th>content_pred_1</th>\n","      <th>wording_pred_2</th>\n","      <th>content_pred_2</th>\n","      <th>wording_pred_3</th>\n","      <th>content_pred_3</th>\n","      <th>wording_pred</th>\n","      <th>content_pred</th>\n","      <th>content</th>\n","      <th>wording</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000000ffffff</td>\n","      <td>abc123</td>\n","      <td>Example text 1</td>\n","      <td>3</td>\n","      <td>Example text 1</td>\n","      <td>0</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 1</td>\n","      <td>Heading\\nText...</td>\n","      <td>3</td>\n","      <td>...</td>\n","      <td>-1.338309</td>\n","      <td>-1.523791</td>\n","      <td>-1.190337</td>\n","      <td>-1.436432</td>\n","      <td>-1.349174</td>\n","      <td>-1.540069</td>\n","      <td>-1.532378</td>\n","      <td>-1.339146</td>\n","      <td>-1.495245</td>\n","      <td>-1.311898</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>111111eeeeee</td>\n","      <td>def789</td>\n","      <td>Example text 2</td>\n","      <td>3</td>\n","      <td>Example text 2</td>\n","      <td>0</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 2</td>\n","      <td>Heading\\nText...</td>\n","      <td>3</td>\n","      <td>...</td>\n","      <td>-1.338309</td>\n","      <td>-1.523791</td>\n","      <td>-1.190337</td>\n","      <td>-1.436432</td>\n","      <td>-1.349174</td>\n","      <td>-1.540069</td>\n","      <td>-1.534876</td>\n","      <td>-1.340151</td>\n","      <td>-1.495245</td>\n","      <td>-1.311898</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>222222cccccc</td>\n","      <td>abc123</td>\n","      <td>Example text 3</td>\n","      <td>3</td>\n","      <td>Example text 3</td>\n","      <td>0</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 1</td>\n","      <td>Heading\\nText...</td>\n","      <td>3</td>\n","      <td>...</td>\n","      <td>-1.343294</td>\n","      <td>-1.520616</td>\n","      <td>-1.190337</td>\n","      <td>-1.436432</td>\n","      <td>-1.349174</td>\n","      <td>-1.540069</td>\n","      <td>-1.533215</td>\n","      <td>-1.345367</td>\n","      <td>-1.494451</td>\n","      <td>-1.313144</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>333333dddddd</td>\n","      <td>def789</td>\n","      <td>Example text 4</td>\n","      <td>3</td>\n","      <td>Example text 4</td>\n","      <td>0</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 2</td>\n","      <td>Heading\\nText...</td>\n","      <td>3</td>\n","      <td>...</td>\n","      <td>-1.338309</td>\n","      <td>-1.523791</td>\n","      <td>-1.190337</td>\n","      <td>-1.436432</td>\n","      <td>-1.349174</td>\n","      <td>-1.540069</td>\n","      <td>-1.535747</td>\n","      <td>-1.341987</td>\n","      <td>-1.495245</td>\n","      <td>-1.311898</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4 rows × 29 columns</p>\n","</div>"],"text/plain":["     student_id prompt_id            text  summary_length fixed_summary_text  \\\n","0  000000ffffff    abc123  Example text 1               3     Example text 1   \n","1  111111eeeeee    def789  Example text 2               3     Example text 2   \n","2  222222cccccc    abc123  Example text 3               3     Example text 3   \n","3  333333dddddd    def789  Example text 4               3     Example text 4   \n","\n","   splling_err_num prompt_question     prompt_title       prompt_text  \\\n","0                0    Summarize...  Example Title 1  Heading\\nText...   \n","1                0    Summarize...  Example Title 2  Heading\\nText...   \n","2                0    Summarize...  Example Title 1  Heading\\nText...   \n","3                0    Summarize...  Example Title 2  Heading\\nText...   \n","\n","   prompt_length  ...  wording_pred_1  content_pred_1  wording_pred_2  \\\n","0              3  ...       -1.338309       -1.523791       -1.190337   \n","1              3  ...       -1.338309       -1.523791       -1.190337   \n","2              3  ...       -1.343294       -1.520616       -1.190337   \n","3              3  ...       -1.338309       -1.523791       -1.190337   \n","\n","   content_pred_2  wording_pred_3  content_pred_3 wording_pred  content_pred  \\\n","0       -1.436432       -1.349174       -1.540069    -1.532378     -1.339146   \n","1       -1.436432       -1.349174       -1.540069    -1.534876     -1.340151   \n","2       -1.436432       -1.349174       -1.540069    -1.533215     -1.345367   \n","3       -1.436432       -1.349174       -1.540069    -1.535747     -1.341987   \n","\n","    content   wording  \n","0 -1.495245 -1.311898  \n","1 -1.495245 -1.311898  \n","2 -1.494451 -1.313144  \n","3 -1.495245 -1.311898  \n","\n","[4 rows x 29 columns]"]},"execution_count":368,"metadata":{},"output_type":"execute_result"}],"source":["test"]},{"cell_type":"markdown","metadata":{},"source":["## Create Submission file"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:10.750060Z","iopub.status.busy":"2023-10-04T06:51:10.749559Z","iopub.status.idle":"2023-10-04T06:51:10.758788Z","shell.execute_reply":"2023-10-04T06:51:10.757827Z","shell.execute_reply.started":"2023-10-04T06:51:10.750030Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>content</th>\n","      <th>wording</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000000ffffff</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>111111eeeeee</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>222222cccccc</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>333333dddddd</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     student_id  content  wording\n","0  000000ffffff      0.0      0.0\n","1  111111eeeeee      0.0      0.0\n","2  222222cccccc      0.0      0.0\n","3  333333dddddd      0.0      0.0"]},"execution_count":369,"metadata":{},"output_type":"execute_result"}],"source":["sample_submission"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T06:51:10.760952Z","iopub.status.busy":"2023-10-04T06:51:10.759923Z","iopub.status.idle":"2023-10-04T06:51:10.770471Z","shell.execute_reply":"2023-10-04T06:51:10.769640Z","shell.execute_reply.started":"2023-10-04T06:51:10.760921Z"},"trusted":true},"outputs":[],"source":["test[[\"student_id\", \"content\", \"wording\"]].to_csv(\"submission.csv\", index=False)"]},{"cell_type":"markdown","metadata":{},"source":["## Summary\n","\n","CV result is like this.\n","\n","| | content rmse |wording rmse | mcrmse | LB| |\n","| -- | -- | -- | -- | -- | -- |\n","|baseline| 0.494 | 0.630 | 0.562 | 0.509 | [link](https://www.kaggle.com/code/tsunotsuno/debertav3-baseline-content-and-wording-models)|\n","| use title and question field | 0.476| 0.619 | 0.548 | 0.508 | [link](https://www.kaggle.com/code/tsunotsuno/debertav3-w-prompt-title-question-fields) |\n","| Debertav3 + LGBM | 0.451 | 0.591 | 0.521 | 0.461 | [link](https://www.kaggle.com/code/tsunotsuno/debertav3-lgbm-with-feature-engineering) |\n","| Debertav3 + LGBM with spell autocorrect | 0.448 | 0.581 | 0.514 | 0.459 |nogawanogawa's original code\n","| Debertav3 + LGBM with spell autocorrect and tuning | 0.442 | 0.566 | 0.504 | 0.453 | this notebook |\n","\n","The CV values improved slightly, and the LB value is improved."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
